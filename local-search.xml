<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>随笔：一些DL编程中的小&quot;寄&quot;巧(2)</title>
    <link href="/posts/c6450e7b/"/>
    <url>/posts/c6450e7b/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="近期一些dl编程中的小寄巧二">近期一些DL编程中的小”寄”巧(二)</h1><h2 id="六.-续如何真正有效提高显卡利用率">六.（续）如何真正有效提高显卡利用率？</h2><p>Follow 下述 checklist?</p><ul class="task-list"><li><p><label><input type="checkbox" />1. 你的显卡是否先进？(本质性问题)比如某司2024年还在把NVIDIA V100 16Gx8的卡给实习生用，训练速度感人，其实还不如4张<ahref="https://www.autodl.com">AutoDL</a>的virtual V10032G速度快。。。甚至用了3-4年的V100SXM32G其实远远不如3090，4090，甚至H20/H800(或者更牛的数据中心级GPU芯片)做成多份virtualV100分着用都快多了。训练小模型的速度甚至是6天和10h的区别。</label></p></li><li><p><label><input type="checkbox" />2.开始讲正事了，在显卡算力平台确定的情况下，尽可能的开大batchsize去占满显存，从而尽量提高利用率，同时batchsize变大更有利于模型的收敛与性能提升。</label></p></li><li><p><label><input type="checkbox" />3.执行完第二步后，如果总体预计耗时还是没变甚至稍微更慢了，可以在dataloader部分，整个网路forward部分，backward部分分别计算耗时，平均每100个iteration打印一下平均耗时，看看瓶颈是卡在模型上还是卡在CPUload数据上。如果瓶颈卡在CPUload数据上：可以适当增加num_workers的数量，比如batch-size开到8后，dataloader读一次数据需要6s,但是网络forward一遍只需要0.5s,backward一遍只需要1.3s，那么就可以把num_workers的数量从1开到4，8等倍数增加；另外也可以把网络训练的部分注释掉，看看只读取数据耗时是多少。如果瓶颈卡在网络上：通过不断增加被注释的部分定位出耗时间的瓶颈在哪一部分代码，是否可以用C语言代替一些底层运算在粘起来，或者用numba加速，或者使用CuPy等平行库代替Numpy；同时尽量减少在forward过程中间tensor和numpy的互相转化，减少.to(device)或.cuda()的出现，争取都在GPU上完成计算；减少forward中的非必要串行代码的次数(比如嵌套很多层for循环)等。</label></p></li><li><p><label><input type="checkbox" />4.检查完第三步，如果时间问题能解决，但是内存一直泄露怎么办？(<ahref="https://oier99.cn/posts/6492fdf5/#%E5%9B%9B.-pytoch-%E4%B8%AD%E8%8A%82%E7%BA%A6%E6%98%BE%E5%AD%98%E5%BC%80%E9%94%80%E7%9A%84%E4%B8%80%E4%BA%9B%E6%96%B9%E6%B3%95%E6%8C%81%E7%BB%AD%E6%B1%87%E6%80%BB">参考上一期</a>)。可以借助一些内存开销分析软件去判断是否出现内存泄漏。分析方法和时间分析一样，分别对dataloader和网络forward部分去做消融实验看看哪个导致内存持续增长。绝大多数内存泄露的本质原因是变量没能及时释放。dataloader部分常见泄漏原因：比如你的dataloader设计了一套很复杂的字典格式作为输出,然后这个字典由于各种原因(OpenCV或Pillow的API使用，导致变量一直不被释放；Transform的一些操作占用变量使用权的潜在bug等)每次循环都不能释放，那么增长曲线就会如下图所示(AutoDL资源看板)：<img src="https://s2.loli.net/2025/03/15/cgTvxLsoaOREmbu.png"alt="mem_leak" /></label></p><p>但是只要简单的在每次iteration末尾加上<code>del inputs, outputs, losses</code>把占用内存多的变量及时删除释放，曲线立马就变成如下了：<img src="https://s2.loli.net/2025/03/15/neGiwoYa14HxdsR.png"alt="steady_mem" /></p></li><li><p><label><input type="checkbox" />5. User:Pytorch训练神经网络时发现有明显的内存升高现象，可能有内存泄漏，该如何处理呢Kimi: 以下是处理 PyTorch训练神经网络时内存泄漏问题的一些方法：</label></p></li></ul><blockquote><h3 id="排查内存泄漏位置">排查内存泄漏位置</h3><ul><li><strong>注释代码</strong>：将数据送入模型的代码全部注释掉，仅保留数据加载部分，观察显存是否上涨。如果上涨，说明内存泄漏出现在<code>DataLoader</code>部分。若不上涨，再逐步取消注释模型相关代码，定位问题所在。</li><li><strong>分步注释</strong>：在模型训练过程中，分别注释掉输入数据的不同组成部分（如图像、标签、掩码等），观察注释掉哪部分时显存不再泄漏，从而确定具体导致内存泄漏的数据部分。</li></ul><h3 id="释放内存">释放内存</h3><ul><li><strong>删除张量</strong>：在不再需要某些张量时，使用<code>del</code> 关键字将其删除，例如<code>del tensor</code>，以释放其占用的内存。</li><li><strong>清理缓存</strong>：调用<code>torch.cuda.empty_cache()</code> 函数来清理已经删除的张量占用的 GPU内存。</li><li><strong>使用<code>torch.no_grad()</code></strong>：在推理阶段或不需要计算梯度的部分，使用<code>torch.no_grad()</code>上下文管理器来避免保存计算图，从而减少内存使用。</li></ul><h3 id="优化数据加载与处理">优化数据加载与处理</h3><ul><li><strong>优化 <code>DataLoader</code></strong>：检查<code>DataLoader</code> 的使用，确保其返回的数据类型正确。在<code>Dataset</code> 的 <code>__getitem__</code>方法中，返回基础数据类型而不是张量，然后在 <code>collate_fn</code>中进行转换。</li><li><strong>避免不必要的数据复制</strong>：在数据预处理过程中，避免不必要的数据复制操作，如对大数组进行切片等，这可能会导致内存占用增加。</li><li><strong>使用高效的数据结构</strong>：在处理序列数据时，使用更高效的张量操作，如<code>torch.nn.utils.rnn.pack_sequence</code> 和<code>torch.nn.utils.rnn.pad_sequence</code>，以减少内存使用。</li></ul><h3 id="优化模型训练过程">优化模型训练过程</h3><ul><li><strong>减少模型复杂度</strong>：如果可能，可以尝试简化模型结构或使用更小的批次大小（batchsize），以降低内存消耗。</li><li><strong>使用显存优化库</strong>：例如，使用 PyTorch 的<code>torch.utils.checkpoint</code> 模块来优化大模型的显存使用。</li><li><strong>正确处理循环引用</strong>：对于复杂模型，可能存在循环引用的问题。这种情况下，可能需要调整模型的架构，避免不必要的存储和引用。</li></ul><h3 id="使用工具辅助分析">使用工具辅助分析</h3><ul><li><strong>PyTorch Profiler</strong>：使用 PyTorch 提供的 Profiler工具来分析模型的性能和内存使用情况，找出可能导致内存泄漏的瓶颈。</li><li><strong>内存分析工具</strong>：使用如 NVIDIA 的 Nsight Compute等工具来可视化 PyTorch 的显存使用情况，帮助定位内存泄漏的源头。</li></ul><p>通过以上方法，可以有效排查和解决 PyTorch训练过程中出现的内存泄漏问题，提高训练的效率和稳定性。</p></blockquote><h2 id="七图像文件读取的bug修复">七、图像文件读取的bug修复</h2><ul><li><p>PIL.IMAGE读取png文件时，如果遇到PIL打不开但是线下双击文件能打开的，多半是遇到文件部分损坏的情况。可以加上如下代码或者写个if语句特判一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> ImageFile<br><br>ImageFile.LOAD_TRUNCATED_IMAGES = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure></li><li><p>cv2.imread()也有可能遇到读取失败报错<code>libpng error: Read Error</code>，但是线下双击能打开，可以采用如下方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2, random<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> ImageFile<br><span class="hljs-keyword">import</span> imghdr<br><br>ImageFile.LOAD_TRUNCATED_IMAGES = <span class="hljs-literal">True</span><br><span class="hljs-keyword">if</span> imghdr.what(name) == <span class="hljs-string">&quot;png&quot;</span>:<br>    Image.<span class="hljs-built_in">open</span>(name).convert(<span class="hljs-string">&quot;RGB&quot;</span>).save(name)<br>img = cv2.imread(name)<br>img = np.array(Image.<span class="hljs-built_in">open</span>(name))<br><br></code></pre></td></tr></table></figure><p><code>Image.save()</code>的方法是可以直接覆盖同名文件的。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>编程学习</category>
      
      <category>numpy学习</category>
      
      <category>随笔</category>
      
      <category>Pytorch</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>LaTeX讲解系列(4)： LaTex Preamble区中usepackage常用总结</title>
    <link href="/posts/f55a32b3/"/>
    <url>/posts/f55a32b3/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="latex讲解系列4-latex-preamble区中usepackage常用总结">LaTeX讲解系列(4)：LaTex Preamble区中usepackage常用总结</h1><ol start="0" type="1"><li><p>这些必须引：</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs LaTex"><span class="hljs-keyword">\usepackage</span>&#123;graphicx&#125;<br><span class="hljs-keyword">\usepackage</span>&#123;amsmath&#125;<br></code></pre></td></tr></table></figure></li><li><p>使用<code>\cref{}</code>时要引用<code>\usepackage{cleveref}</code>。而且这个命令的位置<strong>一定一定</strong>要在定义/引用所有的包的最后再import进来，不然会出现奇怪的问题(无法正常超链接等)。举个栗子，比如要在<code>hyperref</code>之后<code>usepackage</code>进来。</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs LaTex"><span class="hljs-comment">% ......</span><br><span class="hljs-keyword">\definecolor</span>&#123;cvprblue&#125;&#123;rgb&#125;&#123;0.21,0.49,0.74&#125;<br><span class="hljs-keyword">\usepackage</span>[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]&#123;hyperref&#125;<br><br><span class="hljs-comment">%%%%%%%%% ADD CREF AT LAST, ESPECIALLY AFTER HYPERREF</span><br><span class="hljs-keyword">\usepackage</span>&#123;cleveref&#125;<br><span class="hljs-comment">% add support for algorithm env</span><br><span class="hljs-keyword">\crefname</span>&#123;algorithm&#125;&#123;Alg.&#125;&#123;Algs.&#125;<br><span class="hljs-keyword">\Crefname</span>&#123;algorithm&#125;&#123;Alg.&#125;&#123;Algs.&#125;<br><br><span class="hljs-keyword">\crefname</span>&#123;figure&#125;&#123;Fig.&#125;&#123;Figs.&#125;<br><span class="hljs-keyword">\Crefname</span>&#123;figure&#125;&#123;Fig.&#125;&#123;Figs.&#125;<br><br><span class="hljs-keyword">\crefname</span>&#123;table&#125;&#123;Tab.&#125;&#123;Tabs.&#125;<br><span class="hljs-keyword">\Crefname</span>&#123;table&#125;&#123;Tab.&#125;&#123;Tabs.&#125;<br><br><span class="hljs-keyword">\crefname</span>&#123;align&#125;&#123;Eq.&#125;&#123;Eqs.&#125;<br><span class="hljs-keyword">\Crefname</span>&#123;align&#125;&#123;Eq.&#125;&#123;Eqs.&#125;<br><br><span class="hljs-keyword">\crefname</span>&#123;equation&#125;&#123;Eq.&#125;&#123;Eqs.&#125;<br><span class="hljs-keyword">\Crefname</span>&#123;equation&#125;&#123;Eq.&#125;&#123;Eqs.&#125;<br></code></pre></td></tr></table></figure><p>其中<code>\crefname</code>和<code>\Crefname</code>是用于自动对相应的环境替换为自己想要的缩写的，比如引用到公式环境时替换为<code>Eq. (编号)</code>等。</p></li><li><p>使用表格时把这几个常用的包引上：</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-comment">% This package is useful for creating more readable and organized tables by allowing flexible content formatting within cells.It allows you to:</span><br><span class="hljs-comment">% Line Breaks in Cells: Use \makecell&#123;&#125; to create line breaks within a single cell.</span><br><span class="hljs-comment">% Custom Alignment: Easily control horizontal and vertical alignment.</span><br><span class="hljs-comment">% Multi-line Headers: Create complex table headers with multiple lines.</span><br><span class="hljs-keyword">\usepackage</span>&#123;makecell&#125;<br><br><br><span class="hljs-comment">% Underlining: Use \uline&#123;text&#125; for a single underline.</span><br><span class="hljs-comment">% Double Underlining: Use \uuline&#123;text&#125; for a double underline.</span><br><span class="hljs-comment">%Strikethrough: Use \sout&#123;text&#125; for a strikethrough.</span><br><span class="hljs-comment">% Wave Underline: Use \uwave&#123;text&#125; for a wavy underline.</span><br><span class="hljs-comment">% 比如表格里比较Performance的时候，最好的结果加粗，第二好的结果下划线。</span><br><span class="hljs-keyword">\usepackage</span>[normalem]&#123;ulem&#125;<br><br><br><span class="hljs-comment">% supertabular:Allows tables to span multiple pages.Usage: Similar to the standard tabular environment, but can handle longer tables that need to continue onto subsequent pages.</span><br><span class="hljs-comment">% booktabs:Provides enhanced table rules (lines) for professional-quality tables. Offers commands like \toprule, \midrule, and \bottomrule for creating aesthetically pleasing horizontal lines without vertical lines, following typesetting rules for better readability.</span><br><span class="hljs-keyword">\usepackage</span>&#123;supertabular,booktabs&#125; <br></code></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <categories>
      
      <category>LaTex学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LaTex</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔：Markdown小技巧</title>
    <link href="/posts/4eba5852/"/>
    <url>/posts/4eba5852/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="随笔markdown-小技巧">随笔：Markdown 小技巧</h1><h2 id="图像视频导入">图像/视频导入</h2><h3 id="多图一行内并列插入">多图一行内并列插入</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">align</span>=<span class="hljs-string">&quot;center&quot;</span>&gt;</span><br>   <span class="hljs-tag">&lt;<span class="hljs-name">img</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;XXX.png&quot;</span>  <span class="hljs-attr">height</span>=<span class="hljs-string">160</span>&gt;</span><br>   <span class="hljs-tag">&lt;<span class="hljs-name">img</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;XXX.png&quot;</span> <span class="hljs-attr">height</span>=<span class="hljs-string">160</span>&gt;</span><br>   <span class="hljs-tag">&lt;<span class="hljs-name">img</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;https://s2.loli.net/2023/08/24/GJUPziVsu9SIj2X.png&quot;</span> <span class="hljs-attr">height</span>=<span class="hljs-string">160</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br></code></pre></td></tr></table></figure><p>不过要注意控制大小，否则一行放不下</p><h3 id="mp4视频导入方法">mp4视频导入方法</h3><p>参考链接: https://zhuanlan.zhihu.com/p/350887044</p><h5 id="方法1">方法1</h5><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-comment">&lt;!-- mp4格式 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">video</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;video&quot;</span> <span class="hljs-attr">controls</span>=<span class="hljs-string">&quot;&quot;</span> <span class="hljs-attr">preload</span>=<span class="hljs-string">&quot;none&quot;</span> <span class="hljs-attr">poster</span>=<span class="hljs-string">&quot;封面&quot;</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">source</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;mp4&quot;</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;mp4格式视频&quot;</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;video/mp4&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">videos</span>&gt;</span><br><br><span class="hljs-comment">&lt;!-- webm格式 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">video</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;video&quot;</span> <span class="hljs-attr">controls</span>=<span class="hljs-string">&quot;&quot;</span> <span class="hljs-attr">preload</span>=<span class="hljs-string">&quot;none&quot;</span> <span class="hljs-attr">poster</span>=<span class="hljs-string">&quot;封面&quot;</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">source</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;webm&quot;</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;webm格式视频&quot;</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;video/webm&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">videos</span>&gt;</span><br><br><span class="hljs-comment">&lt;!-- ovg格式 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">video</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;video&quot;</span> <span class="hljs-attr">controls</span>=<span class="hljs-string">&quot;&quot;</span> <span class="hljs-attr">preload</span>=<span class="hljs-string">&quot;none&quot;</span> <span class="hljs-attr">poster</span>=<span class="hljs-string">&quot;封面&quot;</span>&gt;</span><br>      <span class="hljs-tag">&lt;<span class="hljs-name">source</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;ogv&quot;</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;ogv格式视频&quot;</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;video/ogv&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">videos</span>&gt;</span><br></code></pre></td></tr></table></figure><h5 id="方法2">方法2</h5><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">align</span>=<span class="hljs-string">&quot;center&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">iframe</span> </span><br><span class="hljs-tag"><span class="hljs-attr">src</span>=<span class="hljs-string">&quot;README.assets\video_view_compressed.mp4&quot;</span></span><br><span class="hljs-tag"><span class="hljs-attr">scrolling</span>=<span class="hljs-string">&quot;no&quot;</span> </span><br><span class="hljs-tag"><span class="hljs-attr">border</span>=<span class="hljs-string">&quot;0&quot;</span> </span><br><span class="hljs-tag"><span class="hljs-attr">frameborder</span>=<span class="hljs-string">&quot;no&quot;</span> </span><br><span class="hljs-tag"><span class="hljs-attr">framespacing</span>=<span class="hljs-string">&quot;0&quot;</span> </span><br><span class="hljs-tag"><span class="hljs-attr">allowfullscreen</span>=<span class="hljs-string">&quot;true&quot;</span> </span><br><span class="hljs-tag"><span class="hljs-attr">height</span>=<span class="hljs-string">372</span> </span><br><span class="hljs-tag"><span class="hljs-attr">width</span>=<span class="hljs-string">900</span>&gt;</span> <br><span class="hljs-tag">&lt;/<span class="hljs-name">iframe</span>&gt;</span><br>This is a video.<br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br></code></pre></td></tr></table></figure><h5 id="方法3">方法3</h5><p>视频转化为gif格式, 直接当作图片插入markdown中</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Markdown技巧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔：基于大模型机器智能的比较思考</title>
    <link href="/posts/ca0bbe03/"/>
    <url>/posts/ca0bbe03/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="随笔基于大模型机器智能的比较思考">随笔：基于大模型机器智能的比较思考</h1><p>人脑和ChatGPT有和本质区别呢？</p><p>机器智能遵循的是基于计算的”暴力美学”，从一个非常简单的基础单元出发，从简至繁，bottom-up，只要疯狂堆叠拟合一切从而能到全知全能的结果。</p><p>人脑智能遵循的是”规律美学”，我们从大自然纷繁复杂的现象中不断总结规律，不论从几千年的农活中总结二十四节气的顺口溜，还是从复杂的自然现象中抽象成一个简单的公式，我们喜欢化繁为简，走top-down的思考方式。</p><p>More isdifferent。在能量充足的前提下，简单整齐数量多的系统，其进化速度会最终会快速超过小而精美的复杂系统。或许这是一种普世的系统学或哲学规律。就比如看记录片里，二战时单个苏联坦克干不过单个德国坦克(记不清型号了)，但是苏联坦克耐操，价格低廉，生产的快数量多最终还是能干过德国的豹式坦克。</p><p>但其实我们人类智能或许有优势的地方，就是基于大模型的智能可能会消耗超级多的能源和资源;在地球的能量和资源总是有限的情况下，更有利于精美复杂的系统进化也可能正因为如此我们才从草履虫进化为人复杂精美的高级智能(每小时消耗60W的人脑)，而非简单智能体的海量堆叠。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>机器智能</tag>
      
      <tag>人脑智能</tag>
      
      <tag>通用AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔：看《海上钢琴师》有感小记</title>
    <link href="/posts/45543150/"/>
    <url>/posts/45543150/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="随笔看海上钢琴师有感小记">随笔：看《海上钢琴师》有感小记</h1><p>原载：2019年12月23日，手机备忘录</p><blockquote><p>4年后偶然翻到，感慨于自己当时残存的一些文艺细胞，学习大文豪列夫·托尔斯泰的写作习惯，读书看电影出门远游的所感所想，一定会记在手机备忘录里点点滴滴积累自己的文学素材。新冠疫情之后，时过境迁，昔日有真义，如今已忘言，特此小记。</p></blockquote><p>好的想法怎么可能会那么轻易地钻入你的脑子里来?想法不会一蹴而就，而是像蜜蜂采蜜酿蜜一般积累起来的</p><p>就像波澜壮阔的大海无论多么壮美，来到大陆边缘的海浪都会平息––海上钢琴师</p><p>有那么一种人一直活在为自己量身打造的世界里，非常渴望自己被世界遗忘。</p><p>1900与“楚门的世界”的主人公有着相似的生活背景，但最终恰好做出了截然相反的抉择</p><p>爱情，尤其是第一眼怦然心动的感觉，本应是一笔激发我们走进世界里的强大吸引。</p><p>前四个舒缓的音符，仿佛走神后指尖在不经意第顺序滑过，恰似二人目光相相撞时心脏怦然一跳；相遇的目光再也舍不得离开。旋律慢慢上扬，加重，心跳慢慢加速，振奋，目光逐渐痴恋。天籁般起伏的旋律，正是那第一次心动的感觉。手下无心弹奏的音符，就像山岩间淌出的涓涓细流，一下子淌进了一颗轻柔的心里。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>《海上钢琴师》</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔：AI 顶会论文好玩的标题系列</title>
    <link href="/posts/e9d4a375/"/>
    <url>/posts/e9d4a375/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="随笔ai-顶会论文好玩的标题系列">随笔：AI顶会论文好玩的标题系列</h1><ol type="1"><li><p>XXX is (Not) All Your Need: XXXXXXX</p><ul><li>举例：Attention is All Your Need; Money is All Your Need; NaiveBayes is All Your Need; Transformer is Not All Your Need.</li></ul></li><li><p>XXX Matters: XXXXXXX</p><ul><li>举例：Fusion Matters: A Simple but Effective Cross-Modality FusionProxy.(瞎编的哈)</li></ul></li><li><p>XXXFormer 乱七八糟数不胜数。</p></li><li><p>Make it XXX: XXX</p><ul><li>举例：Make it See, Make it Talk，Make it Draw, Make it Walk, Make itDrive…</li></ul></li><li><p>XXX Anything 主打一个暴力美学</p><ul><li>举例：Segment Anything，Detect Anything, Plan Anything…主打全知全能通用性</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文标题</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hexo兼容性问题Fix小记</title>
    <link href="/posts/eb6bb29/"/>
    <url>/posts/eb6bb29/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="hexo-fluid兼容性问题fix小记">Hexo-Fluid兼容性问题Fix小记</h1><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="///music.163.com/outchain/player?type=2&amp;id=22707001&amp;auto=0&amp;height=66"></iframe><h2 id="背景">背景</h2><p>自从2023年开始好久没有捣鼓自己的博客了，也一直没有更新兼容环境。偶然七月的一天发现自己blog里很多公式显示不出来，此外还有很多兼容性错误，于是在此立下一个记录，防止以后迁移到其他工作电脑时再出现兼容性问题。</p><h2 id="定期更新">定期更新</h2><p>还是要记住定期更新hexo-fluid-theme</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm update --save hexo-theme-fluid<br></code></pre></td></tr></table></figure><p>更新<code>hexo-math</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm update --save hexo-math<br></code></pre></td></tr></table></figure><p>更新pandoc: 去githun官方repo看<ahref="https://github.com/jgm/pandoc/releases">release</a>,Windows用户最好更新在系统目录下(for Allusers)，并且<code>_config.yml</code>中固定下pandoc的路径为:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">pandoc:</span><br>    <span class="hljs-attr">pandoc_path:</span> <span class="hljs-string">C:/Program</span> <span class="hljs-string">Files/Pandoc/pandoc.exe</span><br></code></pre></td></tr></table></figure><p>不然<code>hexo g</code>渲染时会出一些奇怪的Error。</p><p>(未完待续……)</p><h2 id="latex公式问题">Latex公式问题</h2><ol type="1"><li><p>尝试Hexo-Fluid主题内置公式插件，渲染结果一直时好时坏。。。最后一狠心全部换成<code>hexo-math</code>的公式插件。详情见<ahref="https://hexo.fluid-dev.com/docs/guide/#latex-%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F">这里</a>。</p></li><li><p>感谢New Bing 教会了我如何使用正则表达式替换<code>$E=mc^2$</code>为<code>$$ E=mc^2 $$</code>。手工替换500+<code>$</code>差点人废了，才想起NewBing查一下。。。（AIGC是程序猿的未来。。。）</p><blockquote><p>Q:</p><p>如何在MarkDown中自动替换<code>$$</code>为<code>$$</code> 和<code>$$</code></p></blockquote><blockquote><p>A:</p><p>你可以使用正则表达式来实现这个功能。例如，你可以使用下面的正则表达式来匹配两个美元符号之间的内容：</p><figure class="highlight django"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs django"><span class="language-xml">\$\$([^\$]*)\$\$</span><br></code></pre></td></tr></table></figure><p>然后，你可以使用下面的代码来替换它：</p><figure class="highlight django"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs django"><span class="language-xml">$$$1$$</span><br></code></pre></td></tr></table></figure><p>这将把两个美元符号之间的内容替换为 <code>$$</code> 和 <code>$$</code>标记。</p><p>希望这能帮到你。如果你还有其他问题，请告诉我。</p></blockquote><blockquote><p>Q:</p><p>正则表达式中$1代表什么意思?</p></blockquote><blockquote><p>A:</p><p>在正则表达式中，$1表示第一个捕获组的内容。捕获组是由括号括起来的正则表达式的一部分，它们可以用来匹配和提取文本。例如，如果你使用下面的正则表达式：</p><figure class="highlight django"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs django"><span class="language-xml">(\d&#123;3&#125;)-(\d&#123;4&#125;)</span><br></code></pre></td></tr></table></figure><p>它将匹配一个由连字符分隔的电话号码，其中第一个捕获组将匹配前三个数字，第二个捕获组将匹配后四个数字。如果你在替换字符串中使用$1 和 $2，它们将分别被替换为第一个和第二个捕获组的内容。</p></blockquote></li><li><p>目前探索的，行内公式直接替换为:</p><figure class="highlight django"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs django"><span class="language-xml">$$$1$$</span><br></code></pre></td></tr></table></figure><p>行间公式替换为</p><figure class="highlight django"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs django"><span class="language-xml">   $$</span><br><span class="language-xml">$1</span><br><span class="language-xml">$$</span><br></code></pre></td></tr></table></figure><p>其他说明配置写在<code>_config.yml</code>里，如下</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># hexo-math https://github.com/hexojs/hexo-math</span><br><span class="hljs-attr">math:</span><br>  <span class="hljs-attr">katex:</span><br>    <span class="hljs-attr">css:</span> <span class="hljs-string">&#x27;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css&#x27;</span><br>    <span class="hljs-attr">options:</span><br>      <span class="hljs-attr">throwOnError:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">mathjax:</span><br>    <span class="hljs-attr">css:</span> <span class="hljs-string">&#x27;https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css&#x27;</span><br>    <span class="hljs-attr">options:</span><br>      <span class="hljs-attr">conversion:</span><br>        <span class="hljs-attr">display:</span> <span class="hljs-literal">false</span><br>        <span class="hljs-attr">scale:</span> <span class="hljs-number">1.5</span><br>      <span class="hljs-attr">tex:</span><br>        <span class="hljs-attr">tags:</span> <span class="hljs-string">&#x27;none&#x27;</span><br>      <span class="hljs-attr">svg:</span><br></code></pre></td></tr></table></figure><p>配置项如何搞参看该<ahref="https://github.com/hexojs/hexo-math">README</a></p></li><li><p>Markdown中插入<ahref="https://hexo.fluid-dev.com/docs/guide/#tag-%E6%8F%92%E4%BB%B6">tag</a>的时候，不太支持tag之间以及tag和hexo-math插件之前互相嵌套。因此非常建议利用html的方式来实现，可插入便签(note)如下:</p></li></ol><p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;note note-primary&quot;</span>&gt;</span><br>    紫色标签<br><span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></td></tr></table></figure></p><p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;note note-secondary&quot;</span>&gt;</span><br>    灰色标签<br><span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></td></tr></table></figure></p><p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;note note-success&quot;</span>&gt;</span><br>    绿色标签<br><span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></td></tr></table></figure></p><p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;note note-danger&quot;</span>&gt;</span><br>    红色标签<br><span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></td></tr></table></figure></p><p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;note note-warning&quot;</span>&gt;</span><br>    黄色标签<br><span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></td></tr></table></figure></p><p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;note note-info&quot;</span>&gt;</span><br>    蓝色标签<br><span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></td></tr></table></figure></p><p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;note note-light&quot;</span>&gt;</span><br>    黑色标签<br><span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></td></tr></table></figure></p><p class="note note-warning">注意用html标签括起来的note中间可以换行，但不能隔着一行及以上(即Markdown中的分段)，有空行会显示错误。</p><p>改变为行内标签的方式，则为</p><p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">span</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;label label-primary&quot;</span>&gt;</span>Label<span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
    <tags>
      
      <tag>兼容性问题</tag>
      
      <tag>Fluid</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔：一些DL编程中的小&quot;寄&quot;巧</title>
    <link href="/posts/6492fdf5/"/>
    <url>/posts/6492fdf5/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="近期一些dl编程中的小寄巧">近期一些DL编程中的小”寄”巧</h1><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="//music.163.com/outchain/player?type=2&amp;id=411314184&amp;auto=0&amp;height=66"></iframe><h2 id="一.-python-numpy-tofilefromfilesaveload">一. python numpy –tofile，fromfile，save，load</h2><p>参考资料:https://blog.csdn.net/weixin_39087379/article/details/118048780</p><iframe src="https://blog.csdn.net/weixin_39087379/article/details/118048780" width="100%" height="500" name="topFrame" scrolling="yes" noresize="noresize" frameborder="0" id="topFrame"></iframe><p>笔记：</p><ol type="a"><li><code>tofile</code>和<code>fromfile</code>是一对，二进制读写进文件。由于不保存数组形状和元素数据格式等信息，因此一定要设置正确的<code>dtype</code>参数，并且<code>reshape</code>正确的形状。</li></ol><p>举例，SemanticKitti的语义标签数据读取<code>np.fromfile(path, dtype=np.uint32)</code>。当时犯了2个错：</p><ul><li>首先使用<code>np.load</code>读取的<code>.label</code>格式文件，导致读入的Ground-Truth标签一直有误，且数量和点云数量对不上。</li><li>换成<code>np.fromfile</code>后，<code>dtype</code>忘记设置导致格式不对。除此之外,NuScenes的label也需要<code>np.fromfile</code>打开.一般来说,<code>tofile</code>和<code>fromfile</code>处理的是<code>.bin</code>后缀的文件,<code>load</code>和<code>save</code>处理的时<code>.npy</code>后缀的文件。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">f=<span class="hljs-string">&#x27;xxx.fea&#x27;</span><br>arr1.astype(<span class="hljs-string">&#x27;float&#x27;</span>).tofile(f)<br>arr2=np.fromfile(f, dtype=<span class="hljs-string">&#x27;float&#x27;</span>) <span class="hljs-comment">#必须设置dtype且和保存时一致，否则结果错误</span><br></code></pre></td></tr></table></figure><ol start="2" type="a"><li><code>save</code>和<code>load</code>是一对，是<code>Numpy</code><strong>专用的</strong>二进制格式保存数据，可以自动处理元素类型和形状信息。之前犯过的错误有以下几种:</li></ol><ul><li><code>np.load</code>下意识的设置<code>dtype</code>反而报错</li><li>错把普通二进制文件用<code>np.load</code>打开导致数据错误</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">np.save(f，a)<br>np.load(f)<br></code></pre></td></tr></table></figure><p>此外<code>numpy.savez_compressed</code>可以把多个array存到一个文件中并压缩.如下面例子:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>test_array = np.random.rand(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>test_vector = np.random.rand(<span class="hljs-number">4</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.savez_compressed(‘/tmp/<span class="hljs-number">123</span>’, a=test_array, b=test_vector) <span class="hljs-comment"># Arrays to save to the file. Please use keyword arguments (see kwds below) to assign names to arrays. Arrays specified as args will be named “arr_0”, “arr_1”, and so on.</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>loaded = np.load(‘/tmp/<span class="hljs-number">123.</span>npz’)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(np.array_equal(test_array, loaded[‘a’]))<br><span class="hljs-literal">True</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(np.array_equal(test_vector, loaded[‘b’]))<br><span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><ol start="3" type="a"><li><code>savetxt()</code>和<code>loadtxt()</code>,读写1维和2维数组的文本文件；也可以用它们读写CSV格式的文本文件</li></ol><p>例子:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.arrange(<span class="hljs-number">0</span>, <span class="hljs-number">12</span>, <span class="hljs-number">0.5</span>).reshape(<span class="hljs-number">4</span>, -<span class="hljs-number">1</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.savetxt(<span class="hljs-string">&quot;a.txt&quot;</span>, a) <span class="hljs-comment"># 默认按照`%.18e`格式保存数据,以空格分隔</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>np.loadtxt(<span class="hljs-string">&quot;a.txt&quot;</span>)<br>...<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.savetxt(<span class="hljs-string">&quot;b.txt&quot;</span>, a, fmt=<span class="hljs-string">&quot;%d&quot;</span>, delimiter=<span class="hljs-string">&quot;,&quot;</span>) <span class="hljs-comment"># 改为整数格式保存,逗号分隔</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>np.loadtxt(<span class="hljs-string">&quot;b.txt&quot;</span>, delimiter=<span class="hljs-string">&quot;,&quot;</span>) <span class="hljs-comment"># 读入时也需要指定都好分隔</span><br>...<br></code></pre></td></tr></table></figure><h2 id="二.-关于pytorch分布式训练distributeddataparallel中的一些坑">二.关于Pytorch分布式训练DistributedDataParallel中的一些坑</h2><p>说明:博主目前还在用着落后的<code>python -m torch.distributed.launch -nproc_per_node=8 XXX.py</code>的方式启动,还没学pytorch官方推荐的<code>torchrun</code></p><p>此外不太理解DDP的小伙伴可以先读一下<ahref="https://blog.csdn.net/magic_ll/article/details/122359490">博客</a>理解一下</p><ol type="1"><li><p>如果你的神经网络中有batchnorm层,一定要在模型从cpu移到gpu前(<code>my_model.cuda()</code>)加上这一句,让batchnorm参数在各个节点之间建立同步关系.</p><blockquote><p>Currently SyncBatchNorm only supports DistributedDataParallel<strong>with single GPU per process</strong>. Usetorch.nn.SyncBatchNorm.convert_sync_batchnorm() to convert BatchNormlayer to SyncBatchNorm before wrapping Network with DDP.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># DDP的sync_bn，让多卡训练的bn范围正常</span><br>    <span class="hljs-keyword">if</span> args.local_rank != -<span class="hljs-number">1</span>:<br>        my_model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(my_model)<br>    my_model.cuda()<br>    <span class="hljs-keyword">if</span> args.local_rank != -<span class="hljs-number">1</span>:<br>        my_model = torch.nn.parallel.DistributedDataParallel(my_model, device_ids=[args.local_rank],<br>                                                             output_device=args.local_rank,)<br></code></pre></td></tr></table></figure><p>参考资料: pytorch官方文档自己去搜</p></li><li></li></ol><table><thead><tr><th style="text-align: center;"></th><th style="text-align: center;">Sampler</th><th style="text-align: center;">Dataloader</th></tr></thead><tbody><tr><td style="text-align: center;">单卡</td><td style="text-align: center;">torch.utils.data.sampler.Sampler</td><td style="text-align: center;">torch.utils.data.DataLoader</td></tr><tr><td style="text-align: center;">Distributed Parallel</td><tdstyle="text-align: center;">torch.utils.data.distributed.DistributedSampler(train_dataset)<br /></td><td style="text-align: center;">torch.utils.data.DataLoader</td></tr></tbody></table><p>其实Pytorch是通过继承Sampler的不同子类,来实现对DIstributedParallel的支持.Dataloader提供的接口时一样的(做个比喻: 仓库还是同样的仓库,只不过从原来的一个worker干活变成多个worker分头干活,所需要的调度器sampler不同).</p><p>这里就有一个坑: <code>train_sampler.set_epoch(epoch)</code>这一项一定要在dataloader的实例生成迭代器(iterator)之前设置好,否则无法在多个节点上通过sampler实现shuffle数据的功能(前提是<code>torch.utils.data.DataLoader(my_dataset, sampler=train_sampler)</code>设置好的基础上,因为<code>sampler</code>和<code>shuffle=True</code>不能同时设置).例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">train_dataset_loader = torch.utils.data.DataLoader(dataset=train_dataset,<br>                                                   batch_size=train_batch_size,<br>                                                   collate_fn=collate_fn_BEV,<br>                                                   sampler=train_sampler,<br>                                                   pin_memory=<span class="hljs-literal">True</span>,<br>                                                   num_workers=num_worker)<br>epoch = <span class="hljs-number">0</span><br><span class="hljs-keyword">while</span> epoch &lt; <span class="hljs-number">20</span>:<br>    epoch += <span class="hljs-number">1</span><br>train_sampler.set_epoch(epoch) <span class="hljs-comment"># 一定要在enumerate(train_dataset_loader)之前,不要放在while循环的末尾</span><br><span class="hljs-keyword">for</span> i_iter, train_dict <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dataset_loader):<br>    ...<br></code></pre></td></tr></table></figure><p>补充一个小福利:https://github.com/huggingface/transformers中好像有<code>SequentialDistributedSampler</code>的实现,好像很多人都在使用.在分布式并行时,这个类用于数据集的Eval和Test顺序读取比较好用.</p><details><summary>Demo代码–快来戳我┗|｀O′|┛ 嗷<sub></sub></summary><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SequentialDistributedSampler</span>(torch.utils.data.sampler.Sampler):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Distributed Sampler that subsamples indicies sequentially,</span><br><span class="hljs-string">    making it easier to collate all results at the end.</span><br><span class="hljs-string">    Even though we only use this sampler for eval and predict (no training),</span><br><span class="hljs-string">    which means that the model params won&#x27;t have to be synced (i.e. will not hang</span><br><span class="hljs-string">    for synchronization even if varied number of forward passes), we still add extra</span><br><span class="hljs-string">    samples to the sampler to make it evenly divisible (like in `DistributedSampler`)</span><br><span class="hljs-string">    to make it easy to `gather` or `reduce` resulting tensors at the end of the loop.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dataset, batch_size, rank=<span class="hljs-literal">None</span>, num_replicas=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-keyword">if</span> num_replicas <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed.is_available():<br>                <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&quot;Requires distributed package to be available&quot;</span>)<br>            num_replicas = torch.distributed.get_world_size()<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed.is_available():<br>                <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&quot;Requires distributed package to be available&quot;</span>)<br>            rank = torch.distributed.get_rank()<br>        <span class="hljs-variable language_">self</span>.dataset = dataset<br>        <span class="hljs-variable language_">self</span>.num_replicas = num_replicas<br>        <span class="hljs-variable language_">self</span>.rank = rank<br>        <span class="hljs-variable language_">self</span>.batch_size = batch_size<br>        <span class="hljs-variable language_">self</span>.num_samples = <span class="hljs-built_in">int</span>(math.ceil(<span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.dataset) * <span class="hljs-number">1.0</span> / <span class="hljs-variable language_">self</span>.batch_size / <span class="hljs-variable language_">self</span>.num_replicas)) * <span class="hljs-variable language_">self</span>.batch_size<br>        <span class="hljs-variable language_">self</span>.total_size = <span class="hljs-variable language_">self</span>.num_samples * <span class="hljs-variable language_">self</span>.num_replicas<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__iter__</span>(<span class="hljs-params">self</span>):<br>        indices = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.dataset)))<br>        <span class="hljs-comment"># add extra samples to make it evenly divisible</span><br>        indices += [indices[-<span class="hljs-number">1</span>]] * (<span class="hljs-variable language_">self</span>.total_size - <span class="hljs-built_in">len</span>(indices))<br>        <span class="hljs-comment"># subsample</span><br>        indices = indices[<span class="hljs-variable language_">self</span>.rank * <span class="hljs-variable language_">self</span>.num_samples : (<span class="hljs-variable language_">self</span>.rank + <span class="hljs-number">1</span>) * <span class="hljs-variable language_">self</span>.num_samples]<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">iter</span>(indices)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.num_samples<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">distributed_concat</span>(<span class="hljs-params">tensor, num_total_examples</span>):<br>    output_tensors = [tensor.clone() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(torch.distributed.get_world_size())]<br>    torch.distributed.all_gather(output_tensors, tensor)<br>    concat = torch.cat(output_tensors, dim=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># truncate the dummy elements added by SequentialDistributedSampler</span><br>    <span class="hljs-keyword">return</span> concat[:num_total_examples]<br></code></pre></td></tr></table></figure>用法 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">eval_sampler = SequentialDistributedSampler(val_dataset, val_batch_size)<br></code></pre></td></tr></table></figure></details><ol start="3" type="1"><li><p>分布式中节点间传递消息: 除了通信的API之外(<code>reduce</code>,<code>gather</code>, <code>all-gather</code>等), 有一种最简单最傻的方式,按照某个格式保存到本地文件,然后只让0号卡负责读取,其他的进程<code>torch.distributed.barrier()</code>.这种对于大量数据的传递比较好用. 应用实例:比如有一个自己手工作坊制造的类,在单卡上很好用,但是在多卡上你发现改写成分布式的太麻烦了(比如涉及到读写冲突,进程通信同步等问题)烦死了,你不想浪费宝贵的科研时间单纯改造这个类, 不妨试试上面的傻瓜方法.</p></li><li><p>分布式训练出来的权重再次load时候</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> args.resume:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;load&#x27;</span>)<br>        pretrained_model = torch.load(model_load_path)<br>        <span class="hljs-comment"># 消除分布式训练时在保存参数的时候多出来的module.</span><br>        weights_dict = &#123;&#125;<br>        <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> pretrained_model.items():<br>            new_k = k.replace(<span class="hljs-string">&#x27;module.&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>) <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;module&#x27;</span> <span class="hljs-keyword">in</span> k <span class="hljs-keyword">else</span> k<br>            weights_dict[new_k] = v<br>        <span class="hljs-comment"># # debug的时候查看参数量</span><br>        <span class="hljs-comment"># model_dict = my_model.state_dict()</span><br>        my_model.load_state_dict(weights_dict)<br></code></pre></td></tr></table></figure><p>— (2024.07更新) —</p><ol start="5" type="1"><li>在使用多卡推理的时候，建议每个DDP进程把自己负责的每个图片文件的推理结果都同步过来，类似</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">dist.all_gather_object(ddp_metric_list, single_node_metric_dict)<br></code></pre></td></tr></table></figure><p>来做多进程结果手机。这样多卡推理时metric计算误差最小，基本和单卡推理结果一模一样。不太建议比如每张卡先算个平均的分指标/求和算个总指标，然后只同步一个总的数字，然后再8个节点上算平均，这样误差大(甚至波动1个点)。</p><ol start="6" type="1"><li>注意tensorboardX使用summarywriter记录日志时,最好只用一个进程(比如rank0)来创建，或者每个进程日志创建地址名字结尾用进程名字作为隔离<code>(f"XXX_tb_process_{rank}")</code>，不然一直报错”FileExists”错误。</li><li>通过如下方式，只对rank0进程上使用tqdm避免显示混乱。<code>_data_loader = tqdm(_data_loader, ascii=True) if self.rank==0 else _data_loader</code></li><li>DDP多卡从obs远程下载/读取文件到云节点的Docker时，一定注意避免多个进程读/写同一个文件导致一致性冲突。比如如果在<code>__init__</code>阶段需要copy同一个文件到本地(8个进程都需要)，不如提前下载好到本地，然后执行代码；或者8个进程下载8次，每次文件名后缀用<code>f"_process_{rank}"</code>区分，，如果在<code>__get_item_</code>阶段读取文件，因为数据集会被8个进程完整分成8份互不相交的文件，不应该有读写冲突。所以关注不同文件名下的文件有没有重名现象，避免重名造成一些隐性的error。</li><li>注意算推理时间的时候，还是尽可能把可视化和文件读写关闭一下。因为关闭可视化和文件读写，有可能会省去网络模型的某些操作，算FPS和Latency时能更快。</li></ol><h2 id="三.-pytorch-中节约时间开销的一些方法持续汇总">三. Pytorch中节约时间开销的一些方法(持续汇总)</h2><ol type="1"><li><p>(最新，亲测非常好用)<code>torch.set_float32_matmul_precision("highest|high|medium")</code></p><ul><li><p>pytorch参考doc:直接搜<code>set_float32_matmul_precision</code></p></li><li><p>相关博客解释：https://blog.csdn.net/sikh_0529/article/details/131243173【使用混合精度技术加速大型语言模型】总体来说，就是Float32充分利用NVIDIA显卡中的TensorCore模块，原先做Tensor乘法完全用Float32的数据格式做，速度自然慢</p></li><li><p>性能对比：转自上述博客，开启high模式混合精度矩阵乘法，速度快几倍且精度能略微上涨。博客解释原因是1.深度网络对精度的适当下降，如从FL32变为BF32格式，不会有收敛性上的太大损失；2.精度适当的截取下降，配合上混合精度，可以给网络带来更多噪声，有利于防止过拟合，增强泛化能力。目前在自己代码上亲测效果，4090能加速5-6倍的训练速度，精度目前还在调bug中。</p></li><li><p>支持条件: <code>torch.cuda.is_bf16_supported()</code></p><p><imgsrc="https://s2.loli.net/2023/09/08/SinMrh4NFPKsELf.png" /></p></li></ul></li><li><p>总原则：CPU把算子和任务下发到CUDA上。尽可能不要让CUDA等CPU，CPU一次性把没有数据依赖的算子下发完毕，然后让cuda去算。(下发的斜线越倾斜越好，cudastream越满越好)</p></li><li><p>在前向传播中，尽量少出现.to(device),.cpu()等搬运数据的操作；可以用torch.arrange(<strong>,device=“cuda”)直接把数据放在cuda上，或torch.oneslike(</strong>,device=“cuda”)</p><blockquote><h3 id="avoid-unnecessary-cpu-gpu-synchronization">Avoid unnecessaryCPU-GPU synchronization</h3><p>Avoid unnecessary synchronizations, to let the CPU run ahead of theaccelerator as much as possible to make sure that the accelerator workqueue contains many operations.</p><p>When possible, avoid operations which require synchronizations, forexample:</p><ul><li><code>print(cuda_tensor)</code></li><li><code>cuda_tensor.item()</code></li><li>memory copies: <code>tensor.cuda()</code>,<code>cuda_tensor.cpu()</code> and equivalent<code>tensor.to(device)</code> calls</li><li><code>cuda_tensor.nonzero()</code></li><li>python control flow which depends on results of operations performedon cuda tensors e.g. <code>if (cuda_tensor != 0).all()</code></li><li>更多参见:https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html</li></ul></blockquote></li><li><p>有人建议把GPU显存利用的满一些(batchsize 开大一点).但是通过实验好像发现,对于20系和30系的N卡,有效显存大概6G~8G左右,也就是CUDA数量一次能够处理的最大显存容量.如果超过这个容量的数据还是会放在显存里.通过自己的代码多次实验发现,基本显存占用&gt;8G后,batch-size开的再大总时间也不会减少了.</p></li><li><p>图像用lmdb（不过不一定必须，当CPU利用率还有充足空间，硬盘不是太慢如本来就是SSD时，这个不一定是瓶颈）</p></li><li><p>自己profile, 找到代码的加速瓶颈</p></li><li><p>使用pytorch.amp混合精度加速(开O1)</p></li><li><p>AutoDL显卡租用平台的官方文档中：https://www.autodl.com/docs/perf/总结的非常好！此外，AutoDl还推荐了Pytorch官方的小技巧总结:https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html.</p></li></ol><h2 id="四.-pytoch-中节约显存开销的一些方法持续汇总">四. Pytoch中节约显存开销的一些方法(持续汇总)</h2><ol type="1"><li><p><code>torch.cuda.empty_cache()</code>,每跑完一个Epoch用一下,防止显存占用一直增长(好像在OpenMMLab家族系列的框架中包含了这一点)</p></li><li><p>稀疏化的tensor表达会省很多显存,尤其是在计算<code>CE_loss</code>的时候,稀疏化的<code>CE_loss</code>会给反向传播节省很多显存.</p></li><li><p>太大的变量不要一直等着自动回收,可以手工定期delete一下</p></li><li><p><code>torch.load(XXX.pt, map_location="cpu")</code>,可以避免在开多卡时,某张卡的显存占用明显比其他卡多2~3个G导致整个程序OOM.</p></li></ol><figure><img src="https://s2.loli.net/2023/02/12/chQd6PVY8qM5LjD.png"alt="Pytorch Doc 提示" /><figcaption aria-hidden="true">Pytorch Doc 提示</figcaption></figure><ol start="5" type="1"><li><p>发现的一个现象：pytorch中的dataset的num_workers，如果开少了可能CPU难以微保GPU，导致GPU空等CPU利用率不高；但是如果num_worker开的过高，比如一口气开到10以上(我有一次开到过32)，会出现:</p><ol type="1"><li>CPU核被占的非常多，如果这台机器就你自己用还好，如果多人用，别人的程序也会被你连累了，变得特别慢；</li><li>内存开销会陡然增加很多，会导致容易内存OOM，程序训练了几十个Epoch后不知道啥时候突然寄了，还没有报错原因，或者报错原因是signal9killed，因为超过内存被系统kill调了。如果是在物理机上，由于有硬盘可以作为虚拟内存的机制，或许能缓冲一下不会直接挂掉;但如果是在容器里环境基本立马被干掉。</li><li>pytorch中num_workers开多了会放大内存泄露的风险。某一次实验观察发现：某多卡DDP的训练程序，开了num_workers=4，看内存占用曲线图有少量的内存泄漏，但是训练300多个Epoch一共泄漏不到几十G；但是一旦开到num_workers=32，瞬间十几个Epoch后就泄露了100多G。而且这种num_workers的现象在mindspore框架最初1.1~1.5版本中也出现过内存泄漏现象。估计这都是深度学习框架的通病吧，不能开太大了。。。</li></ol><p>（2025.03.15 更新：<ahref="https://oier99.cn/posts/c6450e7b/">优雅的方法</a>使得batchsize更大，num_workers敢开更多，并行训练速度更快，且显存不泄露）</p><p>(4)补充：在Docker中，如果单卡训练且num_workers开的过大，可能会在遍历dataloader取数据的过程中，超过Docker默认的最大打开文件数量，会报错“[OSError26]: Open too many files.” 的错误而强制退出。</p></li></ol><h2 id="五.-pytorch-关于tensor下标操作的一些技巧">五. Pytorch关于Tensor下标操作的一些技巧</h2><p>常用的一些函数如:<code>torch.all(), torch.nonzero(), torch.where(), torch.masked_fill(), torch.masked_select(), torch.scatter_select()</code>等等，PyTorch提供了非常丰富的下标操作函数，尽可能让我们避免在<code>__forward__()</code>函数中使用for循环，因为for循环需要CPU串行参与计算，无法完全实现在GPU上并行化的计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># Part 1 使用torch.all()和torch.nonzero()选取值所在的下标索引</span><br><span class="hljs-comment">#torch.all(input, dim, keepdim=False, *, out=None)，判断一个张量的某一维度的某一行是否全部为true.最常见的用法是torch.all(a==c, dim=XXX), 判断某个某一个向量是否存在于该张量的某一行中。</span><br><span class="hljs-comment">#:param value: Tensor(M,) a one-dimension vector of M elements</span><br>value = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]) <span class="hljs-comment"># (1,M)</span><br><span class="hljs-comment">#:param t: Tensor(N,M) a two-dimension tensor of N vectors, each vector has M elements</span><br>t = torch.tensor([[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]])<br>condition = torch.<span class="hljs-built_in">all</span>(t == value, dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(condition)<br><span class="hljs-comment"># tensor([False, False, True, False, True])</span><br><span class="hljs-comment"># 此后在用torch.nonzero(input, *, out=None, as_tuple=False), 比如输入一个n维tensor，会返回一个2-D tensor (z x n)的结果，z表示tensor中一共有z个非0元素，每一行是一个n维向量，代表该非0值所在的下标索引时(注意是as_tuple=False的时候)。</span><br>index = torch.nonzero(condition)<br><span class="hljs-built_in">print</span>(index)<br><span class="hljs-comment"># tensor([</span><br><span class="hljs-comment"># [2],</span><br><span class="hljs-comment"># [4]</span><br><span class="hljs-comment"># ])</span><br><span class="hljs-comment"># 当condition是一维张量时，输出结果会自动省略第一列的0.原本结果应该是tesor([[0,2], [0,4]]), 前面的0会自动省略。</span><br><br><span class="hljs-comment"># Part 2 torch.where(condition, input, other, *, out=None)</span><br><span class="hljs-comment"># condition(BoolTensor)，如果input和other是tensor,那么都要和condition的尺寸相同。如果某个元素位置condition是True,那么久填充input所在索引的值，否则填充other所在索引的值。如果input，other某一个是标量，那么直接填充这个标量值。</span><br><span class="hljs-built_in">print</span>(~condition.unsqueeze(<span class="hljs-number">1</span>).expand_as(t))<br><span class="hljs-comment"># tensor([[True,  True,  True],</span><br><span class="hljs-comment">#        [ True,  True,  True],</span><br><span class="hljs-comment">#        [ False, False, False],</span><br><span class="hljs-comment">#        [ True,  True,  True],</span><br><span class="hljs-comment">#        [ False, False, False]])</span><br><br>value_v1 = torch.where(~condition.unsqueeze(<span class="hljs-number">1</span>).expand_as(t), t, -<span class="hljs-number">1</span>) <span class="hljs-comment"># 功能：通过value查询t中和value相同的向量，并把这些向量都改成[-1, -1, ..., -1]</span><br><span class="hljs-built_in">print</span>(value_v1)<br><span class="hljs-comment">#tensor([[ 0,  1,  2],</span><br><span class="hljs-comment">#        [ 4,  5,  6],</span><br><span class="hljs-comment">#        [-1, -1, -1],</span><br><span class="hljs-comment">#        [ 0,  1,  3],</span><br><span class="hljs-comment">#        [-1, -1, -1]])</span><br><br><span class="hljs-comment"># Part 3 torch.masked_select(input, mask, *, out=None) 注意mask和input的尺寸相同或者mask is broadcastable，最后把mask中为True的元素提取出来，返回的是1-D Tensor。</span><br><br><span class="hljs-comment"># broadcastbale version</span><br><span class="hljs-built_in">print</span>(~condition.unsqueeze(<span class="hljs-number">1</span>))<br><span class="hljs-comment"># tensor([[ True],</span><br><span class="hljs-comment">#         [ True],</span><br><span class="hljs-comment">#         [False],</span><br><span class="hljs-comment">#         [ True],</span><br><span class="hljs-comment">#         [False]])</span><br><span class="hljs-built_in">print</span>(torch.masked_select(t, ~condition.unsqueeze(<span class="hljs-number">1</span>)))<br><span class="hljs-comment"># tensor([0, 1, 2, 4, 5, 6, 0, 1, 3])</span><br><br><span class="hljs-comment"># same shape version</span><br><span class="hljs-built_in">print</span>(torch.masked_select(t, ~condition.unsqueeze(<span class="hljs-number">1</span>).expand_as(t)))<br><span class="hljs-comment"># tensor([0, 1, 2, 4, 5, 6, 0, 1, 3])</span><br>cond_exp_new = condition.unsqueeze(<span class="hljs-number">1</span>).expand_as(t)<br>cond_exp_new[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] = <span class="hljs-literal">True</span><br><span class="hljs-built_in">print</span>(torch.masked_select(t, ~cond_exp_new))<br><span class="hljs-comment"># tensor([4, 5, 6, 0, 1, 3])</span><br><br><br><span class="hljs-comment"># Part 4 torch.tensor.masked_fill_(mask, value)  可以根据mask填充value，比如对特定条件情况补0，补-1等操作，加上特殊判断语句。</span><br><br><span class="hljs-comment"># Part 5 torch.select_scatter(input, src, dim, index) src需要和 torch.select(input, dim, index)的尺寸相同，然后把src的内容拷贝到input中。This function returns a tensor with fresh storage; it does not create a view.</span><br>a = torch.zeros(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>b = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br><span class="hljs-built_in">print</span>(a.select_scatter(b, dim=<span class="hljs-number">0</span>, index=<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(a.select_scatter(b, dim=<span class="hljs-number">0</span>, index=<span class="hljs-number">1</span>))<br><span class="hljs-built_in">print</span>(a.select_scatter(b, dim=<span class="hljs-number">1</span>, index=<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(a.select_scatter(b, dim=<span class="hljs-number">1</span>, index=<span class="hljs-number">1</span>))<br><br><span class="hljs-comment"># dim 控制对a横向还是纵向填充(选取的是填充索引，这里dim=0是行，dim=1是列)，index控制要填充数据的索引(比如dim=0在横着填充情况下，index=0表示填充第0行，index=1填充第1行，假如a中还有多行，index可以控制只填充其中几行)</span><br><br><span class="hljs-comment">#tensor([[1., 2.],</span><br><span class="hljs-comment">#        [0., 0.]])</span><br><br><span class="hljs-comment">#tensor([[0., 0.],</span><br><span class="hljs-comment">#        [1., 2.]])</span><br><br><span class="hljs-comment">#tensor([[1., 0.],</span><br><span class="hljs-comment">#        [2., 0.]])</span><br><br><span class="hljs-comment">#tensor([[0., 1.],</span><br><span class="hljs-comment">#        [0., 2.]])</span><br><br></code></pre></td></tr></table></figure><p>此外还要区分一下torch.stack()和torch.concat()的区别：</p><p>torch.stack(): Concatenates a sequence of tensors along a<strong><u>new</u></strong> dimension. <u><strong>All tensors need to beof the same size.</strong></u>所有尺寸必须相同，是要新建一个dim维度然后再新维度上把多个尺寸相同的拼起来。</p><p>torch.cat(): concatenates the given sequence along <strong><u>anexisting dimension</u></strong>.已有维度上变得更宽。除了dim选择的维度大小可以不一样，其他维度尺寸必须相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python">imoprt torch<br><br>a = torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>], [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]]) <span class="hljs-comment"># (2,3)</span><br>b = torch.tensor([[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>],[<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>],[<span class="hljs-number">8</span>,<span class="hljs-number">9</span>,<span class="hljs-number">10</span>]]) <span class="hljs-comment">#(3,3)</span><br>c = torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>]]) <span class="hljs-comment"># (2,3)</span><br><br>torch.cat((a,b), dim=<span class="hljs-number">0</span>)<br><span class="hljs-comment">#tensor([[ 1,  2,  3],</span><br><span class="hljs-comment">#        [ 4,  5,  6],</span><br><span class="hljs-comment">#        [ 2,  3,  4],</span><br><span class="hljs-comment">#        [ 5,  6,  7],</span><br><span class="hljs-comment">#        [ 8,  9, 10]])</span><br><br>torch.stack((a,b), dim=<span class="hljs-number">0</span>)<br><span class="hljs-comment">#Traceback (most recent call last):</span><br><span class="hljs-comment">#  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="hljs-comment">#RuntimeError: stack expects each tensor to be equal size, but got [2, 3] at entry 0 and [3, 3] at entry 1</span><br><br>torch.cat((a,c), dim=<span class="hljs-number">0</span>)<br>tensor([[ <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],<br>        [ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>],<br>        [ <span class="hljs-number">1</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">1</span>],<br>        [-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>]])<br><br>torch.cat((a,c), dim=<span class="hljs-number">1</span>)<br><span class="hljs-comment">#tensor([[ 1,  2,  3,  1,  1,  1],</span><br><span class="hljs-comment">#        [ 4,  5,  6, -1, -1, -1]])</span><br><br>torch.satck((a,c), dim=<span class="hljs-number">0</span>)<br><span class="hljs-comment">#tensor([[[ 1,  2,  3],</span><br><span class="hljs-comment">#         [ 4,  5,  6]],</span><br><span class="hljs-comment">#        [[ 1,  1,  1],</span><br><span class="hljs-comment">#         [-1, -1, -1]]])</span><br><span class="hljs-comment"># shape (2, 2, 3) 在第0维插入新维度，并stack起来</span><br><br>torch.stack((a,c), dim=<span class="hljs-number">1</span>)<br><span class="hljs-comment">#tensor([[[ 1,  2,  3],</span><br><span class="hljs-comment">#         [ 1,  1,  1]],</span><br><span class="hljs-comment">#       [[ 4,  5,  6],</span><br><span class="hljs-comment">#         [-1, -1, -1]]])</span><br><span class="hljs-comment"># shape (2, 2, 3)  在第1维插入新维度，并stack起来</span><br><br><br>torch.stack((a,c), dim=<span class="hljs-number">2</span>)<br><span class="hljs-comment">#tensor([[[ 1,  1],</span><br><span class="hljs-comment">#         [ 2,  1],</span><br><span class="hljs-comment">#         [ 3,  1]],</span><br><span class="hljs-comment">#        [[ 4, -1],</span><br><span class="hljs-comment">#         [ 5, -1],</span><br><span class="hljs-comment">#         [ 6, -1]]])</span><br><span class="hljs-comment"># shape (2, 3, 2)  在第2维插入新维度，并stack起来</span><br><br></code></pre></td></tr></table></figure><p>高维空间不容易想象，建议最好用简单的二维情况处理。如果需要高维，建议用小数据量多做做实验。比如点云Voxel中常见(BatchSize,Num_Cameras, Channel, X_axis, Y_axis, Z_axis)形状的Tensor，你要凭空用脑袋去想对某个dim进行stack或cat等操作，脑子都要炸了。建议用几个简单样例跑一跑，纸笔画画图，具体情况具体分析。</p>]]></content>
    
    
    <categories>
      
      <category>编程学习</category>
      
      <category>numpy学习</category>
      
      <category>随笔</category>
      
      <category>Pytorch</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>probability_beginner</title>
    <link href="/posts/18466784/"/>
    <url>/posts/18466784/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>numpy部分函数讲解</title>
    <link href="/posts/5d51c93/"/>
    <url>/posts/5d51c93/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="//music.163.com/outchain/player?type=2&amp;id=1845153293&amp;auto=0&amp;height=66"></iframe><h1 id="numpy部分函数讲解">Numpy部分函数讲解</h1><p>学习笔记，记录一些<code>numpy</code>中一些不太熟悉但是非常好用的API用法。</p><h2 id="numpy.lexsort"><code>numpy.lexsort()</code></h2><p>官方doc讲解:https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html</p><p>v1.2.4版本为例</p><blockquote><p>numpy.<strong>lexsort</strong>(<em>keys</em>, <em>axis=-1</em>)</p><p>Perform an indirect stable sort using a sequence of keys.</p><p>Given multiple sorting keys, which can be interpreted as columns in aspreadsheet, lexsort returns an array of integer indices that describesthe sort order by multiple columns. <strong>The last key in the sequenceis used for the primary sort order, the second-to-last key for thesecondary sort order, and so on.</strong> The keys argument must be asequence of objects that can be converted to arrays of the same shape.If a 2D array is provided for the keys argument, its rows areinterpreted as the sorting keys and sorting is according to the lastrow, second last row etc.</p><ul><li>Parameters:</li></ul><p><strong>keys</strong>(k, N) array or tuple containing k (N,)-shapedsequences. The <em>k</em> different “columns” to be sorted. The lastcolumn (or row if <em>keys</em> is a 2D array) is the primary sortkey.</p><p><strong>axis</strong>(int, optional) Axis to be indirectly sorted. Bydefault, sort over the last axis.</p><ul><li>Returns:</li></ul><p><strong>indices</strong>(N,) ndarray of ints.Array of indices thatsort the keys along the specified axis.</p></blockquote><p>打个比方，类似于考试排名时，首先以总成绩排名，当总成绩相同时一次按照语文，数学，英语的分数顺序排名，实现了多个关键字的排序功能。例如</p><table><thead><tr><th style="text-align: center;"></th><th style="text-align: center;">语文</th><th style="text-align: center;">数学</th><th style="text-align: center;">英语</th></tr></thead><tbody><tr><td style="text-align: center;">小明</td><td style="text-align: center;">92</td><td style="text-align: center;">96</td><td style="text-align: center;">100</td></tr><tr><td style="text-align: center;">小红</td><td style="text-align: center;">82</td><td style="text-align: center;">90</td><td style="text-align: center;">85</td></tr><tr><td style="text-align: center;">小丽</td><td style="text-align: center;">82</td><td style="text-align: center;">60</td><td style="text-align: center;">85</td></tr><tr><td style="text-align: center;">小亮</td><td style="text-align: center;">82</td><td style="text-align: center;">90</td><td style="text-align: center;">70</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy<br><br>a = np.array([<span class="hljs-number">92</span>, <span class="hljs-number">82</span>, <span class="hljs-number">82</span>, <span class="hljs-number">82</span>]) <span class="hljs-comment"># first column</span><br>b = np.array([<span class="hljs-number">96</span>, <span class="hljs-number">90</span>, <span class="hljs-number">60</span>, <span class="hljs-number">90</span>]) <span class="hljs-comment"># second column</span><br>c = np.array([<span class="hljs-number">100</span>, <span class="hljs-number">85</span>, <span class="hljs-number">85</span>, <span class="hljs-number">70</span>]) <span class="hljs-comment"># third column</span><br><br>np.lexsort((c, b, a)) <span class="hljs-comment"># default, last column is the primary key</span><br><br><span class="hljs-comment"># result is array([2, 3, 1, 0])</span><br><span class="hljs-comment"># 小丽&lt;小亮&lt;小红&lt;小明</span><br><br><span class="hljs-comment"># 等价写法</span><br><span class="hljs-comment"># 2D array 排序，相当于原始表格行列转置了</span><br>datasheet = np.array([c, b, a])<br><span class="hljs-comment"># array([[100,  85,  85,  70],</span><br><span class="hljs-comment">#       [ 96,  90,  60,  90],</span><br><span class="hljs-comment">#       [ 92,  82,  82,  82]])</span><br><br>np.lexsort(datasheet)<br><br></code></pre></td></tr></table></figure><p>有一些需要注意的细节:</p><ul><li>默认从小到大排序</li><li>在输入<code>key</code>这一项的时候，如果输入是多个一维数组(数组的长度必须相同)，那么<code>primary key</code>的优先顺序是最后一列，倒数第二列，……，第一列(所以顺序是<code>(c,b,a)</code>不是<code>(a,b,c)</code>)。如果是2Darray(比如作为一个datasheet输入)，那么顺序是最后一行，倒数第二行，……，第一行。</li></ul><p>此外，程序里一维数组都是行向量，组成2D array时数学写法应该是 <spanclass="math inline">$\left[ \begin{array}{c} c \\b \\ a \end{array}\right]$</span>，不过程序里<code>np.array([c, b, a])</code>只能横着写，按照以前的习惯容易误以为是四行三列了，实际是三行四列的。所以<code>numpy</code>官方很体贴的将2Darray直接改为按行比较。</p><h2 id="numpy.insert"><code>numpy.insert()</code></h2><p>官方doc讲解:https://numpy.org/doc/stable/reference/generated/numpy.insert.html</p><p>直接看example</p><blockquote><p>Examples 是否指定axis的区别</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>a<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>       [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>],<br>       [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(a, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>)<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(a, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, axis=<span class="hljs-number">1</span>)<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>],<br>       [<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>],<br>       [<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>]])<br></code></pre></td></tr></table></figure><p>Difference between sequence and scalars:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(a, [<span class="hljs-number">1</span>], [[<span class="hljs-number">1</span>],[<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>]], axis=<span class="hljs-number">1</span>)<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>       [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>],<br>       [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.array_equal(np.insert(a, <span class="hljs-number">1</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], axis=<span class="hljs-number">1</span>),<br><span class="hljs-meta">... </span>               np.insert(a, [<span class="hljs-number">1</span>], [[<span class="hljs-number">1</span>],[<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>]], axis=<span class="hljs-number">1</span>))<br><span class="hljs-literal">True</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>b = a.flatten()<br><span class="hljs-meta">&gt;&gt;&gt; </span>b<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(b, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, ..., <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(b, <span class="hljs-built_in">slice</span>(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>), [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, ..., <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(b, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">7.13</span>, <span class="hljs-literal">False</span>]) <span class="hljs-comment"># type casting</span><br>array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">7</span>, ..., <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>x = np.arange(<span class="hljs-number">8</span>).reshape(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>idx = (<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(x, idx, <span class="hljs-number">999</span>, axis=<span class="hljs-number">1</span>)<br>array([[  <span class="hljs-number">0</span>, <span class="hljs-number">999</span>,   <span class="hljs-number">1</span>,   <span class="hljs-number">2</span>, <span class="hljs-number">999</span>,   <span class="hljs-number">3</span>],<br>       [  <span class="hljs-number">4</span>, <span class="hljs-number">999</span>,   <span class="hljs-number">5</span>,   <span class="hljs-number">6</span>, <span class="hljs-number">999</span>,   <span class="hljs-number">7</span>]])<br></code></pre></td></tr></table></figure></blockquote><h2 id="numpy.unique"><code>numpy.unique()</code></h2><p>https://numpy.org/doc/stable/reference/generated/numpy.unique.html#numpy-unique</p><p>返回一个数组去重后的结果</p><blockquote><p>numpy.<strong>unique</strong>(<em>ar</em>,<em>return_index=False</em>, <em>return_inverse=False</em>,<em>return_counts=False</em>, <em>axis=None</em>, **<em>,</em>equal_nan=True*)[<ahref="https://github.com/numpy/numpy/blob/v1.24.0/numpy/lib/arraysetops.py#L138-L320">source]</a></p><p>Find the unique elements of an array.</p><p>Returns the sorted unique elements of an array. There are threeoptional outputs in addition to the unique elements:</p><ul><li>the indices of the input array that give the uniquevalues（注：第一次出现unique value的位置）</li><li><strong>the indices of the unique array that reconstruct the inputarray</strong></li><li>the number of times each unique value comes up in the inputarray</li></ul></blockquote><p>看example</p><p>1D 情况: 注意输入是多维array, 不指定axis的话会被拉平成一维。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>np.unique([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>], return_index=<span class="hljs-literal">True</span>, return_inverse=<span class="hljs-literal">True</span>, return_counts=<span class="hljs-literal">True</span>)<br>(array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]), array([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>]), array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]), array([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]))<br><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.unique(a)<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br></code></pre></td></tr></table></figure><p>2D情况：</p><p>下面这个情况常见，例如3D点云Voxel稀疏化操作中，a数组是<code>(N,3)</code>代表记录着<code>N</code>个点三维坐标空间信息。原始点云坐标经过Voxel划分后，得到每个点所在的Voxel的下标索引。稀疏化操作就是只记录有点云存在的Voxel的下标位置，并只对这些位置进行几何变换的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.unique(a, axis=<span class="hljs-number">0</span>, return_index=<span class="hljs-literal">True</span>, return_inverse=<span class="hljs-literal">True</span>, return_counts=<span class="hljs-literal">True</span>)<br>(array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]), array([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>]), array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]), array([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>]))<br></code></pre></td></tr></table></figure><p><code>numpy.unique()</code>后重建原来数组</p><p>Reconstruct the input array from the unique values and inverse:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>u, indices = np.unique(a, return_inverse=<span class="hljs-literal">True</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>u<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>indices<br>array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>u[indices]<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>])<br></code></pre></td></tr></table></figure><p>同理2D:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>u, indices = np.unique(a, axis=<span class="hljs-number">0</span>, return_inverse=<span class="hljs-literal">True</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>u<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>       [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>indices<br>array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>u[indices]<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>       [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>       [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>编程学习</category>
      
      <category>numpy学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>numpy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Music Collection</title>
    <link href="/posts/5d4e3cda/"/>
    <url>/posts/5d4e3cda/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script>        <div id="aplayer-qwcodfXo" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;"></div>  <script>  var options = {"narrow":false,"autoplay":false,"showlrc":3,"mode":"random","mutex":true,"theme":"#e6d0b2","preload":"metadata","listmaxheight":"513px","music":[{"title":"player 桜ノ雨","author":"halyosy/初音ミク","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/2Bf8xe5dmDOzEQ0GIu2J0ABRples20kI/%E6%A1%9C%E3%83%8E%E9%9B%A8-halyosy-%E5%88%9D%E9%9F%B3%E3%83%9F%E3%82%AF.mp3","pic":"https://p1.music.126.net/YLL0XkhJwp3q0ZSyzFjOgg==/888405395266415.jpg?param=90y90"},{"title":"ひだまりLiving","author":"藤東知夏","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/4KCAwNYierrnGh6TToKTWCjmnXRtzbVF/%E3%81%B2%E3%81%A0%E3%81%BE%E3%82%8ALiving-%E8%97%A4%E6%9D%B1%E7%9F%A5%E5%A4%8F.mp3","pic":"https://p2.music.126.net/B_sCLXthAHgoVEMIRNU1hw==/109951163559682176.jpg?param=90y90"},{"title":"ごり押しの言い訳","author":"百石元","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/D8CMwMFDlrVK4uWV60d9WqbnW9Lcz4Es/%E3%81%94%E3%82%8A%E6%8A%BC%E3%81%97%E3%81%AE%E8%A8%80%E3%81%84%E8%A8%B3-%E7%99%BE%E7%9F%B3%E5%85%83.mp3","pic":"https://p2.music.126.net/B_sCLXthAHgoVEMIRNU1hw==/109951163559682176.jpg?param=90y90"},{"title":"God knows","author":"平野綾","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/jkPObTbA4F1MQYDv25d9z2Ba1T97V1Cn/Godknows-%E5%B9%B3%E9%87%8E%E7%B6%BE.mp3","pic":"https://p2.music.126.net/LTEcJoeajuhP5C2BtbGhtA==/109951165822358136.jpg?param=90y90"},{"title":"となりのトトロ","author":"久石譲","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/HpuI9xUvQg21sya05w9VeY7w636tpJNP/%E3%81%A8%E3%81%AA%E3%82%8A%E3%81%AE%E3%83%88%E3%83%88%E3%83%AD-%E4%B9%85%E7%9F%B3%E8%AD%B2.mp3","pic":"https://s2.loli.net/2023/01/03/V3LGNSls97peEHK.png"},{"title":"几许风雨","author":"罗文","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/5X2Y4agBl7If7qLiunueTg8JUdq4erlr/%E5%87%A0%E8%AE%B8%E9%A3%8E%E9%9B%A8-%E7%BD%97%E6%96%87.mp3","pic":"https://p2.music.126.net/vRo5uQ8gIs2OuNJQqtMD_Q==/109951163197518435.jpg?param=90y90"},{"title":"H.He.Li.Be.B.C.N.O.F.Ne.","author":"金元寿子","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/GavUNyn8XG0Hq44KAuLbc4a43VBAStCz/H.He.Li.Be.B.C.N.O.F.Ne.-%E9%87%91%E5%85%83%E5%AF%BF%E5%AD%90.mp3","pic":"https://p2.music.126.net/lbUnHPcNshuVgCRnVAjIhg==/18517974836952010.jpg?param=90y90"},{"title":"I Seek the Truth (From Frozen 2/Outtake)","author":"Kristen Anderson-Lopez / Patti Murin","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/ntxowMgKX7tSsqCOHY9DmtsUNWPDMqrr/I%20Seek%20the%20Truth%20%28From%20-Frozen%202-_Outtake%29.mp3","pic":"https://p1.music.126.net/IUy2ZL9d1eLwAywlFDg7VA==/109951165270313538.jpg?param=130y130"},{"title":"日々の遑","author":"折戸伸治","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/vFPbF2EvCh7Ojwp00RkpOYrPkRBDzNyl/%E6%97%A5%E3%80%85%E3%81%AE%E9%81%91-%E6%8A%98%E6%88%B8%E4%BC%B8%E6%B2%BB.mp3","pic":"https://p2.music.126.net/PtLd62-khEg8iCutTdE5Vg==/109951163286813093.jpg?param=90y90"},{"title":"Liebesleid","author":"Fritz Kreisler, Sergiu SchwartzRuzhka,Charakchieva","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/NujmXqdawNaccRXIfCtLA14PRiwiycqd/Liebesleid-Fritz%20KreislerSergiu%20SchwartzRuzhka%20Charakchieva.mp3","pic":"https://p1.music.126.net/2VzQnMcjtR0vkwP8SGYDZw==/109951163162679744.jpg?param=90y90"}]};  options.element = document.getElementById("aplayer-qwcodfXo");  var ap = new APlayer(options);    window.aplayers || (window.aplayers = []);  window.aplayers.push(ap);  </script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>LaTeX讲解系列(1)：如何优雅地插入图表</title>
    <link href="/posts/b2b25956/"/>
    <url>/posts/b2b25956/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="latex讲解系列1-如何优雅地插入图表">LaTeX讲解系列(1)：如何优雅地插入图表</h1><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="//music.163.com/outchain/player?type=2&amp;id=526656&amp;auto=0&amp;height=66"></iframe><p>包括插入子图，多图排列，画模型图等等</p><p>具体代码以会议/期刊所给的模板格式为准</p><h2 id="插入图片">插入图片</h2><p><strong>插入一张图片</strong></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-comment">%\begin&#123;center&#125; % 外部居中</span><br> <span class="hljs-keyword">\begin</span>&#123;figure&#125;[!htbp]<br> <span class="hljs-keyword">\centering</span> <span class="hljs-comment">% 内部居中</span><br> <span class="hljs-keyword">\includegraphics</span>[width=0.4<span class="hljs-keyword">\textwidth</span>]&#123;introduction<span class="hljs-built_in">_</span>3<span class="hljs-built_in">_</span>cases.pdf&#125; <span class="hljs-comment">% Reduce the figure size so that it is slightly narrower than the column.</span><br> <span class="hljs-keyword">\caption</span>&#123;XXXXXX&#125;<br> <span class="hljs-keyword">\label</span>&#123;fig1:three<span class="hljs-built_in">_</span>cases&#125;<br> <span class="hljs-keyword">\end</span>&#123;figure&#125; <span class="hljs-comment">% Note that it should be below includegraphics</span><br> <span class="hljs-comment">%\end&#123;center&#125;</span><br></code></pre></td></tr></table></figure><p><strong>插入多张图片</strong></p><ol type="1"><li><p>对于插入多张子图的组图，如果数量$$4的，建议在PPT或者<code>draw.io</code>中手动排版一下，调整间距大小截图角度等组合成一个整体，作为单张图片来插入。</p></li><li><p>如果数量很多，比如<spanclass="math inline">5 × 6</span>的阵列，可以使用<code>MulimgViewer</code>软件(https://github.com/nachifur/MulimgViewer)进行多图联排展现可视化，软件会自动合成一张大单图。</p></li><li><p>若使用LaTex的<code>subfigure</code>来组图，实现方法如下:</p><p>但是困难是 1)一旦图片变多调试会变得非常麻烦 2)非常容易overfull超过页面大小，除非把每张图片缩小的特别小，但这样就看不清了3) 图片内部之间的白色间隔难以消除。非必要不用这种方式。</p></li></ol><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;center&#125;<br><span class="hljs-keyword">\begin</span>&#123;figure&#125;[htbp]<span class="hljs-comment">% 一个figure环境套多个subfigure环境(保证Figure序号递增)</span><br>    <span class="hljs-keyword">\centering</span><br>    <span class="hljs-keyword">\subfigure</span>&#123;<span class="hljs-comment">% 每个subfigure负责控制一行图片的排列，下辖多个minipage环境</span><br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.4<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.45]&#123;Fig/beat.png&#125;<span class="hljs-comment">% 不同的子图控制不同的缩放比</span><br>            <span class="hljs-keyword">\caption</span>&#123;Beating&#125;<span class="hljs-keyword">\label</span>&#123;Beating&#125;<span class="hljs-comment">% 不同的子图设置不同的标题</span><br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>        <br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.6<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.45]&#123;Fig/lean.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;leaning&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>    &#125;<br>    <span class="hljs-keyword">\centering</span><br>    <span class="hljs-keyword">\subfigure</span>&#123;<br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.4<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.45]&#123;Fig/catch<span class="hljs-built_in">_</span>up.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;Catch up&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>        <br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.6<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.45]&#123;Fig/sneak.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;Sneaking&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>    &#125;<br>    <br>    <span class="hljs-keyword">\subfigure</span>&#123;<br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.4<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.3]&#123;Fig/stand.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;Stand still&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>        <br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.6<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.35]&#123;Fig/slippery.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;Slippery&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>    &#125;<br>    <span class="hljs-keyword">\centering</span><br>    <span class="hljs-keyword">\subfigure</span>&#123;<br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.4<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.3]&#123;Fig/turnaround.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;Upside down&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>        <br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.6<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.3]&#123;Fig/struggle.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;Struggle to turn around&#125;<span class="hljs-keyword">\label</span>&#123;struggle&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>    &#125;<br><span class="hljs-keyword">\end</span>&#123;figure&#125;<br><span class="hljs-keyword">\end</span>&#123;center&#125;<br></code></pre></td></tr></table></figure><p>效果图:</p><p><img src="https://s2.loli.net/2022/12/29/TJ81KenuBiYUSwW.png" /></p><h2 id="插入表格">插入表格</h2><p>检查插入如下必要的宏包</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\usepackage</span>&#123;graphicx&#125;<br><span class="hljs-keyword">\usepackage</span>&#123;booktabs&#125;<br></code></pre></td></tr></table></figure><h3 id="cs科研表格">CS科研表格</h3><p>效果图：</p><p><img src="https://s2.loli.net/2023/01/08/Wmd5CvpjbPEF9UJ.png" alt="table1.png" style="zoom:67%;" /></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs LaTex"><span class="hljs-keyword">\begin</span>&#123;table*&#125;[htbp] <span class="hljs-comment">% 注意在双栏模板中，带*表示跨栏排版，不带星号表示单栏排版；</span><br><span class="hljs-keyword">\centering</span><br><span class="hljs-keyword">\resizebox</span>&#123;1.0<span class="hljs-keyword">\textwidth</span>&#125;&#123;!&#125;&#123;<span class="hljs-comment">% 注意使用resizebox需要\usepackage&#123;graphicx&#125;，\textwidth是设定好的标准的一行的长度数值。如果要被双栏装下，需改成0.5\textwidth,以此类推。</span><br><span class="hljs-keyword">\begin</span>&#123;tabular&#125;&#123;l|cccc|ccc|ccc|c|c&#125; <span class="hljs-comment">% 需要12列,l,c,r分别代表居左中右对齐，|代表竖杠分割线</span><br><span class="hljs-keyword">\toprule</span> <span class="hljs-comment">%添加表格头部粗线,toptule, midrule和bottomrule 都需要\usepackage&#123;booktabs&#125;宏包</span><br>Method <span class="hljs-built_in">&amp;</span> PPP <span class="hljs-built_in">&amp;</span> PPPP <span class="hljs-built_in">&amp;</span> QQ <span class="hljs-built_in">&amp;</span> QQQ <span class="hljs-built_in">&amp;</span> QQQQ <span class="hljs-built_in">&amp;</span> RRR <span class="hljs-built_in">&amp;</span> RRRR <span class="hljs-built_in">&amp;</span> TTTT <span class="hljs-built_in">&amp;</span> TTTTT <span class="hljs-built_in">&amp;</span> ZZZZZZ <span class="hljs-built_in">&amp;</span> mIoU <span class="hljs-built_in">&amp;</span> FPS <span class="hljs-keyword">\\</span><br> <span class="hljs-comment">%有n个&amp;，就表示该行有n+1列</span><br><span class="hljs-keyword">\hline</span> <span class="hljs-comment">%绘制一条水平横线</span><br> Method-A <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> - <span class="hljs-keyword">\\</span><br><br> Method-B <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> - <span class="hljs-keyword">\\</span><br><br> Method-C <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> - <span class="hljs-keyword">\\</span><br><span class="hljs-keyword">\bottomrule</span> <span class="hljs-comment">%添加表格底部粗线</span><br><span class="hljs-keyword">\end</span>&#123;tabular&#125;<br>&#125;<br><br><span class="hljs-keyword">\caption</span>&#123;XXXXXX Performance.&#125;<br><span class="hljs-keyword">\label</span>&#123;tab1:performance&#125; <span class="hljs-comment">% 添加作为引用的锚点label</span><br><span class="hljs-keyword">\end</span>&#123;table*&#125;<br></code></pre></td></tr></table></figure><h2 id="三线表">三线表</h2><p><img src="https://s2.loli.net/2023/01/08/OLaECYzh54Zd3R6.png" alt="Table2" style="zoom:67%;" /></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs Latex"><span class="hljs-keyword">\begin</span>&#123;table&#125;[htbp] <span class="hljs-comment">% 注意在双栏模板中，带*表示跨栏排版，不带星号表示单栏排版；</span><br><span class="hljs-keyword">\small</span> <span class="hljs-comment">% 设置字号</span><br><span class="hljs-keyword">\centering</span><br><span class="hljs-keyword">\resizebox</span>&#123;0.6<span class="hljs-keyword">\textwidth</span>&#125;&#123;!&#125;&#123;<span class="hljs-comment">% 注意使用resizebox需要\usepackage&#123;graphicx&#125;，\textwidth是设定好的标准的一行的长度数值。如果要被双栏装下，需改成0.5\textwidth,以此类推。</span><br><span class="hljs-keyword">\begin</span>&#123;tabular&#125;&#123;ccccc&#125; <span class="hljs-comment">% 需要5列,l,c,r分别代表居左中右对齐，|代表竖杠分割线</span><br><span class="hljs-keyword">\toprule</span> <span class="hljs-comment">%添加表格头部粗线</span><br>Method <span class="hljs-built_in">&amp;</span> PPP <span class="hljs-built_in">&amp;</span> PPPP <span class="hljs-built_in">&amp;</span> QQ <span class="hljs-built_in">&amp;</span> QQQ <span class="hljs-keyword">\\</span><br> <span class="hljs-comment">%有n个&amp;，就表示该行有n+1列</span><br><span class="hljs-keyword">\midrule</span>  <span class="hljs-comment">% 三线表中的顶格线</span><br> Method-A <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0  <span class="hljs-keyword">\\</span><br><br> Method-B <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0  <span class="hljs-keyword">\\</span><br><br> Method-C <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0  <span class="hljs-keyword">\\</span><br><span class="hljs-keyword">\bottomrule</span> <span class="hljs-comment">%添加表格底部粗线</span><br><span class="hljs-keyword">\end</span>&#123;tabular&#125;<br>&#125;<br><br><span class="hljs-comment">% \setlength&#123;\abovecaptionskip&#125;&#123;5pt&#125; 设置标题与上方/下方的距离间隔，一定要紧贴`\caption`命令</span><br><span class="hljs-comment">% \setlength&#123;\belowcaptionskip&#125;&#123;6pt&#125;</span><br><span class="hljs-keyword">\caption</span>&#123;XXXXXX Performance.&#125;<br><br><span class="hljs-keyword">\label</span>&#123;tab2:three<span class="hljs-built_in">_</span>line&#125; <span class="hljs-comment">% 添加作为引用的锚点label</span><br><span class="hljs-keyword">\end</span>&#123;table&#125;<br></code></pre></td></tr></table></figure><p>补充：</p><ol type="1"><li><p>Latex字号大小<code>\tiny &lt; \scriptsize &lt; \footnotesize &lt; \small &lt; \normalsize &lt; \large &lt; Large &lt; LARGE &lt; huge &lt; Huge</code>，一般默认是<code>\normalsize</code>。此外，在正文里若想只对一部分文字改变字号，可以<code>{\Large Input your text here}</code></p></li><li><p><code>\resizebox{宽度}{高度}{}</code> 和<code>\scalebox{0.65}</code>区别在于高度控制的不同。“两者的区别在于高度的控制，前者控制相对高度，后者控制总高度，如果命令中的宽度或高度使用了感叹号代替，表示按照高度或宽度保持宽高比进行缩放”<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="https://www.cnblogs.com/ybl20000418/p/11478324.html">[3]</span></a></sup></p></li><li><p>列宽调整<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="https://www.cnblogs.com/ybl20000418/p/11478324.html">[3]</span></a></sup></p><blockquote><p>将代码中的<code>\begin{tabular}{c|ccc}</code>中的每个c改成<code>\begin{tabular}{ p{5pt} &lt;&lt;span class="hljs-cell"&gt;{\centering} | ccc}</code>，其中花括号里面填写宽度数值，如果需要该列居中则加入<code>&lt;&lt;span class="hljs-cell"&gt;{\centering}</code></p></blockquote></li><li><p>好像三线表中<code>\toprule</code>,<code>\bottomrule</code>,如果要用竖线分割时会导致竖线在顶线和底线处断开。目前暂时没找到好的解决方法。如果均换成<code>\hline</code>，虽然竖线不会断开但是也没有粗细区别了<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="https://zhuanlan.zhihu.com/p/580123090">[4]</span></a></sup>。</p></li></ol><h2 id="多个子列合并">多个子列合并</h2><p>首先<code>\usepackage{multirow}</code></p><p>下面两个博客讲解的都特别好，因此推荐看原文，就不再赘述。源代码中做了一些小改进，附文在下：</p><p>参考资料:</p><p>https://blog.csdn.net/liu16659/article/details/111656244使用latex做三线表</p><iframe src="https://blog.csdn.net/liu16659/article/details/111656244" width="100%" height="500" name="topFrame" scrolling="yes" noresize="noresize" frameborder="0" id="topFrame"></iframe><hr /><p>E.g. 1</p><figure><img src="https://s2.loli.net/2023/01/08/lW82YLAftpSiDKI.png"alt="Table3" /><figcaption aria-hidden="true">Table3</figcaption></figure><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs LaTex"><span class="hljs-comment">% multirow,multicolumn 使用1</span><br><span class="hljs-keyword">\begin</span>&#123;table&#125;[!htbp] <br>    <span class="hljs-keyword">\centering</span><br>    <span class="hljs-keyword">\begin</span>&#123;tabular&#125;&#123;ccccccccccc&#125; <span class="hljs-comment">%需要10列</span><br>        <span class="hljs-keyword">\toprule</span> <span class="hljs-comment">%添加表格头部粗线</span><br>        <span class="hljs-keyword">\multicolumn</span>&#123;3&#125;&#123;c&#125;&#123;<span class="hljs-keyword">\multirow</span>&#123;2&#125;&#123;*&#125;&#123;Method&#125;&#125;<span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\multicolumn</span>&#123;3&#125;&#123;c&#125;&#123;Homographic&#125;<span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span><span class="hljs-keyword">\multicolumn</span>&#123;3&#125;&#123;c&#125;&#123;Hetergraphic&#125;<span class="hljs-keyword">\\</span><br>        <span class="hljs-keyword">\multicolumn</span>&#123;3&#125;&#123;c&#125;&#123;&#125; <span class="hljs-built_in">&amp;</span> Precision  <span class="hljs-built_in">&amp;</span> Recall <span class="hljs-built_in">&amp;</span> F1 <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span> Precision  <span class="hljs-built_in">&amp;</span>Recall <span class="hljs-built_in">&amp;</span> F1<span class="hljs-keyword">\\</span>  <span class="hljs-comment">%有n个&amp;，就表示该行有n+1列, 这里有7个&amp;，本应该有8列，再加上\multicolumn&#123;3&#125;&#123;c&#125;&#123;&#125;把第一列拆成3列，故共10列</span><br>        <span class="hljs-keyword">\hline</span> <span class="hljs-comment">%绘制一条水平横线</span><br>        <span class="hljs-keyword">\multicolumn</span>&#123;3&#125;&#123;c&#125;&#123;A&#125;<span class="hljs-built_in">&amp;</span> 50  <span class="hljs-built_in">&amp;</span> 0   <span class="hljs-built_in">&amp;</span> 100<span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span>200 <span class="hljs-built_in">&amp;</span> 300 <span class="hljs-built_in">&amp;</span> 300<span class="hljs-keyword">\\</span>   <span class="hljs-comment">% 占两列，列名为A；后面陆续跟着数字</span><br>        <span class="hljs-keyword">\multicolumn</span>&#123;3&#125;&#123;c&#125;&#123;B&#125;<span class="hljs-built_in">&amp;</span> 100 <span class="hljs-built_in">&amp;</span> 100 <span class="hljs-built_in">&amp;</span>0   <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span>100 <span class="hljs-built_in">&amp;</span> 200 <span class="hljs-built_in">&amp;</span> 200<span class="hljs-keyword">\\</span><br>        <span class="hljs-keyword">\multicolumn</span>&#123;3&#125;&#123;c&#125;&#123;C&#125;<span class="hljs-built_in">&amp;</span> 150 <span class="hljs-built_in">&amp;</span> 200 <span class="hljs-built_in">&amp;</span>100 <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span>0   <span class="hljs-built_in">&amp;</span> 100 <span class="hljs-built_in">&amp;</span> 200<span class="hljs-keyword">\\</span><br>        <span class="hljs-keyword">\bottomrule</span> <span class="hljs-comment">%添加表格底部粗线</span><br>    <span class="hljs-keyword">\end</span>&#123;tabular&#125;<br>    <span class="hljs-keyword">\caption</span>&#123;hihihi&#125;<br><span class="hljs-keyword">\end</span>&#123;table&#125;<br></code></pre></td></tr></table></figure><p>补充:</p><ol type="1"><li>注意用到multirow和multicol命令时需要使用宏包<code>\usepackage{multirow}</code></li><li>源博客代码中<code>&amp;Recall &amp; F1&amp;\\</code>这里多了一个<code>&amp;</code>，这里去掉了多余的<code>&amp;</code>修复bug</li></ol><hr /><p>E.g. 2</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><code class="hljs LaTex"><span class="hljs-comment">% from `tbl/tb1_main_table.tex`</span><br><span class="hljs-keyword">\begin</span>&#123;tabular&#125;&#123;<br><span class="hljs-comment">% r, c, l means aligned to right, center and left. @&#123;&#125; means delete default column width, and the following p&#123;2.0em&#125; means setting fixed column width while p refers to `parabox`.</span><br>@&#123;&#125;l <br><span class="hljs-comment">% c@&#123;&#125; c@&#123;&#125;p&#123;1.0em&#125;@&#123;&#125;</span><br>c@&#123;<span class="hljs-keyword">\hspace</span>&#123;0.5em&#125;&#125; c@&#123;<span class="hljs-keyword">\hspace</span>&#123;0.5em&#125;&#125; c@&#123;&#125;p&#123;1.0em&#125;@&#123;&#125;<br><span class="hljs-comment">% c@&#123;\hspace&#123;0.5em&#125;&#125;c @&#123;&#125;p&#123;2.0em&#125;@&#123;&#125; </span><br>c@&#123;<span class="hljs-keyword">\hspace</span>&#123;0.5em&#125;&#125; c@&#123;<span class="hljs-keyword">\hspace</span>&#123;0.5em&#125;&#125;c @&#123;&#125;p&#123;1.0em&#125;@&#123;&#125;<br>c@&#123;<span class="hljs-keyword">\hspace</span>&#123;0.5em&#125;&#125; c@&#123;<span class="hljs-keyword">\hspace</span>&#123;0.5em&#125;&#125;c @&#123;&#125;p&#123;1.0em&#125;@&#123;&#125;<br>c@&#123;&#125;<br>&#125;<br><br><span class="hljs-keyword">\toprule</span><br><br><span class="hljs-keyword">\multirow</span>&#123;2&#125;&#123;*&#125;&#123;Method&#125; <span class="hljs-built_in">&amp;</span><br><span class="hljs-keyword">\multicolumn</span>&#123;3&#125;&#123;c&#125;&#123;A&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span><br><span class="hljs-keyword">\multicolumn</span>&#123;3&#125;&#123;c&#125;&#123;<span class="hljs-keyword">\PrivateDatasetOne</span>&#123;&#125;&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span><br><span class="hljs-keyword">\multicolumn</span>&#123;3&#125;&#123;c&#125;&#123;<span class="hljs-keyword">\PrivateDatasetTwo</span>&#123;&#125;&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span><br><span class="hljs-keyword">\multirow</span>&#123;2&#125;&#123;*&#125;&#123;Avg Rank&#125;<br><span class="hljs-keyword">\\</span><br><br><span class="hljs-built_in">&amp;</span> <br>R1↓ <span class="hljs-built_in">&amp;</span><br>R2↓ <span class="hljs-built_in">&amp;</span> <br>R3↑ <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span><br>R1↓ <span class="hljs-built_in">&amp;</span><br>R2↓ <span class="hljs-built_in">&amp;</span> <br>R3↑ <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span><br>R1↓ <span class="hljs-built_in">&amp;</span><br>R2↓ <span class="hljs-built_in">&amp;</span> <br>R3↑<br><span class="hljs-keyword">\\</span><br><br><span class="hljs-keyword">\midrule</span><br><br>Method1~<span class="hljs-keyword">\cite</span>&#123;foo<span class="hljs-built_in">_</span>1&#125; <span class="hljs-built_in">&amp;</span> <br>1.910 <span class="hljs-built_in">&amp;</span> 10.2 <span class="hljs-built_in">&amp;</span> 89.5 <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span> <br> <span class="hljs-built_in">&amp;</span>  <span class="hljs-built_in">&amp;</span>  <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span><br> <span class="hljs-built_in">&amp;</span>  <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span><br><span class="hljs-keyword">\\</span><br><br>Method2 ~<span class="hljs-keyword">\cite</span>&#123;foo<span class="hljs-built_in">_</span>2&#125; <span class="hljs-built_in">&amp;</span> <br>1.724 <span class="hljs-built_in">&amp;</span> 9.3 <span class="hljs-built_in">&amp;</span> 91.0 <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span> <br> <span class="hljs-built_in">&amp;</span>  <span class="hljs-built_in">&amp;</span>  <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span><br> <span class="hljs-built_in">&amp;</span>  <span class="hljs-built_in">&amp;</span>  <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span><br><span class="hljs-keyword">\\</span><br><br><br><span class="hljs-keyword">\midrule</span><br><br>Ours <span class="hljs-built_in">&amp;</span><br>1.772 <span class="hljs-built_in">&amp;</span> 8.7 <span class="hljs-built_in">&amp;</span> 91.4  <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span><br><span class="hljs-built_in">&amp;</span>  <span class="hljs-built_in">&amp;</span>   <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span><br><span class="hljs-built_in">&amp;</span>  <span class="hljs-built_in">&amp;</span>  <span class="hljs-keyword">\textbf</span>&#123;1.4&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span><br><br><span class="hljs-keyword">\\</span><br><br><span class="hljs-keyword">\bottomrule</span><br><br><span class="hljs-keyword">\end</span>&#123;tabular&#125;<br><br><br><span class="hljs-comment">% from `main.tex` </span><br><span class="hljs-keyword">\begin</span>&#123;table*&#125;[t!]<br>    <span class="hljs-keyword">\centering</span><br>    <span class="hljs-keyword">\caption</span>&#123;<br>        <span class="hljs-keyword">\textbf</span>&#123;Main Performance&#125;<br>        of <span class="hljs-keyword">\method</span>&#123;&#125; with 123<br>    &#125;<br>    <span class="hljs-keyword">\resizebox</span>&#123;0.9<span class="hljs-keyword">\linewidth</span>&#125;&#123;!&#125;&#123;<br><span class="hljs-keyword">\input</span>&#123;tbl/tb1<span class="hljs-built_in">_</span>main<span class="hljs-built_in">_</span>table&#125;<br><span class="hljs-keyword">\label</span>&#123;table:main<span class="hljs-built_in">_</span>perform&#125;<br>    &#125;<br>    <span class="hljs-keyword">\\</span><br>    <span class="hljs-keyword">\begin</span>&#123;minipage&#125;&#123;<span class="hljs-keyword">\linewidth</span>&#125;<br>        <span class="hljs-keyword">\scriptsize</span><br>        <span class="hljs-keyword">\vspace</span>&#123;0.4em&#125;<br>        <span class="hljs-keyword">\begin</span>&#123;itemize&#125;<br>        <span class="hljs-keyword">\item</span>[<span class="hljs-built_in">$</span><span class="hljs-built_in">^</span><span class="hljs-keyword">\dagger</span><span class="hljs-built_in">$</span>]<br>        Most baselines are sourced from Fooo<span class="hljs-built_in">_</span>0~<span class="hljs-keyword">\cite</span>&#123;Foo<span class="hljs-built_in">_</span>0&#125;, except for the A benchmark. <br>        <span class="hljs-keyword">\end</span>&#123;itemize&#125;<br>    <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br><span class="hljs-keyword">\end</span>&#123;table*&#125;<br><br></code></pre></td></tr></table></figure><p>补充：</p><ol type="1"><li>值得学习的地方：①把<code>table</code>环境和<code>tabular</code>环境分开，分到两个文件方便层次化的管理，避免一个文件内内容太杂。②可以在表格末尾插入<code>minipage</code>,放一些想要添加的描述说明；表格标题放在<code>tabular</code>的上面。 ③通过增加额外的空白列实现一定的分隔效果。④使用<code>\makecell{}</code>命令，当单元格内容较长时在该环境里可以实现更轻松的直接换行，不需要外部重新计算<code>multirow</code>或<code>multicolumn</code>拆分为多少个。</li></ol><hr /><p>E.g. 3</p><p>https://blog.csdn.net/qq_34823530/article/details/112504329Latex复杂三线图的处理</p><iframe src="https://blog.csdn.net/qq_34823530/article/details/112504329" width="100%" height="500" name="topFrame" scrolling="yes" noresize="noresize" frameborder="0" id="topFrame"></iframe><figure><img src="https://s2.loli.net/2023/01/08/AhXYQdcDwpnTeBK.png"alt="Table4" /><figcaption aria-hidden="true">Table4</figcaption></figure><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-comment">% multirow,multicolumn 使用</span><br><span class="hljs-comment">% \cline&#123;2-6&#125;  % 这样使用cline能画一条横线在2-6 排之间</span><br><span class="hljs-keyword">\begin</span>&#123;table*&#125;<br><span class="hljs-keyword">\centering</span><br>        <span class="hljs-keyword">\resizebox</span>&#123;1.0<span class="hljs-keyword">\textwidth</span>&#125;&#123;!&#125;&#123;<br>    <span class="hljs-keyword">\begin</span>&#123;tabular&#125;&#123;ccccccc&#125; <span class="hljs-comment">% 控制表格的格式，7列</span><br>    <span class="hljs-keyword">\toprule</span> <br>    <span class="hljs-keyword">\multirow</span>&#123;2&#125;&#123;*&#125;&#123;<span class="hljs-keyword">\textbf</span>&#123;Model&#125;&#125; <span class="hljs-built_in">&amp;</span><span class="hljs-keyword">\multirow</span>&#123;2&#125;&#123;*&#125;&#123;<span class="hljs-keyword">\textbf</span>&#123;Skipped&#125;&#125;<span class="hljs-built_in">&amp;</span>    <span class="hljs-keyword">\multirow</span>&#123;2&#125;&#123;*&#125;&#123;<span class="hljs-keyword">\textbf</span>&#123;Dilated&#125;&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\multirow</span>&#123;2&#125;&#123;*&#125;&#123;<span class="hljs-keyword">\textbf</span>&#123;Attention&#125;&#125;<span class="hljs-built_in">&amp;</span><span class="hljs-keyword">\multirow</span>&#123;2&#125;&#123;*&#125;&#123;<span class="hljs-keyword">\textbf</span>&#123;Fine-Tune&#125;&#125;  <span class="hljs-built_in">&amp;</span> <br>    <span class="hljs-keyword">\multicolumn</span>&#123;2&#125;&#123;c&#125;&#123;<span class="hljs-keyword">\textbf</span>&#123;Accuracy&#125;&#125;<br>    <span class="hljs-keyword">\\</span><span class="hljs-built_in">&amp;</span><span class="hljs-built_in">&amp;</span><span class="hljs-built_in">&amp;</span><span class="hljs-built_in">&amp;</span><span class="hljs-built_in">&amp;</span>5-way 1-shot  <span class="hljs-built_in">&amp;</span> 5-way 5-shot  <span class="hljs-keyword">\\</span><br>    <span class="hljs-keyword">\midrule</span><br>    <span class="hljs-keyword">\textbf</span>&#123;Relation Net &#125;<span class="hljs-built_in">&amp;</span>  <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span>   <span class="hljs-built_in">&amp;</span>  <span class="hljs-built_in">&amp;</span>   <span class="hljs-built_in">&amp;</span> 0.8449 <span class="hljs-built_in">&amp;</span>0.8325 <span class="hljs-keyword">\\</span><br>    <span class="hljs-keyword">\textbf</span>&#123;Relation Net&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span>   <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span>  <span class="hljs-built_in">&amp;</span> 0.8950<span class="hljs-built_in">&amp;</span>0.8488 <span class="hljs-keyword">\\</span><br>    <span class="hljs-keyword">\textbf</span>&#123;Relation Net&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span>  <span class="hljs-built_in">&amp;</span>  <span class="hljs-built_in">&amp;</span>   <span class="hljs-built_in">&amp;</span>0.8624<span class="hljs-built_in">&amp;</span>0.8450 <span class="hljs-keyword">\\</span><br>    <span class="hljs-keyword">\textbf</span>&#123;Relation Net&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span>  <span class="hljs-built_in">&amp;</span>  <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span>  <span class="hljs-built_in">&amp;</span> 0.8866<span class="hljs-built_in">&amp;</span>0.8647 <span class="hljs-keyword">\\</span><br>    <span class="hljs-keyword">\textbf</span>&#123;Relation Net&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span>  <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span>   <span class="hljs-built_in">&amp;</span> 0.8532<span class="hljs-built_in">&amp;</span>0.8599 <span class="hljs-keyword">\\</span><br>    <span class="hljs-keyword">\textbf</span>&#123;Relation Net&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span>  <span class="hljs-built_in">&amp;</span> 0.9268<span class="hljs-built_in">&amp;</span>0.8925 <span class="hljs-keyword">\\</span><br>    <span class="hljs-keyword">\cline</span>&#123;1-7&#125; <span class="hljs-comment">%画一条横线</span><br>            <span class="hljs-comment">% \hline</span><br>    <span class="hljs-keyword">\textbf</span>&#123;Our model&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span>  <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-built_in">&amp;</span> 0.8990<span class="hljs-built_in">&amp;</span>0.8537 <span class="hljs-keyword">\\</span><br>    <span class="hljs-keyword">\textbf</span>&#123;Our model&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span>  <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\checkmark</span><span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\textbf</span>&#123;0.9486&#125;<span class="hljs-built_in">&amp;</span><span class="hljs-keyword">\textbf</span>&#123;0.9039&#125; <span class="hljs-keyword">\\</span><br>    <span class="hljs-keyword">\bottomrule</span><br>    <span class="hljs-keyword">\end</span>&#123;tabular&#125;<br>        &#125;<br><span class="hljs-keyword">\label</span>&#123;tbl:table1&#125;<br><span class="hljs-keyword">\caption</span>&#123;Comparison of different obfuscations in terms of their transformation capabilities&#125;<br><span class="hljs-keyword">\end</span>&#123;table*&#125;<br></code></pre></td></tr></table></figure><p>补充:</p><p>​1.为了好看方便对齐，在源代码基础上增加了<code>\resize{}{}{}</code>控制命令。</p><ol start="2" type="1"><li><p><code>\cline{}</code>命令和<code>\hline</code>命令不同之处：</p><blockquote><p>The command produces a horizontal line extending across the entiretable. It can only be issued after a line break \. The command draws ahorizontal line from the left side of column n to the right side ofcolumn m. It may only be issued after a line break, and several can beissued atonce.<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="https://instruct.math.lsa.umich.edu/support/latex/tables.tex">[1]</span></a></sup></p></blockquote></li><li><p>注意图表中使用的对号<code>\checkmark</code>命令需要引用<code>\usepackage{amssymb}</code>宏包。此外对号也有不同的风格<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="https://tex.stackexchange.com/questions/132783/how-to-write-checkmark-in-latex">[2]</span></a></sup></p><figure><img src="https://i.stack.imgur.com/Qgpj3.png" alt="checkmart" /><figcaption aria-hidden="true">checkmart</figcaption></figure><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\documentclass</span>&#123;article&#125;<br><span class="hljs-keyword">\usepackage</span>&#123;bbding&#125;<br><span class="hljs-keyword">\usepackage</span>&#123;pifont&#125;<br><span class="hljs-keyword">\usepackage</span>&#123;wasysym&#125;<br><span class="hljs-keyword">\usepackage</span>&#123;amssymb&#125;<br><br><span class="hljs-keyword">\begin</span>&#123;document&#125;<br><br><span class="hljs-comment">% amssymb</span><br><span class="hljs-keyword">\checkmark</span><br><br><span class="hljs-comment">% bbding</span><br><span class="hljs-keyword">\Checkmark</span><br><span class="hljs-keyword">\CheckmarkBold</span><br><br><span class="hljs-comment">% pifont</span><br><span class="hljs-keyword">\ding</span>&#123;51&#125;<br><span class="hljs-keyword">\ding</span>&#123;52&#125;<br><br><span class="hljs-comment">% wasysym</span><br><span class="hljs-keyword">\CheckedBox</span><br><br><span class="hljs-keyword">\end</span>&#123;document&#125;<br></code></pre></td></tr></table></figure></li></ol><h2 id="参考资料">参考资料</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1"class="footnote-text"><span>https://instruct.math.lsa.umich.edu/support/latex/tables.tex<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2"class="footnote-text"><span>https://tex.stackexchange.com/questions/132783/how-to-write-checkmark-in-latex<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3"class="footnote-text"><span>https://www.cnblogs.com/ybl20000418/p/11478324.html<a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:4"class="footnote-text"><span>https://zhuanlan.zhihu.com/p/580123090<a href="#fnref:4" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>LaTex学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LaTex</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LaTeX讲解系列(3)：如何插入代码块/伪代码</title>
    <link href="/posts/200e31c0/"/>
    <url>/posts/200e31c0/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="latex讲解系列3-如何插入代码块伪代码">LaTeX讲解系列(3)：如何插入代码块/伪代码</h1><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="//music.163.com/outchain/player?type=2&amp;id=28708806&amp;auto=0&amp;height=66"></iframe><div class="note note-primary">            <p>资料更新：用LaTeX优雅地书写伪代码——Algorithm2e简明指南</p><p>https://zhuanlan.zhihu.com/p/166418214</p><p>这篇网上找到的知乎讲的非常好，简明常用优雅~建议阅读</p>          </div><h2 id="一latex-插入代码块的方法">一、LaTex 插入代码块的方法</h2><p>参考资料：overleaf官方文档(https://www.overleaf.com/learn/latex/Code_listing)</p><p><strong>方法1 使用verbatim环境</strong></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;verbatim&#125;<span class="hljs-string"></span><br><span class="hljs-string">% 插入你的代码</span><br><span class="hljs-string"></span><span class="hljs-keyword">\end</span>&#123;verbatim&#125;<br></code></pre></td></tr></table></figure><p>但是这种方式插入的效果是最朴素的，没有代码高亮、关键字边框，字体大小和行间距调整等格式设置。就是朴素的单倍行距显示，比较丑陋。</p><p><strong>方法2 使用listings环境进行优化</strong></p><p>step-1 引入宏包 <code>\usepackage&#123;listings&#125;</code>,颜色宏包<code>\usepackage&#123;xcolor&#125;</code>.</p><p>step-2 设置格式，下方通过宏定义的方式定义了<code>mystyle</code></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\lstdefinestyle</span>&#123;mystyle&#125;&#123;<br>    <span class="hljs-comment">% backgroundcolor=\color&#123;backcolour&#125;,% 背景色   </span><br>    keywordstyle= <span class="hljs-keyword">\color</span>&#123; blue!70&#125;,<span class="hljs-comment">% 关键字/程序语言中的保留字颜色</span><br>    commentstyle= <span class="hljs-keyword">\color</span>&#123;red!50!green!50!blue!50&#125;,<span class="hljs-comment">% 程序中注释的颜色</span><br>    <span class="hljs-comment">% commentstyle= \color[RGB]&#123;40, 400, 255&#125;</span><br>    numberstyle=<span class="hljs-keyword">\tiny</span><span class="hljs-keyword">\color</span>&#123;codegray&#125;,<span class="hljs-comment">% 左侧行号显示的颜色</span><br>    stringstyle=<span class="hljs-keyword">\color</span>&#123;codepurple&#125;,<br>    basicstyle=<span class="hljs-keyword">\ttfamily</span><span class="hljs-keyword">\footnotesize</span>,<br>    breakatwhitespace=false,         <br>    breaklines=true,<span class="hljs-comment">% 对过长的代码自动换行                </span><br>    captionpos=b,                    <br>    keepspaces=true,                 <br>    numbers=left,<span class="hljs-comment">% 在左侧显示行号                 </span><br>    numbersep=5pt,                  <br>    showspaces=false,                <br>    showstringspaces=false,<span class="hljs-comment">% 不显示字符串中的空格</span><br>    showtabs=false,                  <br>    tabsize=2,<br>    <span class="hljs-comment">% frame=none,% 不显示边框</span><br>    frame=shadowbox,<span class="hljs-comment">% 边框阴影</span><br>    <span class="hljs-comment">%   escapebegin=\begin&#123;CJK*&#125;,escapeend=\end&#123;CJK*&#125;,      % 代码中出现中文必须加上，否则报错</span><br>&#125;<br></code></pre></td></tr></table></figure><p><strong>step -3 导入代码</strong></p><p>可以从源文件导入，只需要一行命令如下:</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\lstinputlisting</span>[language=Python, style=mystyle ]&#123;ReduceClass.py&#125;<br></code></pre></td></tr></table></figure><p>填写好语言项(<code>languange=Python</code>),选择需要的格式(<code>style=mystyle</code>,假如定义了多个style可以选择一个来用)，最后在花括号里选择引用的代码文件。</p><p>也可以选择创建一个环境，如下：</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>[languange=Python]&#123;lstlisting&#125;<br><span class="hljs-comment">% 代码块</span><br><span class="hljs-keyword">\end</span>&#123;lstlisting&#125;<br></code></pre></td></tr></table></figure><h2 id="二latex中插入伪代码的方法">二、LaTex中插入伪代码的方法</h2><h3 id="易出错的地方总结">2.1 易出错的地方总结</h3><ol type="1"><li>注意换行的时候， <spanstyle="color:orange"><strong>可以末尾用<code>\;</code>，或者多空一行，但是不要像写公式换行一行用<code>//</code></strong></span>。<code>//</code>只是段内增加一行，不会开启新一行的编号。往往太长了放不下的时候可以<code>//</code>，如果要写下一行code还是用上述前两种方法。</li></ol><h3 id="插入伪代码的方法">2.2 插入伪代码的方法</h3><p>常用的伪代码宏包有<code>algorithm</code>, <code>algorithm2e</code>,<code>algorithmic</code>等等。那么如何选择使用呢？首先，如果有论文投稿计划的话，按照相关期刊/会议要求LaTex使用伪代码的规定使用哪些宏包；如果没有明确规定，那么怎么顺手怎么来。</p><p><strong><code>algorithm</code>宏包</strong>，支持简单的伪代码格式。</p><p>基本格式如下:</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;algorithm&#125;[htbp]<br><span class="hljs-keyword">\KwIn</span>&#123;XXX&#125;<span class="hljs-comment">% 算法输入</span><br>    <span class="hljs-keyword">\KwOut</span>&#123;XXX&#125;<span class="hljs-comment">% 算法输出</span><br>    <span class="hljs-keyword">\BlankLine</span><br>    <span class="hljs-keyword">\caption</span>&#123;XXX&#125;<span class="hljs-comment">% 算法标题</span><br>    <span class="hljs-keyword">\label</span>&#123;&#125; <span class="hljs-comment">% 定义该段算法的标签</span><br>    <br>    <span class="hljs-comment">%写伪代码</span><br><span class="hljs-keyword">\end</span>&#123;algorithm&#125;<br></code></pre></td></tr></table></figure><div class="note note-danger">            <p>algorithm算法环境中<code>\label{}</code>尽量紧跟在<code>\caption{}</code>后面,而且五个命令的顺序尽可能和示例中一样，不然使用<code>\cref{}</code>时可能会出现奇怪的问题。</p><p>聪明的联网版GPT4o给的解释是:在 LaTeX 中，<code>\label</code>的位置很重要，因为它引用的是最近的编号对象。如果 <code>\label</code> 在<code>\caption</code>之前，可能会默认引用到前一个环境的编号或行号，导致错误。放在<code>\caption</code>后面可以确保它引用的是当前算法的编号。这样就能正确地使用<code>cleveref</code> 进行引用。确保 <code>\label</code>紧跟在产生编号的命令之后是个好习惯，以避免类似问题。</p>          </div><p><strong><code>algorithm2e</code>宏包</strong></p><p>详细讲解强烈推荐这篇博客：https://blog.csdn.net/qq_43486745/article/details/124344365</p><p>部分内容从该博客里“借鉴”，结合本人一些实践经验补充。</p><p>Step-0 宏包参数使用</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\usepackage</span>[linesnumbered,ruled,vlined]&#123;algorithm2e&#125;<br></code></pre></td></tr></table></figure><table><thead><tr><th>参数</th><th>作用</th></tr></thead><tbody><tr><td>linesnumbered</td><td>显示行号</td></tr><tr><td>ruled</td><td>标题显示在上方，不加就默认显示在下方</td></tr><tr><td>vlined</td><td>代码段中用线连接</td></tr><tr><td>boxed</td><td>将算法插入在一个盒子里</td></tr></tbody></table><div class="note note-warning">            <p>如果遇到一篇文档里有多段伪代码，第二段伪代码的行号没有重新开始计数的情况时，可以采用如下办法(Poweredby smart GPT4o)：</p><ul><li>如果是在algorithm2e环境中，可以在每段伪代码前加入<code>\setcounter{AlgoLine}{0}</code></li><li>如果是在algorithmic环境中，可以在每段伪代码前加入<code>\setcounter{ALC@line}{0}</code></li></ul>          </div><p>step-1包裹在<code>algorithm</code>环境中(同时也要有<code>algorithm宏包</code>)</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\usepackage</span>[vlined,ruled,linesnumbered]&#123;algorithm2e&#125;<br></code></pre></td></tr></table></figure><p>step-2 For循环怎么写</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs LaTex"><span class="hljs-keyword">\For</span>&#123;<span class="hljs-built_in">$</span>i <span class="hljs-keyword">\leftarrow</span> 1<span class="hljs-built_in">$</span> to <span class="hljs-built_in">$</span>100<span class="hljs-built_in">$</span>&#125;&#123;<br>    XXX<span class="hljs-comment">% 循环体内容</span><br>&#125;<br></code></pre></td></tr></table></figure><p>step-3 While循环怎么写</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\While</span>&#123;a &gt; 0&#125;&#123;<br>XXX<span class="hljs-comment">% 循环体内容</span><br>&#125;<br></code></pre></td></tr></table></figure><p>step-4 If-else 判断语句</p><p>if…then…</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\If</span>&#123;<span class="hljs-built_in">$</span>i&gt;1<span class="hljs-built_in">$</span>&#125;&#123;<span class="hljs-built_in">$</span><span class="hljs-keyword">\operatorname</span>&#123;QuickSort&#125;(A[1,<span class="hljs-keyword">\cdots</span>,i-1])<span class="hljs-built_in">$</span>&#125;<br></code></pre></td></tr></table></figure><p>if…then…else…</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\eIf</span>&#123;<span class="hljs-built_in">$</span>j<span class="hljs-built_in">$</span> is odd&#125;&#123;<br>if执行以及<br>&#125;&#123;else执行语句&#125;<br></code></pre></td></tr></table></figure><p>step-5 定义子函数</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\SetKwFunction</span>&#123;FMain&#125;&#123;你定义的函数名，如gcd&#125;<br><span class="hljs-keyword">\SetKwProg</span>&#123;Fn&#125;&#123;Function&#125;&#123;:&#125;&#123;&#125;<br><span class="hljs-keyword">\Fn</span>&#123;<span class="hljs-keyword">\FMain</span>&#123;函数的参变量,如int a, int b&#125;&#125;&#123;<br>XXX<span class="hljs-comment">% 函数内容</span><br>&#125;<br><span class="hljs-keyword">\textbf</span>&#123;End Function&#125;<br>...<br>c <span class="hljs-keyword">\leftarrow</span> gcd(a, b) <span class="hljs-comment">% 在某个地方调用</span><br></code></pre></td></tr></table></figure><p>step-6 其他</p><table><thead><tr><th style="text-align: center;">代码</th><th style="text-align: center;">作用</th></tr></thead><tbody><tr><td style="text-align: center;">;</td><td style="text-align: center;">行末添加行号并自动换行</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;">插入算法名称</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;">显示“Data：输入信息”</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;">显示“Output：输出信息”</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;">显示“Result：输出信息”</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;">For循环</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;">If条件判断</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;">If-else判断语句</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;">While循环</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;">ForEach遍历</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;">显示“* 注释 *”</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;">显示“\注释”</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;">显示“每个结尾的end”</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;">显示行号</td></tr><tr><td style="text-align: center;"></td><td style="text-align: center;"></td></tr></tbody></table><p>step-7 修改算法部分为中文</p><p>使用以下语句可将默认的“Algorithm”修改为中文“算法”</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs LaTex"><span class="hljs-keyword">\renewcommand</span>&#123;<span class="hljs-keyword">\algorithmcfname</span>&#125;&#123;算法&#125;<br></code></pre></td></tr></table></figure><p>修改Input、Output为中文</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\SetKwInOut</span>&#123;KwIn&#125;&#123;输入&#125;<br><span class="hljs-keyword">\SetKwInOut</span>&#123;KwOut&#125;&#123;输出&#125;<br></code></pre></td></tr></table></figure><p>自定义算法编号</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs LaTex"><span class="hljs-keyword">\renewcommand</span>&#123;<span class="hljs-keyword">\thealgocf</span>&#125;&#123;3-1&#125;<br></code></pre></td></tr></table></figure><p>添加算法目录</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs LaTex"><span class="hljs-keyword">\renewcommand</span>&#123;<span class="hljs-keyword">\listalgorithmcfname</span>&#125;&#123;算<span class="hljs-keyword">\ </span>法<span class="hljs-keyword">\ </span>目<span class="hljs-keyword">\ </span>录&#125;<br><span class="hljs-comment">% 生成算法目录命令</span><br><span class="hljs-keyword">\listofalgorithms</span><br></code></pre></td></tr></table></figure><hr /><p>e.g.1. 冒泡排序</p><p>效果图：</p><p><img src="https://s2.loli.net/2022/12/29/p3NC4QEujBvq9Fd.png" style="zoom:80%;" /></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;algorithm&#125;[H]<br>    <span class="hljs-keyword">\KwIn</span>&#123;An array <span class="hljs-built_in">$</span>A[1,<span class="hljs-keyword">\dots</span>,n]<span class="hljs-built_in">$</span>&#125;<br>    <span class="hljs-keyword">\KwOut</span>&#123;<span class="hljs-built_in">$</span>A<span class="hljs-built_in">$</span> sorted nondecreasingly&#125;<br>    <span class="hljs-keyword">\BlankLine</span><br>    <span class="hljs-keyword">\caption</span>&#123;BubbleSort&#125;<span class="hljs-keyword">\label</span>&#123;Alg-Bubble&#125;<br><br>    <span class="hljs-built_in">$</span>i<span class="hljs-keyword">\leftarrow</span> 1<span class="hljs-built_in">$</span>; <span class="hljs-built_in">$</span>sorted<span class="hljs-keyword">\leftarrow</span> false<span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br><br>    <span class="hljs-keyword">\While</span>&#123;<span class="hljs-built_in">$</span>i<span class="hljs-keyword">\leq</span> n-1<span class="hljs-built_in">$</span> <span class="hljs-keyword">\textbf</span>&#123;and not&#125; <span class="hljs-built_in">$</span>sorted<span class="hljs-built_in">$</span>&#125;&#123;<br>        <span class="hljs-built_in">$</span>sorted<span class="hljs-keyword">\leftarrow</span> true<span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br>        <span class="hljs-keyword">\For</span>&#123;<span class="hljs-built_in">$</span>j<span class="hljs-keyword">\leftarrow</span> n <span class="hljs-built_in">$</span> <span class="hljs-keyword">\textbf</span>&#123;downto&#125; <span class="hljs-built_in">$</span>i+1<span class="hljs-built_in">$</span>&#125;&#123;<br>            <span class="hljs-keyword">\If</span>&#123;<span class="hljs-built_in">$</span>A[j]&lt;A[j-1]<span class="hljs-built_in">$</span>&#125;&#123;<br>                interchange <span class="hljs-built_in">$</span>A[j]<span class="hljs-built_in">$</span> and <span class="hljs-built_in">$</span>A[j-1]<span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br>                <span class="hljs-built_in">$</span>sorted<span class="hljs-keyword">\leftarrow</span> false<span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br>            &#125;<br>        &#125;<br>        <span class="hljs-built_in">$</span>i<span class="hljs-keyword">\leftarrow</span> i+1<span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br>    &#125;<br><span class="hljs-keyword">\end</span>&#123;algorithm&#125;<br></code></pre></td></tr></table></figure><p>e.g.2. 大数据分析中经典的AMS-F2估计算法(并使用Median Trick, AverageTrick)</p><p>效果图：</p><p><img src="https://s2.loli.net/2022/12/29/oOpJuM1kHL3mbcr.png" style="zoom: 80%;" /></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;algorithm&#125;[H]<br>        <span class="hljs-keyword">\KwIn</span>&#123;4-wise independent hash family <span class="hljs-built_in">$</span>h<span class="hljs-built_in">$</span>, stream <span class="hljs-built_in">$</span>a<span class="hljs-built_in">_</span>i <span class="hljs-keyword">\in</span> [n]<span class="hljs-built_in">$</span>, <span class="hljs-built_in">$</span><span class="hljs-keyword">\epsilon</span><span class="hljs-built_in">$</span>, <span class="hljs-built_in">$</span><span class="hljs-keyword">\eta</span><span class="hljs-built_in">$</span>, <span class="hljs-built_in">$</span><span class="hljs-keyword">\delta</span><span class="hljs-built_in">$</span>&#125;<br>        <span class="hljs-keyword">\KwOut</span>&#123;<span class="hljs-built_in">$</span>z<span class="hljs-built_in">^</span>2<span class="hljs-built_in">$</span>&#125;<br>        <span class="hljs-keyword">\BlankLine</span><br>        <span class="hljs-keyword">\caption</span>&#123;AMS Scheme for <span class="hljs-built_in">$</span>F<span class="hljs-built_in">_</span>2<span class="hljs-built_in">$</span>&#125;<br>        <br>        <span class="hljs-comment">% 设置子函数</span><br>        <span class="hljs-keyword">\SetKwFunction</span>&#123;FMain&#125;&#123;AMS-F2&#125;<br>        <span class="hljs-keyword">\SetKwProg</span>&#123;Fn&#125;&#123;Function&#125;&#123;:&#125;&#123;&#125;<br>        <span class="hljs-keyword">\Fn</span>&#123;<span class="hljs-keyword">\FMain</span>&#123;stream <span class="hljs-built_in">$</span>a<span class="hljs-built_in">_</span>i<span class="hljs-built_in">$</span>, hash family <span class="hljs-built_in">$</span>h(<span class="hljs-keyword">\cdot</span>)<span class="hljs-built_in">$</span>&#125;&#125;&#123;<br>            <span class="hljs-built_in">$</span> z <span class="hljs-keyword">\longleftarrow</span> 0 <span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span>  <br><br>            <span class="hljs-keyword">\While</span>&#123;stream <span class="hljs-built_in">$</span>a<span class="hljs-built_in">_</span>i<span class="hljs-built_in">$</span> is not empty&#125;&#123;<br>                <span class="hljs-built_in">$</span>a<span class="hljs-built_in">_</span>i<span class="hljs-built_in">$</span> is current item<span class="hljs-keyword">\;</span><br><br>                <span class="hljs-built_in">$</span>z <span class="hljs-keyword">\longleftarrow</span> z + h(a<span class="hljs-built_in">_</span>i)<span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br>            &#125;<br><br>            <span class="hljs-keyword">\textbf</span>&#123;return&#125; <span class="hljs-built_in">$</span> z<span class="hljs-built_in">^</span>2<span class="hljs-built_in">$</span> <br>        &#125;<br>        <span class="hljs-keyword">\textbf</span>&#123;End Function&#125;<br><br>// apply average trick<br><br><span class="hljs-built_in">$</span>q <span class="hljs-keyword">\longleftarrow</span> <span class="hljs-keyword">\lceil</span> <span class="hljs-keyword">\frac</span>&#123;4&#125;&#123;<span class="hljs-keyword">\eta</span> <span class="hljs-keyword">\epsilon</span><span class="hljs-built_in">^</span>2&#125; <span class="hljs-keyword">\rceil</span><span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br><br><span class="hljs-built_in">$</span>sum<span class="hljs-built_in">_</span>1 <span class="hljs-keyword">\longleftarrow</span> 0<span class="hljs-built_in">$</span><br><br><span class="hljs-keyword">\For</span>&#123;<span class="hljs-built_in">$</span>j <span class="hljs-keyword">\leftarrow</span> 1<span class="hljs-built_in">$</span> to <span class="hljs-built_in">$</span>q<span class="hljs-built_in">$</span>&#125;&#123;<br>    <span class="hljs-built_in">$</span>sum<span class="hljs-built_in">_</span>1 <span class="hljs-keyword">\longleftarrow</span> sum<span class="hljs-built_in">_</span>1 +<span class="hljs-built_in">$</span> AMS-F2(stream <span class="hljs-built_in">$</span>a<span class="hljs-built_in">_</span>i<span class="hljs-built_in">$</span>, <span class="hljs-built_in">$</span>h(<span class="hljs-keyword">\cdot</span>)<span class="hljs-built_in">$</span>)<br>&#125;<br><br><span class="hljs-built_in">$</span>avg <span class="hljs-keyword">\longleftarrow</span> sum<span class="hljs-built_in">_</span>1 / q<span class="hljs-built_in">$</span><br><br>//apply median trick<br><br><span class="hljs-built_in">$</span>s <span class="hljs-keyword">\longleftarrow</span> <span class="hljs-keyword">\lceil</span> <span class="hljs-keyword">\frac</span>&#123;3<span class="hljs-keyword">\eta</span><span class="hljs-built_in">^</span>2&#125;&#123;(<span class="hljs-keyword">\frac</span>&#123;1&#125;&#123;2&#125; - <span class="hljs-keyword">\eta</span>)<span class="hljs-built_in">^</span>2&#125;<span class="hljs-keyword">\ln</span><span class="hljs-keyword">\frac</span>&#123;2&#125;&#123;<span class="hljs-keyword">\delta</span>&#125; <span class="hljs-keyword">\rceil</span><span class="hljs-built_in">$</span><br><br>create a empty set <span class="hljs-built_in">$</span><span class="hljs-keyword">\Phi</span><span class="hljs-built_in">$</span><br><br><span class="hljs-keyword">\For</span>&#123;<span class="hljs-built_in">$</span>j <span class="hljs-keyword">\leftarrow</span> 1<span class="hljs-built_in">$</span> to <span class="hljs-built_in">$</span>s<span class="hljs-built_in">$</span>&#125;&#123;<br>    Add the result of AMS-F2(stream <span class="hljs-built_in">$</span>a<span class="hljs-built_in">_</span>i<span class="hljs-built_in">$</span>, <span class="hljs-built_in">$</span>h(<span class="hljs-keyword">\cdot</span>)<span class="hljs-built_in">$</span>) into set <span class="hljs-built_in">$</span><span class="hljs-keyword">\Phi</span><span class="hljs-built_in">$</span><br>&#125;<br><br><span class="hljs-built_in">$</span>med <span class="hljs-keyword">\longleftarrow</span><span class="hljs-built_in">$</span> the smallest median of <span class="hljs-built_in">$</span><span class="hljs-keyword">\Phi</span><span class="hljs-built_in">$</span><br><br><span class="hljs-keyword">\Return</span>&#123;<span class="hljs-built_in">$</span>avg<span class="hljs-built_in">$</span>, <span class="hljs-built_in">$</span>med<span class="hljs-built_in">$</span>&#125; <span class="hljs-keyword">\;</span><br><span class="hljs-keyword">\end</span>&#123;algorithm&#125;<br></code></pre></td></tr></table></figure><p><strong><code>algorithmic</code>宏包</strong></p><p>占个坑，以后填</p><p>(感觉这个宏包不太方便定义指令)</p>]]></content>
    
    
    <categories>
      
      <category>LaTex学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LaTex</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一种简单优雅的Map-Reduce手工实现方式(Python)</title>
    <link href="/posts/77084934/"/>
    <url>/posts/77084934/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="一种简单优雅的map-reduce手工实现方式python">一种简单优雅的Map-Reduce手工实现方式(Python)</h1><p>参考链接：https://blog.csdn.net/weixin_45203607/article/details/127369408Python-多进程(进程,锁,通讯,进程池)https://blog.csdn.net/HaidongDU/article/details/112795797 Python多线程中的join用法</p><h2 id="map-reudce-原理">Map-Reudce 原理</h2><p>Map-Reduce是谷歌提出的，用于处理大数据文本输入进行统计的计算框架。其总体思路简单巧妙，所有的输入文本统一通过<code>map</code>和<code>reduce</code>两段计算过程便可以得到自己想要的统计结果(如词频统计，排序分类等等)。<code>map</code>的计算过程是将输入的流式数据(如文本流，访问流等等)通过某种规则映射到<code>key-value</code>键值对的形式。例如</p><blockquote><p>词频分析：Map计算过程</p><p>输入：“It was the best of times, it was the worst of times. ”</p><p>输出： &lt;it, 2&gt;, &lt;was, 2&gt;, &lt;the, 2&gt;, &lt;of, 2&gt;,&lt;times, 2&gt;</p></blockquote><p>这个过程可以在成千上百个分布式节点上部署，每个节点分别处理海量的文件，然后得到如上键值对形式。经过中间层和网络通信将各个节点的键值对数据shuffle在一起(比如最简单的保存在本地临时存储文件，然后传输这谢谢文件)。</p><p><code>reduce</code>的过程便是将这些中间的键值对结果合并、分类，得到最终的统计结果。这个过程也可以在多个计算节点完成，例如有26个节点负责处理reduce过程，每个节点代表<code>a</code>~<code>z</code>的一个首字母。<code>key</code>所代表的单词首字母传输到符合的<code>reduce</code>节点里，然后将对应的<code>value</code>（例如这里是词频）合并。当然<code>reduce</code>节点的划分要考虑因素比较多，比如考虑负载均衡的问题。因为有些节点可能分到的任务比其他的节点多，(假如<code>b</code>开头的单词会比<code>u</code>开头的单词多，那么上述的划分方式就会导致负载不均衡。</p><blockquote><p>Map节点1输出: &lt;it, 2&gt;, &lt;was, 2&gt;, &lt;the, 2&gt;, &lt;of,2&gt;, &lt;times, 2&gt;</p><p>Map节点2输出：&lt;he, 1&gt;, &lt;was, 1&gt;, &lt;taller,1&gt;,&lt;than, 1&gt;, &lt;Mike, 1&gt;</p><p>Reduce节点1结果(负责<code>a</code>~<code>n</code>)：&lt;he, 1&gt;,&lt;it, 2&gt;, &lt;Mike, 1&gt;</p><p>Reduce 节点2结果(负责<code>o</code>~<code>z</code>)：&lt;of, 2&gt;,&lt;taller, 1&gt;, &lt;than, 1&gt;, &lt;the, 2&gt;, &lt;times, 2&gt;,&lt;was, 3&gt;</p></blockquote><h2 id="本demo的实现原理">本demo的实现原理</h2><p>本demo实现一个成绩平均分计算功能。输入文件格式如下:</p><blockquote><p>File1.txt:</p><p>小红 98.3</p><p>小明 85.7</p><p>…</p><p>小倩 86.4</p></blockquote><p>有多组如上图格式的输入文件代表每个学生在不同科目中取得的成绩。要求输出每个学生的平均分(<spanclass="math inline">$\frac{\text{总成绩}}{\text{参加的科目数量}}$</span>)。</p><p>实现结构式<code>multi-mapper one reducer</code>的结构，并利用多进程编程的方式模拟多个<code>mapper</code>处理节点。整个程序由<code>Mapper</code>类，<code>Reducer</code>类和<code>Manager</code>三部分构成。<code>Manager</code>脚本负责实例化多个<code>Mapper</code>和一个<code>Reducer</code>，并开启多个进程，每个进程负责一个计算节点。为方便演示，本demo中只设置了一个<code>Reducer</code>。若设置多个<code>Reducer</code>，需要额外实现对<code>key</code>值的划分功能。此外，中间网络传输部分采用通过字典格式保存到本地路径，再将路径的列表由<code>Mapper -&gt; Manager -&gt; Reducer</code>传递模拟网络传递过程。</p><h3 id="mapper类">Mapper类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> pickle<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Mapper</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;Map-Reduce Mapper</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        filepath (str): the path to load single text file for mapper</span><br><span class="hljs-string">        mid_save_path (str): the path to save intermediate result(don&#x27;t include filename)</span><br><span class="hljs-string">        worker_id (optional, int): id</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, filepath:<span class="hljs-built_in">str</span>, mid_save_path:<span class="hljs-built_in">str</span>, worker_id:<span class="hljs-built_in">int</span>=-<span class="hljs-number">1</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.file_path = filepath<br>        <span class="hljs-variable language_">self</span>.mid_save_path = mid_save_path<br>        <span class="hljs-variable language_">self</span>.score_dict = &#123;&#125; <span class="hljs-comment"># &#123;&quot;Lucy&quot;: &#123;&quot;total_score&quot;: 180, &quot;count&quot;: 2&#125;&#125;</span><br>        <span class="hljs-keyword">if</span> worker_id &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-variable language_">self</span>.worker_id = worker_id<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.worker_id = random.randint(<span class="hljs-number">100000</span>,<span class="hljs-number">10000000</span>) + random.randint(<span class="hljs-number">12</span>,<span class="hljs-number">100</span>)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_add_item</span>(<span class="hljs-params">self, name:<span class="hljs-built_in">str</span>, score:<span class="hljs-built_in">int</span></span>):<br>        <span class="hljs-keyword">if</span> name <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.score_dict.keys():<br>            entry = &#123;<span class="hljs-string">&quot;total&quot;</span>: score, <span class="hljs-string">&quot;count&quot;</span>: <span class="hljs-number">1</span>&#125;<br>        <span class="hljs-keyword">else</span>:<br>            entry = <span class="hljs-variable language_">self</span>.score_dict[name]<br>            entry[<span class="hljs-string">&quot;total&quot;</span>] += score<br>            entry[<span class="hljs-string">&quot;count&quot;</span>] += <span class="hljs-number">1</span><br>        <span class="hljs-variable language_">self</span>.score_dict[name] = entry<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Main Function of Mapper. </span><br><span class="hljs-string">        Load text from source files, split to name and score</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            save_path(str):return path where the key-value result is saved</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-variable language_">self</span>.file_path) <span class="hljs-keyword">as</span> f:<br>            line = f.readline()<br>            <span class="hljs-keyword">while</span> line != <span class="hljs-string">&quot;&quot;</span>:<br>                line = line.split()<br>                name, score = line[<span class="hljs-number">0</span>], line[<span class="hljs-number">1</span>]<br>                <span class="hljs-variable language_">self</span>._add_item(name, score)<br>                line = f.readline()<br>        <br>        filename = <span class="hljs-built_in">str</span>(<span class="hljs-variable language_">self</span>.worker_id) + <span class="hljs-string">&quot;_intermediate_results.pkl&quot;</span><br>        save_path = os.path.join(<span class="hljs-variable language_">self</span>.mid_save_path, filename)<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(save_path, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            pickle.dump(<span class="hljs-variable language_">self</span>.score_dict, f, pickle.HIGHEST_PROTOCOL)<br>        <span class="hljs-keyword">return</span> save_path<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    mapper = Mapper(filepath=<span class="hljs-string">&#x27;file&#123;&#125;.txt&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-number">1</span>), mid_save_path=<span class="hljs-string">&quot;./&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Mapper <span class="hljs-subst">&#123;mapper.worker_id&#125;</span> started...&quot;</span>)<br>    filename=mapper.run()<br></code></pre></td></tr></table></figure><h3 id="reducer-类">Reducer 类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> pickle<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Reducer</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;Map-Reduce Reducer</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        filepath_lists (List[str]): the path lists to load key-value data</span><br><span class="hljs-string">        save_path (str): the path to save final resuls(don&#x27;t include filename)</span><br><span class="hljs-string">        worker_id (optional, int): id</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, filepath_lists, save_path:<span class="hljs-built_in">str</span>, worker_id:<span class="hljs-built_in">int</span>=-<span class="hljs-number">1</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.filepath_lists = filepath_lists<br>        <span class="hljs-variable language_">self</span>.save_path = save_path<br>        <span class="hljs-variable language_">self</span>.result_dict = &#123;&#125;<br>        <span class="hljs-keyword">if</span> worker_id &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-variable language_">self</span>.worker_id = worker_id<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.worker_id = random.randint(<span class="hljs-number">100000</span>,<span class="hljs-number">10000000</span>) + random.randint(<span class="hljs-number">12</span>,<span class="hljs-number">100</span>)<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_merge</span>(<span class="hljs-params">self, kv</span>):<br>        <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> kv.items():<br>            <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.result_dict.keys():<br>                new_entry = &#123;&#125;<br>                new_entry[<span class="hljs-string">&quot;total&quot;</span>] = <span class="hljs-built_in">float</span>(v[<span class="hljs-string">&quot;total&quot;</span>])<br>                new_entry[<span class="hljs-string">&quot;count&quot;</span>] = <span class="hljs-built_in">int</span>(v[<span class="hljs-string">&quot;count&quot;</span>])<br>                <span class="hljs-variable language_">self</span>.result_dict[k] = new_entry<br>            <span class="hljs-keyword">else</span>:<br>                entry1 = <span class="hljs-variable language_">self</span>.result_dict[k]<br>                entry1[<span class="hljs-string">&quot;total&quot;</span>] += <span class="hljs-built_in">float</span>(v[<span class="hljs-string">&quot;total&quot;</span>])<br>                entry1[<span class="hljs-string">&quot;count&quot;</span>] += <span class="hljs-built_in">int</span>(v[<span class="hljs-string">&quot;count&quot;</span>])<br><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_average</span>(<span class="hljs-params">self</span>):<br>        dd = &#123;&#125;<br>        <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.result_dict.items():<br>            avg = v[<span class="hljs-string">&quot;total&quot;</span>] / v[<span class="hljs-string">&quot;count&quot;</span>]<br>            dd[k] = avg<br>        <br>        <span class="hljs-variable language_">self</span>.result_dict = dd<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Main Function of Reducer</span><br><span class="hljs-string">        Reduce key-value results to final averge score for each student</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        kv_lists = []<br>        <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.filepath_lists:<br>            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(each, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>                kv = pickle.load(f)<br>                kv_lists.append(kv)<br>        <br>        <span class="hljs-keyword">for</span> kv <span class="hljs-keyword">in</span> kv_lists:<br>            <span class="hljs-variable language_">self</span>._merge(kv)<br>        <br>        <span class="hljs-variable language_">self</span>._average()<br>        <br>        save_path = os.path.join(<span class="hljs-variable language_">self</span>.save_path, <span class="hljs-string">&quot;results.txt&quot;</span>)<br>        f = <span class="hljs-built_in">open</span>(save_path, <span class="hljs-string">&quot;w&quot;</span>)<br>        <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.result_dict.items():<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;k&#125;</span>: <span class="hljs-subst">&#123;v&#125;</span>分&quot;</span>, file=f)        <br>        <br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    reducer = Reducer(filepath_lists=[<span class="hljs-string">&#x27;2173983_intermediate_results.pkl&#x27;</span>], save_path=<span class="hljs-string">&quot;./&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Reducer <span class="hljs-subst">&#123;reducer.worker_id&#125;</span> started...&quot;</span>)<br>    reducer.run()      <br></code></pre></td></tr></table></figure><h3 id="manager-脚本">Manager 脚本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> multiprocessing <span class="hljs-keyword">as</span> mp<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> Manager<br><span class="hljs-keyword">from</span> MapClass <span class="hljs-keyword">import</span> Mapper<br><span class="hljs-keyword">from</span> ReduceClass <span class="hljs-keyword">import</span> Reducer<br><br>num_mapper = <span class="hljs-number">3</span><br>num_reducer = <span class="hljs-number">1</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">mapper_task</span>(<span class="hljs-params">filenum:<span class="hljs-built_in">int</span>, d:<span class="hljs-built_in">list</span>, barrier</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Mapper Starter FUnction</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        filenum (int): e.g. 1 indicates &quot;file1.txt&quot;</span><br><span class="hljs-string">        d (list): parameters for saving intermediate file paths.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    mapper = Mapper(filepath=<span class="hljs-string">&#x27;file&#123;&#125;.txt&#x27;</span>.<span class="hljs-built_in">format</span>(filenum), mid_save_path=<span class="hljs-string">&quot;./&quot;</span>)<br>    <span class="hljs-comment"># mapper = Mapper(filepath=&#x27;file&#123;&#125;.txt&#x27;.format(filenum), mid_save_path=&quot;./&quot;, worker_id=filenum)</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Mapper <span class="hljs-subst">&#123;mapper.worker_id&#125;</span> started...&quot;</span>)<br>    filename=mapper.run()<br>    d.append(filename)<br>    barrier.wait()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Worker <span class="hljs-subst">&#123;mapper.worker_id&#125;</span> job completed!&quot;</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">reducer_task</span>(<span class="hljs-params">d:<span class="hljs-built_in">list</span></span>):<br>    reducer = Reducer(filepath_lists=d, save_path=<span class="hljs-string">&quot;./&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Reducer <span class="hljs-subst">&#123;reducer.worker_id&#125;</span> started...&quot;</span>)<br>    reducer.run()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment"># use Manager context to create shared memory</span><br>    <span class="hljs-keyword">with</span> Manager() <span class="hljs-keyword">as</span> manager:<br>        intermediate_l = manager.<span class="hljs-built_in">list</span>()<br>        process_list = []<br>        <br>        barrier = mp.Barrier(parties=num_mapper, <span class="hljs-built_in">open</span>)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_mapper):<br>            process_list.append(mp.Process(target=mapper_task, args=(i+<span class="hljs-number">1</span>, intermediate_l, barrier)))<br>            process_list[i].start()<br>            process_list[i].join()<br>        <br>        <span class="hljs-comment"># 结束同步屏障</span><br>        <span class="hljs-comment"># barrier.wait()</span><br>        barrier.abort()<br>        <br>        process_list = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_reducer):<br>            process_list.append(mp.Process(target=reducer_task, args=(intermediate_l,)))<br>            process_list[i].start()<br>            process_list[i].join()<br></code></pre></td></tr></table></figure><h3 id="to-do">To-do</h3><ul><li>barrier同步屏障这一块设置有bug，会进入死锁。暂时未解决该问题。</li></ul>]]></content>
    
    
    <categories>
      
      <category>编程学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Map-Reduce</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>first_encrypted_blog</title>
    <link href="/posts/fd1c2b6f/"/>
    <url>/posts/fd1c2b6f/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><p>参考链接：https://easyhexo.com/3-Plugins-use-and-config/3-4-hexo-blog-encrypt/#%E7%AE%80%E4%BB%8B</p>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>charts_example</title>
    <link href="/posts/29446e2a/"/>
    <url>/posts/29446e2a/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><p>参考链接：https://shen-yu.gitee.io/2020/chartjs/</p><div style="width: 90%;margin: 0 auto">    <canvas id="chart1882" style="height: 300px"></canvas></div><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script><script type="text/javascript">    var ctx = document.getElementById('chart1882').getContext('2d');    var options =     {    type: 'line',    data: {    labels: ['January', 'February', 'March', 'April', 'May', 'June', 'July'],    datasets: [{        label: 'My First dataset',        backgroundColor: 'rgb(255, 99, 132)',        borderColor: 'rgb(255, 99, 132)',        data: [0, 10, 5, 2, 20, 30, 45]        }]    },    options: {        responsive: true,        title: {        display: true,        text: 'Chart.js Line Chart'        }    }};    new Chart(ctx, options);</script>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Spconv编程学习</title>
    <link href="/posts/c5ac3feb/"/>
    <url>/posts/c5ac3feb/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="spconv-编程学习">Spconv 编程学习</h1><p>spconv版本：2.x</p><p>参考链接:https://github.com/traveller59/spconv/blob/master/docs/USAGE.md官方repo给的教程</p><p>下文大部分直接粘贴，然后加上自己实验后修改补充的一些中文讲解内容。</p><h2 id="short-api-description">Short API description</h2><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> spconv.pytorch <span class="hljs-keyword">as</span> spconv<br><span class="hljs-title">from</span> spconv.pytorch <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> Fsp<br><span class="hljs-title">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-title">from</span> spconv.pytorch.utils <span class="hljs-keyword">import</span> PointToVoxel<br><span class="hljs-title">from</span> spconv.pytorch.hash <span class="hljs-keyword">import</span> HashTable<br></code></pre></td></tr></table></figure><table><thead><tr><th>Layer APIs</th><th>Common Usage</th><th>Dense Version</th><th>Note</th></tr></thead><tbody><tr><td><code>spconv.SparseConv3d</code></td><td>Downsample</td><td><code>nn.Conv3d</code></td><td>Use <code>indice_key</code> to save data for inverse</td></tr><tr><td><code>spconv.SubMConv3d</code></td><td>Convolution</td><td>N/A</td><td>Use <code>indice_key</code> to save data for reuse</td></tr><tr><td><code>spconv.SparseInverseConv3d</code></td><td>Upsample</td><td>N/A</td><td>Use pre-saved <code>indice_key</code> to upsample</td></tr><tr><td><code>spconv.SparseConvTranspose3d</code></td><td>Upsample (don’t use this)</td><td><code>nn.ConvTranspose3d</code></td><td>VERY SLOW and CAN’T RECOVER ORIGIN POINT CLOUD</td></tr><tr><td><code>spconv.SparseMaxPool3d</code></td><td>Downsample</td><td><code>nn.MaxPool3d</code></td><td>Use <code>indice_key</code> to save data for inverse</td></tr><tr><td><code>spconv.SparseSequential</code></td><td>Container</td><td><code>nn.Sequential</code></td><td>support layers above and<code>nn.ReLU, nn.BatchNorm, ...</code></td></tr></tbody></table><table><thead><tr><th>Functional APIs</th><th>Usage</th></tr></thead><tbody><tr><td><code>Fsp.sparse_add</code></td><td>Add sparse tensors with same shape and different indices</td></tr></tbody></table><table><thead><tr><th>Input APIs</th><th>Usage</th></tr></thead><tbody><tr><td><code>PointToVoxel</code></td><td>point cloud to voxels</td></tr></tbody></table><table><thead><tr><th>Misc APIs</th><th>Usage</th></tr></thead><tbody><tr><td><code>HashTable</code></td><td>hash table, one-slot</td></tr></tbody></table><table><thead><tr><th>Layer APIs</th><th><ahref="https://github.com/mit-han-lab/torchsparse">torchsparse</a></th><th><ahref="https://github.com/NVIDIA/MinkowskiEngine">MinkowskiEngine</a></th></tr></thead><tbody><tr><td><code>spconv.SparseConv3d</code></td><td><code>Conv3d(stride!=1, transpose=False)</code></td><td><code>MinkowskiConvolution(stride!=1)</code></td></tr><tr><td><code>spconv.SubMConv3d</code></td><td><code>Conv3d(stride=1, transpose=False)</code></td><td><code>MinkowskiConvolution(stride=1)</code></td></tr><tr><td><code>spconv.SparseInverseConv3d</code></td><td><code>Conv3d(stride!=1, transpose=True)</code></td><td><code>MinkowskiConvolutionTranspose</code></td></tr><tr><td><code>spconv.SparseConvTranspose3d</code></td><td>N/A</td><td><code>MinkowskiConvolutionTranspose</code></td></tr><tr><td><code>spconv.SparseMaxPool3d</code></td><td>N/A</td><td><code>MinkowskiMaxPooling</code></td></tr></tbody></table><h2 id="concept">Concept</h2><ul><li>Sparse Conv Tensor: like hybird <ahref="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs">torch.sparse_coo_tensor</a>but only have two difference: 1. SparseConvTensor only have one densedim, 2. indice of SparseConvTensor is transposed. see torch doc for moredetails.</li><li>Sparse Convolution: equivalent to perform dense convolution when youconvert SparseConvTensor to dense. Sparse Convolution only runcalculation on valid data.</li><li>Submanifold Convolution (SubMConv): like Sparse Convolution butindices keeps same. imagine that you copy same spatial structure tooutput, then iterate them, get input coordinates by conv rule, finallyapply convolution <strong>ONLY</strong> in these outputcoordinates.</li></ul><h2 id="sparseconvtensor">SparseConvTensor</h2><ul><li>features: <code>[N, num_channels]</code> tensor.</li><li>indices: <code>[N, (batch_idx + x + y + z)]</code> coordinate tensorwith batch axis. note that the coordinates xyz order MUST match spatialshape and conv params such as kernel size</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> spconv.pytorch <span class="hljs-keyword">as</span> spconv<br>features = <span class="hljs-comment"># your features with shape [N, num_channels]</span><br>indices = <span class="hljs-comment"># your indices/coordinates with shape [N, ndim + 1], batch index must be put in indices[:, 0]</span><br>spatial_shape = <span class="hljs-comment"># spatial shape of your sparse tensor, spatial_shape[i] is shape of indices[:, 1 + i].</span><br>batch_size = <span class="hljs-comment"># batch size of your sparse tensor.</span><br>x = spconv.SparseConvTensor(features, indices, spatial_shape, batch_size)<br>x_dense_NCHW = x.dense() <span class="hljs-comment"># convert sparse tensor to dense NCHW tensor.</span><br></code></pre></td></tr></table></figure><h3 id="sparse-convolution">Sparse Convolution</h3><p>注：此处代码有改动</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>os.environ[<span class="hljs-string">&#x27;SPCONV_DEBUG_SAVE_PATH&#x27;</span>]=<span class="hljs-string">&quot;./spconv.log&quot;</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> spconv.pytorch <span class="hljs-keyword">as</span> spconv<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ExampleNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, shape</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.net = spconv.SparseSequential(<br>        <span class="hljs-comment"># API Paramemter: in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, </span><br>        <span class="hljs-comment"># groups=1, bias=True, padding_mode=&#x27;zeros&#x27;, device=None, dtype=None</span><br>            spconv.SparseConv3d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, padding=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)), <span class="hljs-comment"># just like nn.Conv3d but don&#x27;t support group</span><br>            nn.BatchNorm1d(<span class="hljs-number">64</span>), <span class="hljs-comment"># non-spatial layers can be used directly in SparseSequential.</span><br>            nn.ReLU(),<br>            <span class="hljs-comment"># Kerner是3的时候，注意设置Padding为1，才能保持尺寸一致</span><br>            spconv.SubMConv3d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, padding=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),indice_key=<span class="hljs-string">&quot;subm0&quot;</span>),<br>            nn.BatchNorm1d(<span class="hljs-number">64</span>),<br>            nn.ReLU(),<br>        <span class="hljs-comment"># when use submanifold convolutions, their indices can be shared to save indices generation time.</span><br>            spconv.SubMConv3d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, indice_key=<span class="hljs-string">&quot;subm0&quot;</span>),<br>            nn.BatchNorm1d(<span class="hljs-number">64</span>),<br>            nn.ReLU(),<br>        )<br>        <span class="hljs-variable language_">self</span>.conv1 = spconv.SparseConv3d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>)<br>        <span class="hljs-variable language_">self</span>.bn1 = nn.BatchNorm1d(<span class="hljs-number">64</span>)<br>        <span class="hljs-variable language_">self</span>.relu1 = nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.shape = shape<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, features, coors, batch_size</span>):<br>        coors = coors.<span class="hljs-built_in">int</span>() <span class="hljs-comment"># unlike torch, this library only accept int coordinates.</span><br>        x = spconv.SparseConvTensor(features, coors, <span class="hljs-variable language_">self</span>.shape, batch_size)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.net(x)<br>    <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    points_feature_np = np.array(np.random.random(size=<span class="hljs-number">1000</span>))<br>    points_feature_np = points_feature_np[:, np.newaxis]<br>    <span class="hljs-comment"># 目前Spconv版本要求 变量类型是float32</span><br>    points_feature = torch.from_numpy(points_feature_np).to(torch.float32)<br>    points_feature = points_feature.expand_as(torch.zeros((<span class="hljs-number">1000</span>, <span class="hljs-number">32</span>)))<br>    points_indices_np = np.array([np.zeros(<span class="hljs-number">1000</span>), np.random.randint(<span class="hljs-number">360</span>, size=<span class="hljs-number">1000</span>), np.random.randint(<span class="hljs-number">480</span>, size=<span class="hljs-number">1000</span>), np.random.randint(<span class="hljs-number">32</span>, size=<span class="hljs-number">1000</span>)]).transpose()<br>    points_indices = torch.from_numpy(points_indices_np).to(torch.int32)<br>    <br>    model = ExampleNet(shape=[<span class="hljs-number">360</span>, <span class="hljs-number">480</span>, <span class="hljs-number">32</span>]).cuda()<br>    <span class="hljs-comment"># 注意此处向量需要连续化.contiguous()</span><br>    output_points_feature = model(points_feature.cuda().contiguous(), points_indices.cuda().contiguous(), <span class="hljs-number">1</span>)<br>    <span class="hljs-built_in">print</span>(output_points_feature.features.shape)<br>        <br>        <br></code></pre></td></tr></table></figure><h3 id="inverse-convolution">Inverse Convolution</h3><p>Inverse sparse convolution means “inv” of sparse convolution. theoutput of inverse convolution contains same indices as input of sparseconvolution.</p><p><strong>WARNING</strong> <code>SparseInverseConv</code> isn’tequivalent to <code>SparseConvTranspose</code>. SparseConvTranspose isequivalent to <code>ConvTranspose</code> in pytorch, butSparseInverseConv isn’t.</p><p>Inverse convolution usually used in semantic segmentation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ExampleNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, shape</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.net = spconv.SparseSequential(<br>            spconv.SparseConv3d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, indice_key=<span class="hljs-string">&quot;cp0&quot;</span>),<br>            spconv.SparseInverseConv3d(<span class="hljs-number">64</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, indice_key=<span class="hljs-string">&quot;cp0&quot;</span>), <span class="hljs-comment"># need provide kernel size to create weight</span><br>        )<br>        <span class="hljs-variable language_">self</span>.shape = shape<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, features, coors, batch_size</span>):<br>        coors = coors.<span class="hljs-built_in">int</span>()<br>        x = spconv.SparseConvTensor(features, coors, <span class="hljs-variable language_">self</span>.shape, batch_size)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.net(x)<br></code></pre></td></tr></table></figure><h4 id="common-mistake">Common Mistake</h4><ul><li>issue <ahref="https://github.com/traveller59/spconv/issues/467">#467</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">WrongNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, shape</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.Encoder = spconv.SparseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, indice_key=<span class="hljs-string">&quot;cp1&quot;</span>,algo=algo)<br>        <span class="hljs-variable language_">self</span>.Sparse_Conv = spconv.SparseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>,algo=algo)<br>        <span class="hljs-variable language_">self</span>.Decoder = spconv.SparseInverseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, indice_key=<span class="hljs-string">&quot;cp1&quot;</span>,algo=algo)   <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sparse_tensor</span>):<br>        encoded = <span class="hljs-variable language_">self</span>.Encoder(sparse_tensor)<br>        s_conv = <span class="hljs-variable language_">self</span>.Sparse_Conv(encoded)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.Decoder(s_conv).features<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CorrectNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, shape</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.Encoder = spconv.SparseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, indice_key=<span class="hljs-string">&quot;cp1&quot;</span>,algo=algo)<br>        <span class="hljs-variable language_">self</span>.Sparse_Conv = spconv.SparseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, indice_key=<span class="hljs-string">&quot;cp2&quot;</span>,algo=algo)<br>        <span class="hljs-variable language_">self</span>.Sparse_Conv_Decoder = spconv.SparseInverseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, indice_key=<span class="hljs-string">&quot;cp2&quot;</span>,algo=algo)   <br>        <span class="hljs-variable language_">self</span>.Decoder = spconv.SparseInverseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, indice_key=<span class="hljs-string">&quot;cp1&quot;</span>,algo=algo)   <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sparse_tensor</span>):<br>        encoded = <span class="hljs-variable language_">self</span>.Encoder(sparse_tensor)<br>        s_conv = <span class="hljs-variable language_">self</span>.Sparse_Conv(encoded)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.Decoder(<span class="hljs-variable language_">self</span>.Sparse_Conv_Decoder(s_conv)).features<br></code></pre></td></tr></table></figure><p>The <code>Sparse_Conv</code> in <code>ExampleNet</code> Changespatial structure of output of <code>Encoder</code>, so we can’t inverseback to input of <code>Encoder</code> via <code>Decoder</code>, we needto inverse from <code>Sparse_Conv.output</code> to<code>Encoder.output</code> via <code>Sparse_Conv_Decoder</code>, theninverse from <code>Encoder.output</code> to <code>Encoder.input</code>via <code>Decoder</code>.</p><h3 id="sparse-add">Sparse Add</h3><p>In sematic segmentation network, we may use conv1x3, 3x1 and 3x3 in ablock, but it’s impossible to sum result from these layers becauseregular add requires same indices.</p><p>spconv &gt;= 2.1.17 provide a operation to add sparse tensors withdifferent indices (shape must same), but with limits:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> spconv.pytorch <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> Fsp<br>res_1x3 = conv1x3(x)<br>res_3x1 = conv3x1(x)<br><span class="hljs-comment"># WRONG</span><br><span class="hljs-comment"># because we can&#x27;t &quot;inverse&quot; this operation</span><br>wrong_usage_cant_inverse = Fsp.sparse_add(res_1x3, res_3x1)<br><br><span class="hljs-comment"># CORRECT</span><br><span class="hljs-comment"># res_3x3 already contains all indices of res_1x3 and res_3x1, </span><br><span class="hljs-comment"># so output spatial structure isn&#x27;t changed, we can &quot;inverse&quot; back.</span><br>res_3x3 = conv3x3(x)<br>correct = Fsp.sparse_add(res_1x3, res_3x1, res_3x3)<br></code></pre></td></tr></table></figure><p>If you use a network without <code>SparseInverseConv</code>, limitsabove aren’t exists, the only drawback of <code>sparse_add</code> isthat it run slower than simple aligned add.</p><h3 id="fast-mixed-percision-training">Fast Mixed PercisionTraining</h3><p>see example/mnist_sparse. we support <code>torch.cuda.amp</code>.</p><h3 id="utility-functions">Utility functions</h3><ul><li>convert point cloud to voxel</li></ul><p>voxel generator in spconv generate indices in <strong>ZYX</strong>order, the params format are <strong>XYZ</strong>.</p><p>generated indices don’t include batch axis, you need to add it byyourself.</p><p>see examples/voxel_gen.py for examples.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> spconv.pytorch.utils <span class="hljs-keyword">import</span> PointToVoxel, gather_features_by_pc_voxel_id<br><span class="hljs-comment"># this generator generate ZYX indices.</span><br>gen = PointToVoxel(<br>    vsize_xyz=[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>], <br>    coors_range_xyz=[-<span class="hljs-number">80</span>, -<span class="hljs-number">80</span>, -<span class="hljs-number">2</span>, <span class="hljs-number">80</span>, <span class="hljs-number">80</span>, <span class="hljs-number">6</span>], <br>    num_point_features=<span class="hljs-number">3</span>, <br>    max_num_voxels=<span class="hljs-number">5000</span>, <br>    max_num_points_per_voxel=<span class="hljs-number">5</span>)<br>pc = np.random.uniform(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, size=[<span class="hljs-number">1000</span>, <span class="hljs-number">3</span>])<br>pc_th = torch.from_numpy(pc)<br>voxels, coords, num_points_per_voxel = gen(pc_th, empty_mean=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>If you want to get label for every point of your pc, you need to useanother function to get pc_voxel_id and gather features from sematicsegmentation result:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">voxels, coords, num_points_per_voxel, pc_voxel_id = gen.generate_voxel_with_id(pc_th, empty_mean=<span class="hljs-literal">True</span>)<br>seg_features = YourSegNet(...)<br><span class="hljs-comment"># if voxel id is invalid (point out of range, or no space left in a voxel)</span><br><span class="hljs-comment"># features will be zero.</span><br>point_features = gather_features_by_pc_voxel_id(seg_features, pc_voxel_id)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>编程学习</category>
      
      <category>点云</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python,PyTorch,Spconv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文十问：GASN(ECCV2022)</title>
    <link href="/posts/a69f9b72/"/>
    <url>/posts/a69f9b72/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="论文十问gasneccv-2022">论文十问：GASN(ECCV 2022)</h1><blockquote><p>论文标题: Efficient Point Cloud Segmentation with Geometry-awareSparse Networks 论文地址: <ahref="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990193.pdf">PDF</a>作者单位: HKUST, DeepRoute 代码地址:https://github.com/ItIsFriday/PcdSeg</p></blockquote><h2 id="q1-论文试图解决什么问题">Q1 论文试图解决什么问题？</h2><p>本文提出一个新的3D稀疏网络框架，可用于室外场景大规模自动驾驶数据集，在满足SOTA精度的前提下，有非常快的速度，更少的内存消耗，满足更好的实时性。</p><h2 id="q2-这是否是一个新的问题">Q2 这是否是一个新的问题？</h2><p>不是，很多工作都在尝试做这个</p><h2 id="q3-这篇文章要验证一个什么科学假设">Q3这篇文章要验证一个什么科学假设？</h2><ol type="1"><li>本文认为保证精度的关键在于对点云multi-scale特征的充分利用</li><li>本文认为保证速度并且降低显存的关键，是放弃基于点级别的操作(pooling),网络设计全部基于sparse voxel-basedrepresentation。比如稀疏卷积，池化，多特征融合，MLP均在该稀疏特征层次上完成。</li></ol><h2id="q4-有哪些相关研究如何归类谁是这一课题在领域内值得关注的研究员">Q4有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h2><ol type="1"><li>基于Raw Point的网络: PointNet, PointNet++ (maintain and utilize thepointwise geometry).</li><li>通过采样降低点的数量，提高效率: RandLA，KPConv.（但是采样会导致极大的信息损失，这些网络在室外场景精度表现不高）</li><li>划分/投影到预先设定的Grids里(如2D, 3D, Sparse3D)，再进行卷积/稀疏卷积操作：AF2S3Net, Cylinder3D, SECOND,PointPillar…4.Point, Voxel, Range等多种representation融合感知的网络:PVCNN, SPVCNN, DRINet, RPVNet</li><li>基于图的(主要室内用得多，大规模室外计算速度/显存开销偏大)</li><li>基于transformer的</li></ol><h2 id="q5-论文中提到的解决方案之关键是什么">Q5论文中提到的解决方案之关键是什么？</h2><ol type="1"><li>整体网络由Sparse Feature Encoder(SFE)和Sparse Geometry FeatureEnhancement(SGFE)两个模块组成。前者用稀疏卷积提取特征后送入SGFE；SGFE设计了多尺度稀疏特征投影模块(Multi-scale SparseProjection)来增强对几何信息的提取，将多尺度融合后的特征送到下一层的SFE。</li><li>相比于PVCNN，SPVCNN,DRINet等point+voxel的表征方式，作者只保留了sparse-voxelrepresentation送入SFE和SGFE，不需要point-wiserepresentation，大大减小了计算开销与显存开销。</li><li>point-wise representation能完整地提取点云的局部几何信息。为了弥补放弃使用pointrepresentation带来的几何信息感知缺失，作者设计了多尺度稀疏特征投影模块(Multi-scale SparseProjection)来增强对几何信息的提取。认为不同的尺度下提取的特征是对点云结构的先验知识学习，在不同层次上获取并编码点云的几何特征。</li></ol><h2 id="q6-论文中的实验是如何设计的">Q6 论文中的实验是如何设计的？</h2><p>主要做的语义分割实验，比较mIoU, 参数量，显存占用和速度</p><h2 id="q7-用于定量评估的数据集是什么代码有没有开源">Q7用于定量评估的数据集是什么？代码有没有开源？</h2><p>nuScenes, semanticKitti.</p><h2 id="q8-论文中的实验及结果有没有很好地支持需要验证的科学假设">Q8论文中的实验及结果有没有很好地支持需要验证的科学假设？</h2><p>SemanticKitti和nuScenes，在mIoU和Cylinder3D, RPVNet,AF2S3net等SOTA网络基本持平的基础上，速度快2-5倍，显存占用量少2-3倍，参数量少2-10倍，仅一张2080Ti能跑的很好，性能优异</p><h2 id="q9-这篇论文到底有什么贡献">Q9 这篇论文到底有什么贡献？</h2><p>提出了一个性能好，速度快，显存占用少，适用于大规模室外点云数据集的网络</p><h2 id="q10-下一步呢有什么工作可以继续深入">Q10下一步呢？有什么工作可以继续深入？</h2><p>别卷了，卷不动了，有点强</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>通用点云理解与网络设计</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文十问系列</tag>
      
      <tag>ECCV2022</tag>
      
      <tag>Efficiency</tag>
      
      <tag>Sparse Voxel-Based Rrepresentation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LaTeX讲解系列(2)：常用数学符号大全</title>
    <link href="/posts/98374ae/"/>
    <url>/posts/98374ae/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="latex讲解系列2常用数学符号大全">LaTeX讲解系列(2)：常用数学符号大全</h1><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="//music.163.com/outchain/player?type=2&amp;id=815722&amp;auto=0&amp;height=66"></iframe><div class="note note-primary">            <p>本文整理过程中着重参考了：</p><p>https://www.cnblogs.com/yalphait/articles/8685586.html（LaTex命令符号大全）</p><p>https://blog.csdn.net/u012684062/article/details/78398191 （LaTex所有常用数学符号整理）</p><p>这两份非常全面，非常感谢！尤其第一份大佬的，还介绍了不同符号在数学，物理计算机等不同学科中用法(比如方程组中用希腊小写字母，向量用粗体小写，矩阵用粗体大写等)，还有很多美观排版的建议与事例，强烈安利阅读一番！</p><p>除此之外整合了其他网站提到的常用的公式命令，一并列在最后<ahref="#参考资料">参考资料</a>中。</p>          </div><h2 id="零-写在最开始的话">零、 写在最开始的话</h2><h3 id="preamble常用引入的内容">0.1 Preamble常用引入的内容</h3><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\usepackage</span>&#123;graphicx&#125;<br><span class="hljs-keyword">\usepackage</span>&#123;amsfonts&#125;       <span class="hljs-comment">% blackboard math symbols</span><br><span class="hljs-keyword">\usepackage</span>&#123;amsmath&#125;<br><span class="hljs-keyword">\usepackage</span>&#123;amssymb&#125;<br><span class="hljs-keyword">\usepackage</span>&#123;cases&#125;<br><br><span class="hljs-comment">% 直立体数学符号</span><br><span class="hljs-keyword">\providecommand</span>&#123;<span class="hljs-keyword">\dd</span>&#125;&#123;<span class="hljs-keyword">\mathop</span>&#123;&#125;<span class="hljs-keyword">\!</span><span class="hljs-keyword">\mathrm</span>&#123;d&#125;&#125;<span class="hljs-comment">% 导数，积分等数学符号需要用直立体</span><br><span class="hljs-keyword">\providecommand</span>&#123;<span class="hljs-keyword">\ee</span>&#125;&#123;<span class="hljs-keyword">\mathrm</span>&#123;e&#125;&#125;<br><span class="hljs-keyword">\providecommand</span>&#123;<span class="hljs-keyword">\ii</span>&#125;&#123;<span class="hljs-keyword">\mathrm</span>&#123;i&#125;&#125;<br><span class="hljs-keyword">\providecommand</span>&#123;<span class="hljs-keyword">\jj</span>&#125;&#123;<span class="hljs-keyword">\mathrm</span>&#123;j&#125;&#125;<br><br><span class="hljs-comment">% 定义数学变量的字体样式</span><br><span class="hljs-keyword">\usepackage</span>&#123;isomath&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\mrm</span>&#125;[1]&#123;<span class="hljs-keyword">\mathrm</span>&#123;<span class="hljs-params">#1</span>&#125;&#125;          <span class="hljs-comment">% 直立体：函数、算子，下标等</span><br><span class="hljs-keyword">\renewcommand</span>&#123;<span class="hljs-keyword">\vec</span>&#125;[1]&#123;<span class="hljs-keyword">\mathbold</span>&#123;<span class="hljs-params">#1</span>&#125;&#125;      <span class="hljs-comment">% 向量：小写粗斜体</span><br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\mat</span>&#125;[1]&#123;<span class="hljs-keyword">\mathbold</span>&#123;<span class="hljs-params">#1</span>&#125;&#125;        <span class="hljs-comment">% 矩阵：大写粗斜体</span><br><span class="hljs-comment">% \newcommand&#123;\tens&#125;[1]&#123;\mathsfbfit&#123;#1&#125;&#125;   % 张量：无衬线粗斜体（需isomath）</span><br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\tens</span>&#125;[1]&#123;<span class="hljs-keyword">\mathbf</span>&#123;<span class="hljs-params">#1</span>&#125;&#125;         <span class="hljs-comment">% 张量：大写粗正体（斜体表示矩阵，正体标张量）</span><br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\bb</span>&#125;[1]&#123;<span class="hljs-keyword">\mathbb</span>&#123;<span class="hljs-params">#1</span>&#125;&#125;           <span class="hljs-comment">% 集合符号：虚空花体\mathbb</span><br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\mcal</span>&#125;[1]&#123;<span class="hljs-keyword">\mathcal</span>&#123;<span class="hljs-params">#1</span>&#125;&#125;         <span class="hljs-comment">% chi符号：花体\mathcal</span><br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\trans</span>&#125;&#123;<span class="hljs-keyword">\mathsf</span>&#123;T&#125;&#125;            <span class="hljs-comment">% 转置符号：无衬线正体</span><br><br><span class="hljs-comment">% 常用变量简写</span><br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\vx</span>&#125;&#123;<span class="hljs-keyword">\vec</span>&#123;x&#125;&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\vX</span>&#125;&#123;<span class="hljs-keyword">\mat</span>&#123;X&#125;&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\tX</span>&#125;&#123;<span class="hljs-keyword">\tens</span>&#123;X&#125;&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\vtheta</span>&#125;&#123;<span class="hljs-keyword">\vec</span>&#123;<span class="hljs-keyword">\theta</span>&#125;&#125;<br><br><span class="hljs-comment">% ============================================</span><br><span class="hljs-comment">% 花括号 &#123;&#125; </span><br><span class="hljs-comment">% ============================================</span><br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\set</span>&#125;[1]&#123;<span class="hljs-keyword">\&#123;</span><span class="hljs-params">#1</span><span class="hljs-keyword">\&#125;</span>&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\setb</span>&#125;[1]&#123;<span class="hljs-keyword">\left</span><span class="hljs-keyword">\&#123;</span><span class="hljs-params">#1</span><span class="hljs-keyword">\right</span><span class="hljs-keyword">\&#125;</span>&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\setbig</span>&#125;[1]&#123;<span class="hljs-keyword">\bigl</span><span class="hljs-keyword">\&#123;</span><span class="hljs-params">#1</span><span class="hljs-keyword">\bigr</span><span class="hljs-keyword">\&#125;</span>&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\setBig</span>&#125;[1]&#123;<span class="hljs-keyword">\Bigl</span><span class="hljs-keyword">\&#123;</span><span class="hljs-params">#1</span><span class="hljs-keyword">\Bigr</span><span class="hljs-keyword">\&#125;</span>&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\setbigg</span>&#125;[1]&#123;<span class="hljs-keyword">\biggl</span><span class="hljs-keyword">\&#123;</span><span class="hljs-params">#1</span><span class="hljs-keyword">\biggr</span><span class="hljs-keyword">\&#125;</span>&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\setBigg</span>&#125;[1]&#123;<span class="hljs-keyword">\Biggl</span><span class="hljs-keyword">\&#123;</span><span class="hljs-params">#1</span><span class="hljs-keyword">\Biggr</span><span class="hljs-keyword">\&#125;</span>&#125;<br><br><span class="hljs-comment">% ============================================</span><br><span class="hljs-comment">% 圆括号 () - 建议用 \paren 前缀</span><br><span class="hljs-comment">% ============================================</span><br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\paren</span>&#125;[1]&#123;(<span class="hljs-params">#1</span>)&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\parenb</span>&#125;[1]&#123;<span class="hljs-keyword">\left</span>(<span class="hljs-params">#1</span><span class="hljs-keyword">\right</span>)&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\parenbig</span>&#125;[1]&#123;<span class="hljs-keyword">\bigl</span>(<span class="hljs-params">#1</span><span class="hljs-keyword">\bigr</span>)&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\parenBig</span>&#125;[1]&#123;<span class="hljs-keyword">\Bigl</span>(<span class="hljs-params">#1</span><span class="hljs-keyword">\Bigr</span>)&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\parenbigg</span>&#125;[1]&#123;<span class="hljs-keyword">\biggl</span>(<span class="hljs-params">#1</span><span class="hljs-keyword">\biggr</span>)&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\parenBigg</span>&#125;[1]&#123;<span class="hljs-keyword">\Biggl</span>(<span class="hljs-params">#1</span><span class="hljs-keyword">\Biggr</span>)&#125;<br><br><span class="hljs-comment">% ============================================</span><br><span class="hljs-comment">% 方括号 [] - 建议用 \brack 或 \bracket 前缀</span><br><span class="hljs-comment">% ============================================</span><br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\brc</span>&#125;[1]&#123;[<span class="hljs-params">#1</span>]&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\brcb</span>&#125;[1]&#123;<span class="hljs-keyword">\left</span>[<span class="hljs-params">#1</span><span class="hljs-keyword">\right</span>]&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\brcbig</span>&#125;[1]&#123;<span class="hljs-keyword">\bigl</span>[<span class="hljs-params">#1</span><span class="hljs-keyword">\bigr</span>]&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\brcBig</span>&#125;[1]&#123;<span class="hljs-keyword">\Bigl</span>[<span class="hljs-params">#1</span><span class="hljs-keyword">\Bigr</span>]&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\brcbigg</span>&#125;[1]&#123;<span class="hljs-keyword">\biggl</span>[<span class="hljs-params">#1</span><span class="hljs-keyword">\biggr</span>]&#125;<br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\brcBigg</span>&#125;[1]&#123;<span class="hljs-keyword">\Biggl</span>[<span class="hljs-params">#1</span><span class="hljs-keyword">\Biggr</span>]&#125;<br><br><br><span class="hljs-comment">% \newcommand&#123;\brc&#125;[1]&#123;\left(#1 \right)&#125;</span><br><br><br><span class="hljs-comment">% 国际单位制宏包</span><br><span class="hljs-keyword">\usepackage</span>&#123;siunitx&#125;<br><br><span class="hljs-comment">% 定理环境宏包</span><br><span class="hljs-keyword">\usepackage</span>&#123;amsthm&#125;<br><span class="hljs-comment">% \usepackage&#123;ntheorem&#125;</span><br><br><span class="hljs-comment">% 绘图宏包</span><br><span class="hljs-keyword">\usepackage</span>&#123;tikz&#125;<br><span class="hljs-keyword">\usetikzlibrary</span>&#123;arrows.meta, shapes.geometric&#125;<br></code></pre></td></tr></table></figure><h3 id="易出错的地方">0.2 易出错的地方</h3><ol type="1"><li>在很多公式环境里，Overleaf<spanstyle="color:red"><u><strong>不允许有空行</strong></u></span>出现,否则会报错并且显示不正常。</li><li>括号，成对出现的算符等一定要成对出现！不然在一堆括号里面检查是真的痛苦的PTSD。一开始养成好习惯。。。</li><li>如果公式太长需要换行，可以单独用个<code>\begin&#123;multline&#125; \end&#123;multline&#125;</code>环境，里面第一行不记入公式编号，直接<code>\\</code>换行即可。</li><li>注意好符号系统习惯：在LaTeX中，表示标量（scalar）、向量（vector）和矩阵（matrix）通常有特定的格式和约定。以下是一些常见的表示方法：<ul><li><strong>标量（Scalar）</strong>:标量通常用小写字母表示，可以是正体或斜体（italic）来强调它们是变量。在数学模式中，斜体可以通过使用<code>\mathit&#123;&#125;</code>命令来实现，正体则直接使用字母即可。</li><li><strong>向量（Vector）</strong>:向量通常用小写字母表示，并且是斜体或粗体。在LaTeX中，斜体向量可以通过<code>\mathbf&#123;&#125;</code>命令实现。</li><li><strong>矩阵（Matrix）</strong>:矩阵通常用大写字母表示，并且是粗体。在LaTeX中，矩阵可以通过<code>\mathbf&#123;&#125;</code>命令实现，或者使用<code>\begin&#123;bmatrix&#125; ... \end&#123;bmatrix&#125;</code>环境来表示矩阵，其中元素默认是正体，但整个矩阵作为一个整体被理解为粗体对象。</li></ul></li><li>在<code>\begin&#123;align&#125; \end&#123;align&#125;</code>的多行公式数学环境下，<code>\label&#123;&#125;</code>可以紧贴公式，每行公式都有一个label来索引。</li><li>建议在每个公式环境前，加个注释<code>% --- formula XXX</code>,避免以后写几十页的论文或报告，写到一半发现自己要自己几十页之前的一个公式符号下标写错了。。。。做好注释系统方便快速跨页索引。甚至可以做个表格把重要的公式统计一下，写一个放进去一个。</li></ol><h2 id="一不同的数学模式">一、不同的数学模式</h2><h3 id="重音符号">1.1 重音符号</h3><table><thead><tr><th><span class="math display"><em>â</em></span><code>\hat&#123;a&#125;</code></th><th><span class="math display"><em>ǎ</em></span><code>\check&#123;a&#125;</code></th><th><span class="math display"><em>ã</em></span><code>\tilde&#123;a&#125;</code></th></tr></thead><tbody><tr><td><span class="math display"><em>à</em></span><code>\grave&#123;a&#125;</code></td><td><span class="math display"><em>ȧ</em></span><code>\dot&#123;a&#125;</code></td><td><span class="math display">$$\ddot{a}$$</span><code>\ddot&#123;a&#125;</code></td></tr><tr><td><span class="math display"><em>ā</em></span><code>\bar&#123;a&#125;</code></td><td><span class="math display"><em>a⃗</em></span><code>\vac&#123;a&#125;</code></td><td><span class="math display"><em>Â</em></span><code>\widehat&#123;A&#125;</code></td></tr><tr><td><span class="math display"><em>á</em></span><code>\acute&#123;a&#125;</code></td><td><span class="math display"><em>ă</em></span><code>\breve&#123;a&#125;</code></td><td><span class="math display"><em>Ã</em></span><code>\widetilde&#123;A&#125;</code></td></tr></tbody></table><h3 id="其他">1.2 其他</h3><table><thead><tr><th><span class="math display"><strong>E</strong></span><code>\mathbf&#123;E&#125;</code></th><th><span class="math display">ℝ</span> <code>\mathbb&#123;R&#125;</code></th><th><span class="math display"><em>A</em><em>B</em><em>C</em></span><code>\mathit&#123;ABC&#125;</code></th></tr></thead><tbody><tr><td><span class="math display">ABC</span> <code>\mathrm&#123;ABC&#125;</code></td><td><span class="math display">𝔄𝔅ℭ</span> <code>\mathfrak&#123;ABC&#125;</code>（花体）</td><td><span class="math display">ℒ</span> <code>\mathcal&#123;L&#125;</code>(手写体)</td></tr></tbody></table><h3 id="希腊字母">1.3 希腊字母</h3><table><thead><tr><th><span class="math display"><em>α</em></span><code>\alpha</code></th><th><span class="math display"><em>θ</em></span><code>\theta</code></th><th><span class="math display"><em>o</em></span> <code>o</code></th><th><span class="math display"><em>υ</em></span><code>\upsilon</code></th></tr></thead><tbody><tr><td><span class="math display"><em>β</em></span> <code>\beta</code></td><td><span class="math display"><em>ϑ</em></span><code>\vartheta</code></td><td><span class="math display"><em>π</em></span> <code>\pi</code></td><td><span class="math display"><em>ϕ</em></span> <code>\phi</code></td></tr><tr><td><span class="math display"><em>γ</em></span><code>\gamma</code></td><td><span class="math display"><em>ι</em></span> <code>\iota</code></td><td><span class="math display"><em>ϖ</em></span><code>\varpi</code></td><td><span class="math display"><em>φ</em></span><code>\varphi</code></td></tr><tr><td><span class="math display"><em>δ</em></span><code>\delta</code></td><td><span class="math display"><em>κ</em></span><code>\kappa</code></td><td><span class="math display"><em>ρ</em></span> <code>\rho</code></td><td><span class="math display"><em>χ</em></span> <code>\chi</code></td></tr><tr><td><span class="math display"><em>ϵ</em></span><code>\epsilon</code></td><td><span class="math display"><em>λ</em></span><code>\lambda</code></td><td><span class="math display"><em>𝜚</em></span><code>\varrho</code></td><td><span class="math display"><em>ψ</em></span> <code>\psi</code></td></tr><tr><td><span class="math display"><em>ε</em></span><code>\varepsilon</code></td><td><span class="math display"><em>μ</em></span> <code>\mu</code></td><td><span class="math display"><em>σ</em></span><code>\sigma</code></td><td><span class="math display"><em>ω</em></span><code>\omega</code></td></tr><tr><td><span class="math display"><em>ζ</em></span> <code>\zeta</code></td><td><span class="math display"><em>ν</em></span> <code>\nu</code></td><td><span class="math display"><em>𝜍</em></span><code>\varsigma</code></td><td><span class="math display"><em>ℓ</em></span> <code>\ell</code></td></tr><tr><td><span class="math display"><em>η</em></span> <code>\eta</code></td><td><span class="math display"><em>ξ</em></span> <code>\xi</code></td><td><span class="math display"><em>τ</em></span> <code>\tau</code></td><td></td></tr><tr><td><span class="math display"><em>Γ</em></span><code>\Gamma</code></td><td><span class="math display"><em>Λ</em></span><code>\Lambda</code></td><td><span class="math display"><em>Σ</em></span><code>\Sigma</code></td><td><span class="math display"><em>Ψ</em></span> <code>\Psi</code></td></tr><tr><td><span class="math display"><em>Θ</em></span><code>\Theta</code></td><td><span class="math display"><em>Π</em></span> <code>\Pi</code></td><td><span class="math display"><em>Φ</em></span> <code>\Phi</code></td><td></td></tr></tbody></table><h2 id="二运算符">二、运算符</h2><h3 id="关系符号">2.1 关系符号</h3><p>可以在下列符号的相应命令前加上<code>\not</code>命令，得到其否定形式，例如<span class="math display">∈</span> 与<spanclass="math display">∉</span> 。</p><table><thead><tr><th><span class="math display">≤</span> <code>\le</code></th><th><span class="math display">≥</span> <code>\ge</code></th><th><span class="math display">≡</span> <code>\equiv</code></th><th><span class="math display">≪</span> <code>\ll</code></th></tr></thead><tbody><tr><td><span class="math display">≫</span> <code>\gg</code></td><td><span class="math display">≐</span> <code>\doteq</code></td><td><span class="math display">≺</span> <code>\prec</code></td><td><span class="math display">≻</span> <code>\succ</code></td></tr><tr><td><span class="math display">≃</span> <code>\simeq</code></td><td><span class="math display">⊂</span> <code>\subset</code></td><td><span class="math display">⊃</span> <code>\supset</code></td><td><span class="math display">≈</span> <code>\approx</code></td></tr><tr><td><span class="math display">⊆</span> <code>\subseteq</code></td><td><span class="math display">⊇</span> <code>\supseteq</code></td><td><span class="math display">≅</span> <code>\cong</code></td><td><span class="math display">∈</span> <code>\in</code></td></tr><tr><td><span class="math display">⊢</span> <code>\vdash</code></td><td><span class="math display">⊣</span> <code>\dashv</code></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr></tbody></table><h3 id="运算符">2.2 运算符</h3><table><thead><tr><th><span class="math display">±</span> <code>\pm</code></th><th><span class="math display">∓</span> <code>\mp</code></th><th><span class="math display">⋅</span> <code>\cdot</code></th><th><span class="math display">÷</span> <code>\div</code></th></tr></thead><tbody><tr><td><span class="math display">×</span> <code>\times</code></td><td><span class="math display">\</span> <code>\setminus</code></td><td><span class="math display">⋆</span> <code>\star</code></td><td><span class="math display">∪</span> <code>\cup</code></td></tr><tr><td><span class="math display">∩</span> <code>\cap</code></td><td><span class="math display">*</span> <code>\ast</code></td><td><span class="math display">∘</span> <code>\circ</code></td><td><span class="math display">∨</span> <code>\lor</code></td></tr><tr><td><span class="math display">∧</span> <code>\land</code></td><td><span class="math display">⊕</span> <code>\oplus</code></td><td><span class="math display">⊖</span> <code>\ominus</code></td><td><span class="math display">⋄</span> <code>\diamond</code></td></tr><tr><td><span class="math display">⊗</span> <code>\otimes</code></td><td><span class="math display">⊙</span> <code>\odot</code></td><td><span class="math display">⊘</span> <code>\oslash</code></td><td><span class="math display">∐</span> <code>\amalg</code></td></tr><tr><td><span class="math display">△</span> <code>\bigtrianglep</code></td><td><span class="math display">▽</span><code>\bigtriangledown</code></td><td><span class="math display">†</span> <code>\dagger</code></td><td><span class="math display">‡</span> <code>\ddagger</code></td></tr><tr><td><span class="math display">≀</span> <code>\wr</code></td><td><span class="math display">¬</span> <code>\not</code></td><td></td><td></td></tr></tbody></table><h3 id="大运算符">2.3 ”大“运算符</h3><table><thead><tr><th><span class="math display">$$\sum_i^j$$</span><code>\sum_i^&#123;j&#125;</code></th><th><span class="math display">∏</span> <code>\prod</code></th><th><span class="math display">⋃</span> <code>\bigcup</code></th><th><span class="math display">⋂</span> <code>\bigcap</code></th></tr></thead><tbody><tr><td><span class="math display">⋁</span> <code>\bigvee</code></td><td><span class="math display">⋀</span> <code>\bigwedge</code></td><td><spanclass="math display">∫<sub><em>a</em></sub><sup><em>b</em></sup></span><code>\int_a^b</code></td><td><span class="math display">∮</span> <code>\oint</code></td></tr><tr><td><span class="math display">⨁</span> <code>\bigoplus</code></td><td><span class="math display">⨂</span> <code>\bigotimes</code></td><td><span class="math display">⨀</span> <code>\bigodot</code></td><td></td></tr></tbody></table><h3 id="箭头">2.4 箭头</h3><table><thead><tr><th><span class="math display">←</span> <code>\leftarrow</code></th><th><span class="math display">←</span> <code>\longleftarrow</code></th><th><span class="math display">→</span> <code>\rightarrow</code></th><th><span class="math display">→</span><code>\longrightarrow</code></th></tr></thead><tbody><tr><td><span class="math display">↔︎</span><code>\leftrightarrow</code></td><td><span class="math display">↔︎</span><code>\longleftrightarrrow</code></td><td><span class="math display">⇐</span> <code>\Leftarrow</code></td><td><span class="math display">⇒</span> <code>\Rightarrow</code></td></tr><tr><td><span class="math display">⇔</span><code>\Leftrightarrow</code></td><td><span class="math display">⇔</span><code>\iff(bigger spaces)</code></td><td><span class="math display">↦</span> <code>\mapsto</code></td><td><span class="math display">⇌</span><code>\rightleftharpoons</code></td></tr><tr><td><span class="math display">⤳</span> <code>\leadsto</code></td><td></td><td></td><td></td></tr></tbody></table><h3 id="常用函数符号">2.5 常用函数符号</h3><table><thead><tr><th><span class="math display">sin <em>θ</em></span><code>\sin \theta</code></th><th><span class="math display">cos <em>θ</em></span><code>\cos\theta</code></th><th><span class="math display">tan <em>θ</em></span><code>\tan\theta</code></th><th><span class="math display">arcsin </span> <code>\arcsin</code></th></tr></thead><tbody><tr><td><span class="math display">arccos </span> <code>\arccos</code></td><td><span class="math display">cosh </span> <code>\cosh</code></td><td><span class="math display">tanh </span> <code>\tanh</code></td><td><span class="math display">limsup </span> <code>\limsup</code></td></tr><tr><td><span class="math display">$$f'(x) = \lim_{\Delta x \rightarrow 0}\frac{f(x + \Delta x) - f(x)}{\Delta x}$$</span><code>\lim_&#123;\Delta x \rightarrow 0&#125; \frac&#123;f(x + \Delta x) - f(x)&#125;&#123;\Delta x&#125;</code></td><td><span class="math display">max </span> <code>\max</code></td><td><span class="math display">min </span> <code>\min</code></td><td><span class="math display">inf </span> <code>\inf</code></td></tr><tr><td><span class="math display">log </span> <code>\log</code></td><td><span class="math display">ln </span> <code>\ln</code></td><td><span class="math display">ker </span> <code>\ker</code></td><td><span class="math display">Pr </span> <code>\Pr</code></td></tr><tr><td><span class="math display">dim </span> <code>\dim</code></td><td><span class="math display">det </span> <code>\det</code></td><td><span class="math display">exp </span> <code>\exp</code></td><td></td></tr></tbody></table><h3 id="其他-1">2.6 其他</h3><table><thead><tr><th><span class="math display">|<em>a</em>|</span><code>\lvert a \rvert</code></th><th><span class="math display">‖<em>a</em>‖</span><code>\lVert a \rVert</code></th><th><span class="math display">…</span> <code>\dots</code></th><th><span class="math display">⋯</span> <code>\cdots</code>(formatrix)</th></tr></thead><tbody><tr><td><span class="math display">⋮</span> <code>\vdots</code></td><td><span class="math display">⋱</span> <code>\ddots</code></td><td><span class="math display">∀</span> <code>\forall</code></td><td><span class="math display">∃</span> <code>\exists</code></td></tr><tr><td><span class="math display">∂</span> <code>\partial</code></td><td><span class="math display">∇</span> <code>\nabla</code></td><td><span class="math display">⊥</span> <code>\bot</code></td><td><span class="math display">⊤</span> <code>\top</code></td></tr><tr><td><span class="math display">∠</span> <code>\angle</code></td><td><span class="math display">$$\surd$$</span> <code>\surd</code></td><td><span class="math display">∅</span> <code>\emptyset</code></td><td><span class="math display"><em>ℓ</em></span> <code>\ell</code></td></tr><tr><td><span class="math display">∞</span> <code>\infty</code></td><td><span class="math display">♡</span> <code>\heartsuit</code></td><td><span class="math display">♣</span> <code>\clubsuit</code></td><td><span class="math display">♠</span> <code>\spadesuit</code></td></tr><tr><td><span class="math display">∴</span> <code>\therefore</code></td><td><span class="math display">∵</span> <code>\because</code></td><td><span class="math display">argmin<sub><em>θ</em></sub></span><code>\mathop&#123;\arg\min&#125;_&#123;\theta&#125;</code>(套在LaTeXequation环境下，下标<spanclass="math display"><em>θ</em></span>能自动在字母下面)</td><td></td></tr></tbody></table><h2 id="三矩阵表示">三、矩阵表示</h2><ol type="1"><li></li></ol><p><span class="math display">$$\begin{aligned}    \left[ \begin{matrix}  a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\  a_{21} &amp; a_{22} &amp; \cdots &amp; a_{25} \\  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\  a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn}  \end{matrix}\right]\end{aligned}$$</span></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;align&#125; <span class="hljs-comment">% 推荐用align或equation的数学环境</span><br><span class="hljs-keyword">\left</span>[ <span class="hljs-keyword">\begin</span>&#123;matrix&#125; <span class="hljs-comment">% 推荐使用matrix的环境, 如果使用array环境需要指定&#123;ccc&#125;列数</span><br>  a<span class="hljs-built_in">_</span>&#123;11&#125; <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;12&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\cdots</span> <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;1n&#125; <span class="hljs-keyword">\\</span><br>  a<span class="hljs-built_in">_</span>&#123;21&#125; <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;22&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\cdots</span> <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;25&#125; <span class="hljs-keyword">\\</span><br>  <span class="hljs-keyword">\vdots</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\vdots</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\ddots</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\vdots</span> <span class="hljs-keyword">\\</span><br>  a<span class="hljs-built_in">_</span>&#123;n1&#125; <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;n2&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\cdots</span> <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;nn&#125;<br>  <span class="hljs-keyword">\end</span>&#123;matrix&#125;<span class="hljs-keyword">\right</span>] <span class="hljs-comment">% \left[ 与\right]配对，[还可以换成&#123;, (等 </span><br><span class="hljs-keyword">\end</span>&#123;align&#125;<br><br><span class="hljs-comment">% 如使用nicematrix宏包，需要在完整版的LaTex环境中引用该包</span><br><span class="hljs-comment">% 详见官方连接https://ctan.org/pkg/nicematrix</span><br></code></pre></td></tr></table></figure><ol start="2" type="1"><li>上下大括号</li></ol><p><span class="math display">$$\begin{align}&amp;\begin{matrix} 5050 \\ \overbrace{ 1+2+\cdots+100 }\end{matrix} \\\\&amp;\begin{matrix} \underbrace{ a+b+\cdots+z } \\ 26\end{matrix}\end{align}$$</span></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;align&#125;<br><span class="hljs-built_in">&amp;</span><span class="hljs-keyword">\begin</span>&#123;matrix&#125; 5050 <span class="hljs-keyword">\\</span> <span class="hljs-keyword">\overbrace</span>&#123; 1+2+<span class="hljs-keyword">\cdots</span>+100 &#125;<span class="hljs-keyword">\end</span>&#123;matrix&#125; <span class="hljs-keyword">\\</span><br><span class="hljs-keyword">\\</span><br><span class="hljs-built_in">&amp;</span><span class="hljs-keyword">\begin</span>&#123;matrix&#125; <span class="hljs-keyword">\underbrace</span>&#123; a+b+<span class="hljs-keyword">\cdots</span>+z &#125; <span class="hljs-keyword">\\</span> 26<span class="hljs-keyword">\end</span>&#123;matrix&#125;<br><br><span class="hljs-keyword">\end</span>&#123;align&#125;<br></code></pre></td></tr></table></figure><ol start="3" type="1"><li>方程组/分段函数</li></ol><p><span class="math display">$$=\begin{cases}3x + 5y +  z, &amp; x+y+z &lt;1\\7x - 2y + 4z, &amp; 1\le x+y+z &lt;5\\-6x + 3y + 2z, &amp; x+y+z &gt; 1\end{cases}$$</span></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs latex">=<br><span class="hljs-keyword">\begin</span>&#123;cases&#125;<br>3x + 5y +  z, <span class="hljs-built_in">&amp;</span> x+y+z &lt;1<span class="hljs-keyword">\\</span><br>7x - 2y + 4z, <span class="hljs-built_in">&amp;</span> 1<span class="hljs-keyword">\le</span> x+y+z &lt;5<span class="hljs-keyword">\\</span><br>-6x + 3y + 2z, <span class="hljs-built_in">&amp;</span> x+y+z &gt; 1 <br><span class="hljs-keyword">\end</span>&#123;cases&#125;<br></code></pre></td></tr></table></figure><div class="note note-warning">            <p>记得<code>case</code>包应该是包括在<code>amsmath</code>中的。但是如果编译时遇到奇怪的问题，建议两个都加上：</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\usepackage</span>&#123;amsmath&#125;<br><span class="hljs-keyword">\usepackage</span>&#123;cases&#125;<br></code></pre></td></tr></table></figure>          </div><ol start="4" type="1"><li>数组</li></ol><p><span class="math display">$$\begin{array}{|c|c||c|} a &amp; b &amp; S \\\hline0&amp;0&amp;1\\0&amp;1&amp;1\\1&amp;0&amp;1\\1&amp;1&amp;0\\\end{array}$$</span></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;array&#125;&#123;|c|c||c|&#125; a <span class="hljs-built_in">&amp;</span> b <span class="hljs-built_in">&amp;</span> S <span class="hljs-keyword">\\</span><br><span class="hljs-keyword">\hline</span><br>0<span class="hljs-built_in">&amp;</span>0<span class="hljs-built_in">&amp;</span>1<span class="hljs-keyword">\\</span><br>0<span class="hljs-built_in">&amp;</span>1<span class="hljs-built_in">&amp;</span>1<span class="hljs-keyword">\\</span><br>1<span class="hljs-built_in">&amp;</span>0<span class="hljs-built_in">&amp;</span>1<span class="hljs-keyword">\\</span><br>1<span class="hljs-built_in">&amp;</span>1<span class="hljs-built_in">&amp;</span>0<span class="hljs-keyword">\\</span><br><span class="hljs-keyword">\end</span>&#123;array&#125;<br></code></pre></td></tr></table></figure><h2id="四国际标准与国内学位论文规范iso-80000-2-与-gbt-3102.11-核心原则各类数学符号的详细规范">四、国际标准与国内学位论文规范：ISO80000-2 与 GB/T 3102.11 (核心原则)——各类数学符号的详细规范</h2><h3 id="标量scalars">1. 标量（Scalars）</h3><table><thead><tr><th>属性</th><th>规范</th><th>LaTeX 写法</th><th>示例</th></tr></thead><tbody><tr><td><strong>字体</strong></td><td>斜体（Italic）</td><td>默认数学模式</td><td><span class="math display"><em>x</em></span>, <spanclass="math display"><em>α</em></span> <code>$x$, $\alpha$</code></td></tr><tr><td><strong>大小写</strong></td><td>小写为主</td><td></td><td><spanclass="math display"><em>x</em>, <em>y</em>, <em>z</em>, <em>t</em>, <em>n</em></span><code>$x, y, z, t, n$</code></td></tr><tr><td><strong>颜色</strong></td><td>白体（非粗体）</td><td></td><td></td></tr></tbody></table><p><strong>规则详解</strong>：</p><ul><li>所有表示<strong>物理量</strong>或<strong>变量</strong>的字母，无论拉丁还是希腊字母，一律用<strong>斜体</strong></li><li>数学常数（如 <code>$\mathrm&#123;e&#125;$</code> , <code>$\mathrm&#123;e&#125;$</code> ,<code>$\mathrm&#123;i&#125;$</code> ）用<strong>正体</strong>（罗马体）</li><li>下标若表示变量或数字用斜体，若表示缩写或标签用正体</li></ul><h3 id="向量vectors">2. 向量（Vectors）</h3><div class="note note-warning">            <p>注意：本博客由于使用mathjax渲染器渲染公式，网页效果上导致有些地方看不出来倾斜，是nodejs和hexo的公式渲染方面的问题，实际效果请以LaTex模板中的效果为准。当时hexog 生成的时候公式部分报错：</p><p>[WARNING] Could not convert TeX math,…, redering as TeX</p><p>可能导致网页上显示效果有问题。很多环境得在LaTeX的专业数学环境中才是最准确的渲染显示。</p>          </div><table><thead><tr><th>属性</th><th>规范</th><th>LaTeX 写法</th><th>渲染效果</th></tr></thead><tbody><tr><td><strong>字体</strong></td><td><strong>粗斜体</strong>（Bold Italic）</td><td><code>\boldsymbol&#123;v&#125;</code> 或 <code>\mathbold&#123;v&#125;</code></td><td><span class="math inline"><strong>v</strong></span><code>$\boldsymbol&#123;v&#125;$</code></td></tr><tr><td><strong>大小写</strong></td><td>小写</td><td></td><td><spanclass="math inline"><strong>x</strong>, <strong>θ</strong></span><code>$\boldsymbol&#123;x&#125;, \boldsymbol&#123;\theta&#125;$</code></td></tr><tr><td><strong>希腊字母</strong></td><td>同样粗斜体</td><td><code>\boldsymbol&#123;\alpha&#125;</code></td><td><span class="math inline"><strong>α</strong></span><code>$\boldsymbol&#123;\alpha&#125;$</code></td></tr></tbody></table><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-comment">% 正确</span><br><span class="hljs-built_in">$</span>x<span class="hljs-built_in">_</span>i<span class="hljs-built_in">$</span>          <span class="hljs-comment">% 变量x，下标i（变量，斜体）</span><br><span class="hljs-built_in">$</span>x<span class="hljs-built_in">_</span>&#123;<span class="hljs-keyword">\max</span>&#125;<span class="hljs-built_in">$</span>     <span class="hljs-comment">% 变量x，下标max（缩写，正体）</span><br><span class="hljs-built_in">$</span>v<span class="hljs-built_in">_</span>&#123;<span class="hljs-keyword">\mathrm</span>&#123;in&#125;&#125;<span class="hljs-built_in">$</span>  <span class="hljs-comment">% 变量v，下标in（标签，正体）</span><br><br><span class="hljs-comment">% 错误</span><br><span class="hljs-built_in">$</span>x<span class="hljs-built_in">_</span>&#123;in&#125;<span class="hljs-built_in">$</span>       <span class="hljs-comment">% 下标in会被解释为变量i和变量n的乘积</span><br></code></pre></td></tr></table></figure><p><strong>LaTeX 实现方法</strong>：</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\usepackage</span>&#123;amsmath&#125;      <span class="hljs-comment">% 基础支持</span><br><span class="hljs-keyword">\usepackage</span>&#123;fixmath&#125;      <span class="hljs-comment">% 提供 \mathbold 命令（推荐）</span><br><span class="hljs-keyword">\usepackage</span>&#123;isomath&#125;      <span class="hljs-comment">% 完整的ISO标准支持（最推荐）</span><br><br><span class="hljs-comment">% 定义快捷命令（强烈推荐）</span><br><span class="hljs-keyword">\renewcommand</span>&#123;<span class="hljs-keyword">\vec</span>&#125;[1]&#123;<span class="hljs-keyword">\mathbold</span>&#123;<span class="hljs-params">#1</span>&#125;&#125;  <span class="hljs-comment">% 向量小写粗斜体</span><br><span class="hljs-keyword">\newcommand</span>&#123;<span class="hljs-keyword">\mat</span>&#125;[1]&#123;<span class="hljs-keyword">\mathbold</span>&#123;<span class="hljs-params">#1</span>&#125;&#125;    <span class="hljs-comment">% 矩阵大写粗斜体</span><br><br><span class="hljs-comment">% 使用示例</span><br><span class="hljs-built_in">$</span><span class="hljs-keyword">\vec</span>&#123;x&#125; = [x<span class="hljs-built_in">_</span>1, x<span class="hljs-built_in">_</span>2, <span class="hljs-keyword">\dots</span>, x<span class="hljs-built_in">_</span>n]<span class="hljs-built_in">^</span><span class="hljs-keyword">\mathsf</span>&#123;T&#125;<span class="hljs-built_in">$</span>  <span class="hljs-comment">% 列向量</span><br><span class="hljs-built_in">$</span><span class="hljs-keyword">\vec</span>&#123;<span class="hljs-keyword">\beta</span>&#125; = <span class="hljs-keyword">\vec</span>&#123;X&#125;<span class="hljs-keyword">\vec</span>&#123;w&#125; + <span class="hljs-keyword">\vec</span>&#123;b&#125;<span class="hljs-built_in">$</span>       <span class="hljs-comment">% 含希腊字母</span><br></code></pre></td></tr></table></figure><p><strong>注意</strong>：清华、中科大等高校模板明确要求向量使用<strong>粗斜体</strong>（黑斜体），而非简单的<code>\mathbf</code>（黑正体）</p><hr /><h3 id="矩阵matrices">3. 矩阵（Matrices）</h3><table><thead><tr><th style="text-align: left;">属性</th><th style="text-align: left;">规范</th><th style="text-align: left;">LaTeX 写法</th><th style="text-align: left;">渲染效果</th></tr></thead><tbody><tr><td style="text-align: left;"><strong>字体</strong></td><td style="text-align: left;"><strong>粗斜体</strong>（BoldItalic）</td><td style="text-align: left;"><code>\mathbold&#123;X&#125;</code> 或<code>\boldsymbol&#123;X&#125;</code></td><td style="text-align: left;"><spanclass="math inline"><strong>X</strong></span><code>$\boldsymbol&#123;X&#125;$</code></td></tr><tr><td style="text-align: left;"><strong>大小写</strong></td><td style="text-align: left;"><strong>大写</strong></td><td style="text-align: left;"></td><td style="text-align: left;"><spanclass="math inline"><strong>W</strong></span> , <spanclass="math inline"><strong>Σ</strong></span><code>$\boldsymbol&#123;W&#125;$ , $\boldsymbol&#123;\Sigma&#125;$</code></td></tr><tr><td style="text-align: left;"><strong>希腊字母</strong></td><td style="text-align: left;">同样粗斜体</td><td style="text-align: left;"><code>\mathbold&#123;\Sigma&#125;</code></td><td style="text-align: left;"><spanclass="math inline"><strong>Σ</strong></span><code>$\boldsymbol&#123;\Sigma&#125;</code></td></tr></tbody></table><p><strong>关键区分</strong>：</p><ul><li><strong>矩阵</strong>：大写粗斜体 <spanclass="math display"><strong>X</strong></span></li><li><strong>向量的转置/矩阵的列</strong>：若提取矩阵的第 <spanclass="math display"><em>i</em></span> 列 <spanclass="math display"><strong>x</strong><sub><em>i</em></sub></span>，仍是<strong>向量</strong>，保持粗斜体</li><li><strong>矩阵元素</strong>：标量，小写斜体 <spanclass="math display"><em>x</em><sub><em>i</em><em>j</em></sub></span>（<strong>不加粗</strong>）</li></ul><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-comment">% 矩阵定义</span><br><span class="hljs-built_in">$</span><span class="hljs-keyword">\mat</span>&#123;X&#125; <span class="hljs-keyword">\in</span> <span class="hljs-keyword">\mathbb</span>&#123;R&#125;<span class="hljs-built_in">^</span>&#123;n <span class="hljs-keyword">\times</span> d&#125;<span class="hljs-built_in">$</span>  <span class="hljs-comment">% 矩阵X，大写粗斜体</span><br><span class="hljs-built_in">$</span><span class="hljs-keyword">\mat</span>&#123;W&#125;<span class="hljs-built_in">^</span><span class="hljs-keyword">\mathsf</span>&#123;T&#125;<span class="hljs-keyword">\mat</span>&#123;W&#125; = <span class="hljs-keyword">\mat</span>&#123;I&#125;<span class="hljs-built_in">$</span>  <span class="hljs-comment">% 矩阵运算</span><br><br><span class="hljs-comment">% 矩阵元素（注意：不加粗！）</span><br><span class="hljs-built_in">$</span>x<span class="hljs-built_in">_</span>&#123;ij&#125;<span class="hljs-built_in">$</span> 表示矩阵 <span class="hljs-built_in">$</span><span class="hljs-keyword">\mat</span>&#123;X&#125;<span class="hljs-built_in">$</span> 的第 <span class="hljs-built_in">$</span>i<span class="hljs-built_in">$</span> 行第 <span class="hljs-built_in">$</span>j<span class="hljs-built_in">$</span> 列元素<br><br><span class="hljs-comment">% 矩阵的列（是向量，加粗）</span><br><span class="hljs-built_in">$</span><span class="hljs-keyword">\vec</span>&#123;x&#125;<span class="hljs-built_in">_</span>j<span class="hljs-built_in">$</span> 表示矩阵 <span class="hljs-built_in">$</span><span class="hljs-keyword">\mat</span>&#123;X&#125;<span class="hljs-built_in">$</span> 的第 <span class="hljs-built_in">$</span>j<span class="hljs-built_in">$</span> 列<br></code></pre></td></tr></table></figure><hr /><h3 id="张量tensors">4. 张量（Tensors）</h3><table><thead><tr><th style="text-align: left;">属性</th><th style="text-align: left;">规范</th><th style="text-align: left;">LaTeX 写法</th><th style="text-align: left;">渲染效果</th></tr></thead><tbody><tr><td style="text-align: left;"><strong>字体</strong></td><td style="text-align: left;"><strong>无衬线粗斜体</strong>（Sans-serifBold Italic）</td><td style="text-align: left;"><code>\tens&#123;A&#125;</code>（需isomath）</td><td style="text-align: left;"></td></tr><tr><td style="text-align: left;"><strong>或替代</strong></td><td style="text-align: left;"><strong>粗正体</strong></td><td style="text-align: left;"><code>$\mathbf&#123;A&#125;$</code></td><td style="text-align: left;"><spanclass="math inline"><strong>A</strong></span></td></tr><tr><td style="text-align: left;"><strong>或替代</strong></td><td style="text-align: left;">花体/书法体（Calligraphic）</td><td style="text-align: left;"><code>$\mathcal&#123;T&#125;$</code></td><td style="text-align: left;"><span class="math inline">𝒯</span></td></tr><tr><td style="text-align: left;"><strong>大小写</strong></td><td style="text-align: left;">大写为主</td><td style="text-align: left;"></td><td style="text-align: left;"></td></tr></tbody></table><p><strong>ISO 80000-2严格规定</strong>：张量应使用<strong>无衬线粗斜体</strong>（sans-serifbold italic），这是与矩阵的主要区别</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\usepackage</span>&#123;isomath&#125;  <span class="hljs-comment">% 提供张量命令</span><br><br><span class="hljs-comment">% ISO标准写法（无衬线粗斜体）</span><br><span class="hljs-built_in">$</span><span class="hljs-keyword">\tens</span>&#123;T&#125;, <span class="hljs-keyword">\tens</span>&#123;X&#125;, <span class="hljs-keyword">\tens</span>&#123;<span class="hljs-keyword">\alpha</span>&#125;<span class="hljs-built_in">$</span>  <span class="hljs-comment">% \tens需要isomath包</span><br><br><span class="hljs-comment">% 替代方案（若期刊/学校无严格要求）</span><br><span class="hljs-built_in">$</span><span class="hljs-keyword">\mathcal</span>&#123;T&#125;, <span class="hljs-keyword">\mathbfcal</span>&#123;T&#125;<span class="hljs-built_in">$</span>        <span class="hljs-comment">% 花体或粗花体</span><br><span class="hljs-built_in">$</span><span class="hljs-keyword">\mathbf</span>&#123;<span class="hljs-keyword">\mathsf</span>&#123;T&#125;&#125;<span class="hljs-built_in">$</span>               <span class="hljs-comment">% 无衬线粗体（非斜体）</span><br></code></pre></td></tr></table></figure><p><strong>深度学习中</strong>：由于张量与矩阵界限模糊，通常沿用<strong>大写粗正体</strong><spanclass="math display"><strong>X</strong></span>(<code>\mathbf&#123;X&#125;</code>)。(个人看机器学习/模式识别学习国外教科书上，张量用大写粗正体，变为个人习惯)。</p><hr /><h3 id="随机变量random-variables">5. 随机变量（Random Variables）</h3><table><thead><tr><th style="text-align: left;">属性</th><th style="text-align: left;">规范</th><th style="text-align: left;">LaTeX 写法</th><th style="text-align: left;">渲染效果</th></tr></thead><tbody><tr><td style="text-align: left;"><strong>字体</strong></td><tdstyle="text-align: left;">斜体（Italic，<strong>不加粗</strong>）</td><td style="text-align: left;">默认数学模式</td><td style="text-align: left;"><spanclass="math inline"><em>X</em></span>,<spanclass="math inline"><em>Y</em></span>,<spanclass="math inline"><em>Z</em></span> <code>$X$, $Y$, $Z$</code></td></tr><tr><td style="text-align: left;"><strong>大小写</strong></td><td style="text-align: left;">大写</td><td style="text-align: left;"></td><td style="text-align: left;"><spanclass="math inline"><em>X</em> ∼ 𝒩(<em>μ</em>, <em>σ</em><sup>2</sup>)</span><code>$X \sim \mathcal&#123;N&#125;(\mu, \sigma^2)$</code></td></tr></tbody></table><p><strong>区分要点</strong>：</p><ul><li><strong>随机变量</strong>：大写斜体（非粗体）$<spanclass="math inline"><em>X</em></span> $</li><li><strong>观测值/样本</strong>：小写斜体 <spanclass="math display"><em>x</em></span></li><li><strong>矩阵形式的数据</strong>：大写粗斜体 <spanclass="math display"><strong>X</strong></span>（此时是数据矩阵，非随机变量）</li></ul><hr /><h3 id="完整对照表">6.完整对照表</h3><table><thead><tr><th style="text-align: left;">类型</th><th style="text-align: left;">实体示例</th><th style="text-align: left;">数学表示</th><th style="text-align: left;">LaTeX 代码</th></tr></thead><tbody><tr><td style="text-align: left;"><strong>标量</strong></td><td style="text-align: left;">学习率、损失值</td><td style="text-align: left;"><spanclass="math inline"><em>α</em></span>,<spanclass="math inline"><em>L</em></span>, <spanclass="math inline">ℒ</span>, <spanclass="math inline"><em>x</em><sub><em>i</em></sub></span></td><tdstyle="text-align: left;"><code>$\alpha$, $L$, $\mathcal&#123;L&#125;$, $x_i$</code></td></tr><tr><td style="text-align: left;"><strong>向量</strong></td><td style="text-align: left;">权重向量、特征向量</td><td style="text-align: left;"><spanclass="math inline"><em>x⃗</em></span>, <spanclass="math inline"><strong>x</strong></span></td><td style="text-align: left;"><code>\vec&#123;x&#125;, \boldsymbol&#123;x&#125;</code></td></tr><tr><td style="text-align: left;"><strong>矩阵</strong></td><td style="text-align: left;">权重矩阵、协方差</td><td style="text-align: left;"><spanclass="math inline"><strong>W</strong></span>, <spanclass="math inline"><strong>Σ</strong></span></td><tdstyle="text-align: left;"><code>\mat&#123;W&#125;, \mat&#123;\Sigma&#125;,\boldsymbol&#123;W&#125;, \boldsymbol&#123;\Sigma&#125;</code></td></tr><tr><td style="text-align: left;"><strong>张量</strong></td><td style="text-align: left;">高阶张量</td><td style="text-align: left;"><spanclass="math inline"><strong>T</strong></span>或<spanclass="math inline">𝒯</span></td><td style="text-align: left;"><code>\tens&#123;T&#125;</code> 或<code>\mathbfcal&#123;T&#125;</code></td></tr><tr><td style="text-align: left;"><strong>随机变量</strong></td><td style="text-align: left;">概率分布</td><td style="text-align: left;"><spanclass="math inline"><em>X</em></span>, <spanclass="math inline"><em>Y</em> ∼ 𝒩(<em>x</em>)</span></td><tdstyle="text-align: left;"><code>$X$, $Y\sim \mathcal&#123;N&#125;(x)$</code></td></tr><tr><td style="text-align: left;"><strong>集合</strong></td><td style="text-align: left;">空间、定义域</td><td style="text-align: left;"><span class="math inline">𝒳</span>, <spanclass="math inline">ℝ</span></td><tdstyle="text-align: left;"><code>$\mathcal&#123;X&#125;$, $\mathbb&#123;R&#125;$</code></td></tr><tr><td style="text-align: left;"><strong>转置</strong></td><td style="text-align: left;">矩阵转置</td><td style="text-align: left;"><spanclass="math inline">$\boldsymbol{X}^\trans$</span></td><td style="text-align: left;"><code>$\mat&#123;X&#125;^\trans$</code></td></tr><tr><td style="text-align: left;"><strong>微分</strong></td><td style="text-align: left;">微分算子</td><td style="text-align: left;"><spanclass="math inline">d<em>x</em></span>, <spanclass="math inline">∂<em>f</em></span>, <spanclass="math inline">∫</span></td><tdstyle="text-align: left;"><code>$\mathrm&#123;d&#125;x$, $\partial f$, $\int$</code></td></tr></tbody></table><hr /><h2 id="中国学位论文特殊注意事项">中国学位论文特殊注意事项</h2><p>根据 <strong>GB/T 7713.2-2022</strong>和高校模板（清华、中科大、合工大等）：</p><ol type="1"><li><p><strong>大写希腊字母</strong>：默认为斜体（与TeX默认不同），但 Δ（有限增量）固定用<strong>正体</strong></p></li><li><p><strong>积分号、偏微分</strong>：使用<strong>正体</strong> <spanclass="math display">∫</span><code>$\int$</code>与 <spanclass="math inline">$\uppartial$</span><code>\uppartial</code>，具体如下：</p></li></ol><table><thead><tr><th>符号</th><th>形式</th><th>LaTeX 代码</th><th>标准依据</th></tr></thead><tbody><tr><td><strong>积分号</strong></td><td>正体（推荐）</td><td><code>\int</code>（需字体支持）</td><td>ISO 80000-2、GB/T 3102.11</td></tr><tr><td><strong>积分号</strong></td><td>斜体（传统）</td><td><code>\int</code>（默认）</td><td>AMS/TeX 传统</td></tr><tr><td><strong>偏微分</strong></td><td>正体（推荐）</td><td><code>\uppartial</code> 或 <code>\partial</code>（需额外配置）</td><td>GB/T 3102.11、清华/中科大模板</td></tr><tr><td><strong>偏微分</strong></td><td>斜体（默认）</td><td><code>\partial</code></td><td>AMS/TeX 传统</td></tr></tbody></table><ol start="3" type="1"><li><p><strong>小于等于号</strong>：使用倾斜字形 <spanclass="math display">≤</span> <code>$\leqslant$</code> 与<spanclass="math display">≥</span> <code>$\geqslant$</code>（非 <spanclass="math display">≤</span> <code>$\le$</code> 与 <spanclass="math display">≥</span> <code>$\geq$</code>）</p></li><li><p><strong>省略号</strong>：居中 <span class="math inline">⋯</span><code>$\cdots$</code> （非基线对齐<span class="math inline">…</span><code>$\ldots$</code>）</p></li><li><p><strong>自然对数</strong>：用 <spanclass="math display">ln <em>x</em></span>（不用 <spanclass="math display">log <em>x</em></span> ）</p></li></ol><p><strong>关键提醒</strong>：无论使用何种符号，<strong>全文必须统一</strong>！建议在导言区定义语义命令（如<code>\vec</code>, <code>\mat</code>），而非在正文中直接写<code>\mathbf</code> 或<code>\boldsymbol</code>，以便后期统一修改格式。</p><p>(未完待续，动态补充)</p><h2 id="参考资料">参考资料</h2><ol type="1"><li>https://www.cnblogs.com/yalphait/articles/8685586.html</li><li>https://zhuanlan.zhihu.com/p/266267223</li><li>https://blog.csdn.net/u012684062/article/details/78398191</li><li>Kimi大模型搜索结果</li></ol>]]></content>
    
    
    <categories>
      
      <category>LaTex学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LaTex</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文十问:Map-view Transformer(CVPR2022)</title>
    <link href="/posts/7e4de446/"/>
    <url>/posts/7e4de446/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="论文十问cross-view-transformers-for-real-time-map-view-semantic-segmentationcvpr-2022">论文十问：Cross-viewTransformers for real-time Map-view Semantic Segmentation(CVPR2022)</h1><blockquote><p>论文标题：Cross-view Transformers for real-time Map-view SemanticSegmentation(CVPR 2022)<br> 论文地址：<ahref="https://arxiv.org/abs/2205.02833">https://arxiv.org/abs/2205.02833</a><br>作者单位：The Chinese University of Hong Kong<br> 代码地址：<ahref="https://github.com/bradyz/cross_view_transformers">https://github.com/bradyz/cross_view_transformers</a><br>一句话读论文：Our architecture implicitly learns a mapping fromindividual camera views into a canonical map-view representation using acamera-aware cross-view attention mechanism.</p></blockquote><h2 id="q1">Q1</h2><p>论文试图解决什么问题？</p><p>做图像特征与地图特征的融合(“model geometry and relationships betweendifferent view and a canonical map representation”)</p><h2 id="q2">Q2</h2><p>这是否是一个新的问题？</p><p>利用地图信息作为query，参与语义分割网络的跨视图融合，是一个有新意的做法</p><h2 id="q3">Q3</h2><p>这篇文章要验证一个什么科学假设？</p><h2 id="q4">Q4</h2><p>有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</p><p>做俯视图语义分割，已有方法大致可归为如下两类(包含其存在的问题)Image-based depth estimation are error-prone. Depth-based projectionsare a fairly inflexible and rigid bottleneck to map between views.</p><p>附一份知乎笔记连接：https://zhuanlan.zhihu.com/p/511477453</p><h2 id="q5">Q5</h2><p>论文中提到的解决方案之关键是什么？</p><p>通过cross-view transoformer来做Camera View到MapView的融合。相比于已有方法基于显式地几何关系地映射，这种融合的方式是一种隐式函数的映射(“learnany geometric transformation implicitly and directly fromdata”)。此外，transformer需要positionalembedding来区分不同空间位置的特征。本文因此设计了camera-aware和map-view两类positionalembedding。</p><h2 id="q6">Q6</h2><p>论文中的实验是如何设计的？</p><h2 id="q7">Q7</h2><p>用于定量评估的数据集是什么？代码有没有开源？</p><p>nuScenes, 已经开源</p><h2 id="q8">Q8</h2><p>论文中的实验及结果有没有很好地支持需要验证的科学假设？</p><p>在俯视图的语义分割中是SOTA(37.5% mIoU),和基于深度估计与投影等已有方法相比comaprable。但是整个赛道与图像语义分割与3D语义分割结果相比(70-80mIoU)，整体后面还有挖掘空间</p><h2 id="q9">Q9</h2><p>这篇论文到底有什么贡献？</p><p>1）利用了地图信息，这是一个比较有新意的setting.2）注意力系数计算方式比较有新意，可以参考拓展</p><h2 id="q10">Q10</h2><p>下一步呢？有什么工作可以继续深入？</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>多模态融合</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>一句话读论文系列</tag>
      
      <tag>CVPR2022</tag>
      
      <tag>点云语义分割</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一句话读论文:Map-view Transformer(CVPR2022)</title>
    <link href="/posts/17ecbaa3/"/>
    <url>/posts/17ecbaa3/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="一句话读论文cross-view-transformers-for-real-time-map-view-semantic-segmentationcvpr-2022">一句话读论文：Cross-viewTransformers for real-time Map-view Semantic Segmentation(CVPR2022)</h1><blockquote><p>论文标题：Cross-view Transformers for real-time Map-view SemanticSegmentation(CVPR 2022)<br> 论文地址：<ahref="https://arxiv.org/abs/2205.02833">https://arxiv.org/abs/2205.02833</a><br>作者单位：The Chinese University of Hong Kong<br> 代码地址：<ahref="https://github.com/bradyz/%20cross_view_transformers">https://github.com/bradyz/cross_view_transformers<br></a> 一句话读论文：Our architectureimplicitly learns a mapping from individual camera views into acanonical map-view representation using a camera-aware cross-viewattention mechanism.</p></blockquote><h2 id="网络框架">网络框架：</h2><figure><img src="https://s2.loli.net/2022/05/24/okSpGdcwzAjVeiu.png"alt="image-20220523111833587" /><figcaption aria-hidden="true">image-20220523111833587</figcaption></figure><h2 id="核心内容"><strong>核心内容：</strong></h2><p>​ Motivation: 想做图像特征与地图特征的融合(“model geometry andrelationships between different view and a canonical maprepresentation”)</p><p>​ 已有方法的问题：</p><ul><li>Image-based depth estimation are error-prone.</li><li>Depth-based projections are a fairly inflexible and rigid bottleneckto map between views.</li></ul><p>​ 本文的思路：</p><p>​ 是通过cross-view transoformer来做Map View 到CameraView的融合。相比于已有方法基于显式地几何关系地映射，这种融合的方式是一种隐式函数的映射(“learnany geometric transformation implicitly and directly fromdata”)。此外，transformer需要positionalembedding来区分不同空间位置的特征。</p><p>​ 注意力系数的计算方式比较巧妙： <spanclass="math display"><em>x</em><sup>(<em>I</em>)</sup> ≃ <em>K</em><sub><em>k</em></sub><em>R</em><sub><em>k</em></sub>(<em>x</em><sup>(<em>W</em>)</sup> − <em>t</em><sub><em>k</em></sub>)</span>​上述公式展示了世界坐标系<spanclass="math display"><em>x</em><sup>(<em>W</em>)</sup></span>与相机坐标系<spanclass="math display"><em>x</em><sup>(<em>I</em>)</sup></span>的映射关系，其中<spanclass="math display"><em>t</em><sub><em>k</em></sub></span>是车辆在行驶时的平移，<spanclass="math display"><em>K</em><sub><em>k</em></sub></span>与<spanclass="math display"><em>R</em><sub><em>k</em></sub></span>分别是相机内参矩阵和外参旋转矩阵(相机位姿相对于LiDAR传感器位姿)。<span class="math display">$$sim_k(x^{(I)}, x^{(W)}) = \frac{(R_K^{-1}K_K^{-1}x^{(I)})\cdot (x^{(W)}- t_k)}{\Vert (R_K^{-1}K_K^{-1}x^{(I)})\Vert \Vert (x^{(W)} - t_k)\Vert}$$</span> ​这样将3D坐标点和2D图像对应的坐标联系起来，计算其相似度系数。由于本文中没有3DLiDAR点云数据参与，所以从地图中只能获得xy坐标的信息，无法获得高度(深度)信息。所以这里用的是经过MLP后提取点特征代替了直接使用坐标。</p><blockquote><p>几何意义：The uprojected image coordinate <spanclass="math display"><em>d</em><sub><em>k</em>, <em>i</em></sub> = <em>R</em><sub><em>k</em></sub><sup>−1</sup><em>K</em><sub><em>k</em></sub><sup>−1</sup><em>x</em><sub><em>i</em></sub><sup>(<em>I</em>)</sup></span>for each image coordinate <spanclass="math display"><em>x</em><sub><em>i</em></sub><sup>(<em>I</em>)</sup></span>described a direction vector from the origin <spanclass="math display"><em>t</em><sub><em>k</em></sub></span> of camera<span class="math display"><em>k</em></span> to the image plane at depth1.</p><p>代码实现：1)We encode this direction vector <spanclass="math display"><em>d</em><sub><em>k</em>, <em>i</em></sub></span>using an MLP(shared across k views) into a D-dimensional positionalembedding <spanclass="math display"><em>δ</em><sub><em>k</em>, <em>i</em></sub> ∈ ℝ<sup><em>D</em></sup></span>…We combine this positional embedding with image features <spanclass="math display"><em>ϕ</em><sub><em>k</em>, <em>i</em></sub></span>in the keys of our cross-view attention mechanism.</p><p>2)<span class="math display"><em>x</em><sup><em>W</em></sup></span>从地图获得，没有高度信息怎么办：We start with a learned positionalencoding <spanclass="math display"><em>c</em><sup>(0)</sup> ∈ ℝ<sup><em>w</em> × <em>h</em> × <em>D</em></sup></span>.We build the map-view representation up over multiple iterations in ourtransformer… Each positional embedding is better able to project themap-view coordinates into a proxy of the 3D environment.</p></blockquote><p>基于上述几何映射的注意力系数计算方式，论文做如下改动作为实际的计算方法：</p><p><span class="math display">$$sim(\delta_{k,i}, \phi_{k,i}, c_j^{n}, \tau_k) = \frac{(\delta_{k,i}+\phi_{k,i})\cdot(c_j^{(n)} - \tau_k)}{\Vert \delta_{k,i}+ \phi_{k,i}\Vert \Vert c_j^{(n)} - \tau_k \Vert }$$</span></p><p>​ <spanclass="math display"><em>δ</em><sub><em>k</em>, <em>j</em></sub></span>表示图像的Camera-viewpositional embedding; <spanclass="math display"><em>ϕ</em><sub><em>k</em>, <em>i</em></sub> = <em>M</em><em>L</em><em>P</em>[<em>d</em><sub><em>k</em>, <em>i</em></sub>] = <em>M</em><em>L</em><em>P</em>[(<em>R</em><sub><em>K</em></sub><sup>−1</sup><em>K</em><sub><em>K</em></sub><sup>−1</sup><em>x</em><sup>(<em>I</em>)</sup>)]</span>表示经过MLP映射后的<spanclass="math display"><em>D</em></span>维向量。相比于原始的计算方式，这里把positionalembedding和图像feature加在一起参与运算。<spanclass="math display"><em>c</em><sub><em>j</em></sub><sup>(<em>n</em>)</sup></span>和<spanclass="math display"><em>τ</em><sub><em>k</em></sub></span>意义和原始公式相同，不同之处是他们也是经过Transforemr和MLP映射到<spanclass="math display"><em>D</em></span>维向量。</p><h2 id="实验结果"><strong>实验结果：</strong></h2><figure><img src="https://s2.loli.net/2022/05/23/sk2fGHMORJ8UjWX.png"alt="image-20220523135309538" /><figcaption aria-hidden="true">image-20220523135309538</figcaption></figure><figure><img src="https://s2.loli.net/2022/05/23/wdDPYJWxlhzFv8s.png"alt="image-20220523135440759" /><figcaption aria-hidden="true">image-20220523135440759</figcaption></figure><figure><img src="https://s2.loli.net/2022/05/23/Y84bdTZ75nNxlJ2.png"alt="image-20220523135515717" /><figcaption aria-hidden="true">image-20220523135515717</figcaption></figure><figure><img src="https://s2.loli.net/2022/05/23/z4uAlfWCjPGHJ7v.png"alt="image-20220523140115644" /><figcaption aria-hidden="true">image-20220523140115644</figcaption></figure><h2 id="related-work可选"><strong>Related Work(可选):</strong></h2><ul><li></li></ul><h2id="你认为优点不足可以拓展改进的地方可选"><strong>你认为优点/不足/可以拓展改进的地方(可选):</strong></h2><p>优点：</p><ul><li>利用了地图信息，这是一个比较有新意的setting.</li><li>提出的transformer不是简单的高维特征计算cos复杂度。从理论上来说，本文是巧妙利用三维点与二维相机视角存在的仿射变换的关系构建的计算方法。原始公式中只需要坐标信息，然后在此基础上拓展了图像和地图的positionalembedding以及各自的特征信息(如transformer得到的特征向量，图像颜色，点云的极坐标信息等)，可拓展性强。</li></ul><p>不足：</p><ul><li>本文中没有用到LiDAR点云数据。讲道理从LiDAR数据中可以直接获得准确的高度信息，为什么不用呢(是为了提高模型推理速度，避免使用点云数据?)。这样的话从地图中经过多层transformer来估计特征向量的方式总觉得有瑕疵。</li><li>实验评测在语义分割上做的，但是做的是俯视图语义分割而不是3D语义分割。实际应用中3D语义分割和2D图像语义分割比俯视图语义分割实用很多。俯视图语义分割的重要性有待商榷。此外实验结果虽然和传统方法comparable证明这种利用transformer的新技术路线是有用的，但是30多的mIoU和现在3D语义分割中78的mIoU相比仍然逊色很多。</li></ul><p>可以拓展改进的地方：</p><ul><li>后面利用这套同样的idea，去做一做3D语义分割或2D图像分割。这样能挖掘一个更有意思的课题，我从地图中学习到的信息对CameraView和3D点云分割能起到什么帮助？</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>多模态融合</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>一句话读论文系列</tag>
      
      <tag>CVPR2022</tag>
      
      <tag>点云语义分割</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文十问:DeepFusion(CVPR2022)</title>
    <link href="/posts/1cc46c4f/"/>
    <url>/posts/1cc46c4f/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="deepfusioncvpr-2022">DeepFusion(CVPR 2022)</h1><blockquote><p>论文标题：DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3DObject Detection<br> 论文地址：<ahref="https://arxiv.org/abs/2203.08195">https://arxiv.org/abs/2203.08195</a><br>作者单位：Johns Hopkins University, Google<br> 代码地址：<ahref="https://github.com/tensorflow/lingvo">https://github.com/tensorflow/lingvo</a><br>一句话读论文：This study points out the key role of transformed featurealignment process that plays in multi-modal fusion module, and thusproposes InverseAug and LearnableAlign module to overcome incosistencybetween data augmentation and multi-modal fusion.</p></blockquote><h2 id="q1">Q1</h2><p>论文试图解决什么问题？</p><p>本工作试图解决RGB图像-LiDAR点云网络，由于图像和点云分别进行数据增强操作(如随机旋转)，而导致模态特征之间的对应关系被破坏，使得多模态学习带来的增益被削弱的问题。</p><h2 id="q2">Q2</h2><p>这是否是一个新的问题？</p><p>这是一个比较有意思的，也很实用的新问题1.因为大规模LiDAR点云数据需要大量的数据增强操作利于刷榜，如果多模态与之有冲突，那么提点效果会被大幅降低。2.2020年前的多模态融合多是RGB图像-点云伪图像(环形投影，BeV,透视投影等)的融合，本质是2D-2D网络间的融合。而2021年开始有更多3D-2D网络的多模态模型，因此在这个背景下考虑数据增强和特征对齐的冲突问题，还是比较有意思的</p><h2 id="q3">Q3</h2><p>这篇文章要验证一个什么科学假设？</p><p>1.特征对齐对多模态发挥效果很重要(参考Table1,9) -&gt; 提出InverseAug\2. 深层网络的特征向量在不同模态间对齐，对模型学习语义信息有利 -&gt;提出基于注意力机制的融合方式</p><h2 id="q4">Q4</h2><p>有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</p><p>\1. Input-Level Decoration:在原始数据层面,利用图形学中地仿射变换与投影，将LiDAR点与Pixel对应。相关工作有：PointPainting(CVPR 2021), PointAugmentation(CVPR 2021), PMF(ICCV2021)</p><p>\2. Mid-Level Fusion:在网络结构层面，对每一层的特征向量隐式融合。相关工作有： Deep ContinuousFusion(ECCV 2018) EP-Net(CVPR 2018), 4D-Net(ICCV 2021), Cross viewTransformer for real-time Map-view Semantic Segmentation(CVPR 2022)TransFusion(CVPR 2022)等</p><h2 id="q5">Q5</h2><p>论文中提到的解决方案之关键是什么？</p><p>\1. InverseAug:把图像和点云分支的数据增强操作取逆，再用仿射变换投影。</p><p>\2. LearnableAlign: 比较常见的Attention的融合方式</p><h2 id="q6">Q6</h2><p>论文中的实验是如何设计的？</p><p>略………………</p><h2 id="q7">Q7</h2><p>用于定量评估的数据集是什么？代码有没有开源？</p><p>Waymo 。开源。</p><h2 id="q8">Q8</h2><p>论文中的实验及结果有没有很好地支持需要验证的科学假设？</p><p>有，比较好。 1.实验效果比较solid:在各大主流目标检测方法上，提升了6-8%左右(LEVEL 2)。2.可视化结果也很好地验证科学猜想。</p><h2 id="q9">Q9</h2><p>这篇论文到底有什么贡献？</p><p>同时考虑多模态融合与数据增强之间的协同作用，并且从原始数据和网络结构两个层面，提出简单有效的方法，较好地解决冲突并验证猜想。Motivation有新意，实验效果比较solid, 通用价值大。</p><h2 id="q10">Q10</h2><p>下一步呢？有什么工作可以继续深入？</p><p>\1. 对于不可逆的数据增强操作，比如用于图像的RandomCrop,RandomErasing,必然造成图像与点云在几何层面无法对应。原文中只能忽略这些增强操作，或者等对齐模块后单独加入不可逆的数据增强操作。</p><p>\2.如实例分割、全景分割任务中，有一些对于实例的数据增强操作，如从其他场景中随机复制一些实例点到另外的场景，那么图像中是否需要对应生成一些假的物体pixel呢？若存在遮挡关系又如何处理？</p><p>\3. 方法相对简单一些，后续工作或许能继续深入挖掘。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>多模态融合</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CVPR2022</tag>
      
      <tag>论文十问系列</tag>
      
      <tag>多模态，点云目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一句话读论文:Panoptic Neural Fields(CVPR2022)</title>
    <link href="/posts/9cb0ed26/"/>
    <url>/posts/9cb0ed26/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="一句话读论文panoptic-neural-fieldscvpr-2022">一句话读论文：PanopticNeural Fields(CVPR 2022)</h1><blockquote><p>论文标题：Panoptic Neural Fields: A Semantic Object-Aware NeuralScene Representation(CVPR 2022)<br> 论文地址：<ahref="https://arxiv.org/abs/2205.04334">https://arxiv.org/abs/2205.04334</a><br>作者单位：Google Research, Georgia Tech, Simon Fraser University,Stanford University<br> 代码地址：暂无<br> 一句话读论文：“We presentPanoptic Neural Fields(PMF), and object-aware neural scenerepresentation that decomposes a scene into a set of objects(things) andbackground(stuff).”</p></blockquote><p>To Xicc,</p><p>​ 封面图感谢Xicc的无私分享！！！此生无悔入四月。</p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="//music.163.com/outchain/player?type=2&amp;id=30394711&amp;auto=0&amp;height=66"></iframe><h2 id="网络框架"><strong>网络框架：</strong></h2><figure><img src="https://s2.loli.net/2022/05/23/AiBE8Xg3cu5ql1w.png"alt="image-20220523191212464" /><figcaption aria-hidden="true">image-20220523191212464</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图1 Overview of Panoptic Neural Field</center><figure><img src="https://s2.loli.net/2022/05/23/3rnZVSYfXNpMeib.png"alt="image-20220523191253241" /><figcaption aria-hidden="true">image-20220523191253241</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图2 Dynamic Challenging 3D Scenes Description</center><h2 id="核心内容"><strong>核心内容：</strong></h2><p>大佬们请收下我的膝盖！！！</p><p>Motivation:</p><ul><li><p>把多用于室内场景的Nerf首次应用到室外自动驾驶场景中</p></li><li><p>Nerf原先多用于View Synthesis，图形学渲染，重建。本篇工作窥得大佬们的野心：想在室外自动驾驶场景中，把分类、语义分割、目标检测、目标追踪、全景分割、三维重建、深度估计、场景编辑与生成等一系列任务全部做到SOTA指标，从而让Nerf一统2D-3D视觉任务的天下。虽然本篇工作是初步的尝试，但是开辟了一个新的研究领域。</p></li><li><p>相比于之前在Nerf基础上各种incremental类型的工作，本篇工作提出一种室外场景通用类型的Nerf框架。主要分为stuff类别和thing类别，除了分别学习传统Nerf模型所需要的color,pose，density等信息，还加入语义信息，最后将stuff类别与thing类别共同合成panopticradiance field，用于各类下游任务。</p></li><li><p>在已有的语义分支+Nerf,Dynamics+Nerf等各种变体基础上，取消了共享的MLP网络，而是为每一种类别的物体instance设计小的MLP网络；此外在初始化上引入类别的先验信息，设计了category-specificmeta-learned initialization</p><p>本文的方法：在原版Nerf基础上，做如下变化</p></li><li><p>Things类别：</p><ul><li>首先用RGB-only 3D Object Detector&amp; Tracker 得到Bounding boxtrack <spanclass="math display"><em>T</em><sub><em>k</em></sub></span>(由一系列仿射变换矩阵组成)和语义类别<spanclass="math display"><em>k</em></span>.</li><li>对每个物体实例，用标准的Nerf网络提取特征，该网络是由time-invariantMLP组成(不是随时间变化而变化的RNN时序网络),得到包括color, pose,density等参数信息</li><li>损失函数共同优化Nerf网络和<spanclass="math display"><em>T</em><sub><em>k</em></sub></span></li></ul></li><li><p>Stuff 类别：</p><ul><li>用单一的Nerf网络提取Stuff类别，此外还有网络分支学习每个Stuffpixel的语义类别</li></ul></li><li><p>Panoptic-Radiance Field</p><ul><li><p>对color,densiy等通道采取如下融合方式</p></li><li><p><spanclass="math display"><em>c</em>(<em>x</em>|<em>θ</em>) = 𝟙<em>s</em>(<em>x</em>)<em>c</em><sub><em>x</em></sub>(<em>x</em>|<em>θ</em>) + ∑<sub><em>k</em></sub><em>c</em><sub><em>k</em></sub>(<em>T</em><sup>−1</sup><em>x</em>|<em>θ</em>)</span></p></li></ul></li><li><p>Render Panoptic-Radiance Fields</p><ul><li><span class="math display">$$  C(r|\theta) \sim \sum_{i=1}^Nw(t_i)f(\mathbf{r}(t_i)|\theta)$$</span></li></ul></li><li><p>Nerf中权重先验的获取</p><ul><li>Bias initalization(设置stuff MLP的bias为-5，thingMLP的bias为0.1，因为真实室外场景中stuff volume大多数是空的，而thingvolume大多数非空) 和 Meta-learn的方式(<code>FedAvg</code> 算法)</li></ul></li></ul><h2 id="贡献点创新性"><strong>贡献点/创新性：</strong></h2><ul><li>见Motivation的第1-3条</li></ul><h2 id="实验结果"><strong>实验结果：</strong></h2><figure><img src="https://s2.loli.net/2022/05/23/xYOCDHST3kIguh9.png"alt="image-20220523194617216" /><figcaption aria-hidden="true">image-20220523194617216</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图5 实验结果1</center><figure><img src="https://s2.loli.net/2022/05/23/sowYT8tfqBILU1b.png"alt="image-20220523194641296" /><figcaption aria-hidden="true">image-20220523194641296</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图6 实验结果2</center><figure><img src="https://s2.loli.net/2022/05/23/gkLWepDuo41ZQmO.png"alt="image-20220523194752533" /><figcaption aria-hidden="true">image-20220523194752533</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图7 实验结果3</center><figure><img src="https://s2.loli.net/2022/05/23/xG91WXNegAr2Pj7.png"alt="image-20220523194819545" /><figcaption aria-hidden="true">image-20220523194819545</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图8 实验结果4</center><figure><img src="https://s2.loli.net/2022/05/23/ZLzw3K8InV62dcv.png"alt="image-20220523194847881" /><figcaption aria-hidden="true">image-20220523194847881</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图9 实验结果5</center><h2 id="related-work可选-后续再补充"><strong>Related Work(可选,后续再补充):</strong></h2><ul><li>Nerfs</li><li>Nerfs with Semantics</li><li>Nerfs with dynamics</li><li>Nerfs with object decompositions</li><li>Conditional NeRFs</li><li>MVS</li><li>SLAM</li></ul><h2id="你认为优点不足可以拓展改进的地方可选"><strong>你认为优点/不足/可以拓展改进的地方(可选):</strong></h2><p>优点：</p><ul><li>太多了吐槽不完</li></ul><p>缺点：</p><ul><li>虽然很大一统，但是整个框架挺复杂，目前只能在离线的训练和推理，不太容易直接应用在实时场景下。</li></ul><h2 id="其他笔记"><strong>其他笔记:</strong></h2><ul><li>CVer 计算机视觉：https://zhuanlan.zhihu.com/p/513499887</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>全景分割</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>一句话读论文系列</tag>
      
      <tag>CVPR2022</tag>
      
      <tag>点云全景分割</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：矩阵理论(34)——盖尔圆定理与特征值估计</title>
    <link href="/posts/c6e1c86a/"/>
    <url>/posts/c6e1c86a/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记矩阵理论34盖尔圆定理与特征值估计">课程笔记：矩阵理论(34)——盖尔圆定理与特征值估计</h1><p>Note: 中间欠缺了很多部分，有空慢慢填坑。</p><h2 id="矩阵的盖尔圆盘">矩阵的盖尔圆盘</h2><p><span class="math display"><em>N</em> × <em>M</em></span>阶矩阵，每一行可以定义一个盖尔圆盘，每一列也可以定义一个盖尔圆盘。这里先讨论行的情况(关于行的盖尔圆盘)。<spanclass="math display"><em>D</em><sub><em>i</em></sub>(<em>A</em>) = {<strong>x</strong> <strong>∈</strong> ℂ <strong>:</strong> <strong>|</strong><strong>x</strong> <strong>−</strong> <strong>a</strong><sub><strong>i</strong><strong>i</strong></sub><strong>|</strong> <strong>≤</strong> <strong>∑</strong><sub><strong>i</strong> <strong>≠</strong> <strong>j</strong></sub><strong>|</strong><strong>a</strong><sub><strong>i</strong><strong>j</strong></sub><strong>|</strong>}</span>圆的半径是除对角元素外所有元素的<strong>模</strong>的和。圆心是对角线元素(由于是复数，在复平面上有实轴和虚轴坐标)。</p><p>这些圆盘区域的并能够得到盖尔区域。</p><h2 id="盖尔圆盘定理">盖尔圆盘定理</h2><p>盖尔圆盘定理: 设 <span class="math display"><em>A</em></span>是n阶矩阵，矩阵的所有特征值一定会落在某个圆盘里。 <spanclass="math display">$$|\lambda_i - a_{ii}| \le \sum_{i \ne j}^n |a_{ij}|, \exists i \in\{1,2,\dots, n \}$$</span></p><p>Proof: <span class="math display">$$\begin{align*}&amp;\mathbf{A}x = \lambda x \\\Leftrightarrow &amp;(\mathbf{A} - \lambda I)x = 0 \\\Leftrightarrow &amp;\sum_{i \ne j} a_{ij}x_j + (a_{ii} - \lambda)x_i =0 \, , i=1,2,\dots,n\\\Leftrightarrow&amp; |\lambda - a_{ii}||x_i| = |\sum_{i \ne j}a_{ij}x_j|\le \sum_{i \ne j}|a_{ij}||x_j| \\\Leftrightarrow &amp; |\lambda - a_{ii}| \le \sum_{i \ne j}|a_{ij}|\frac{|x_j|}{|x_{max}|} \le \sum_{i \ne j}|a_{ij}| \\\end{align*}$$</span> 从第四行，挑选特征向量最大的分量 <spanclass="math display"><em>x</em><sub><em>m</em><em>a</em><em>x</em></sub></span>，并把该分量index作为选取的矩阵的行。也就是说特征值满足的不等式，是在特征向量中分量最大的位置对应的行。</p><Details><summary>例3.4.4</summary><img src="https://i.loli.net/2021/11/19/7Ms9ftkrDhiBCzU.png" alt="image-20211119213910167" style="zoom:80%;" /></details><div class="note note-warning">            <p>注意：有的圆盘可能有多个特征值，有的圆盘可能没有特征值。如果需要精细的描述，使用精细圆盘定理，一个联通区域有几块圆盘，那么该区域必须<strong>恰好</strong>有相同个数的特征值</p>          </div><details><summary>例3.4.5</summary><img src="https://i.loli.net/2021/11/19/3CJzTgvXesfNMFQ.png" alt="image-20211119221047116" style="zoom:50%;" /></details><p>Note :圆盘定理不能保证每个圆盘一定有特征值。但是下面精细圆盘定理更详细的说明了特征值分布特点：K个圆盘的并集联通区域一定恰好有K个特征值。</p><h2 id="精细圆盘定理">精细圆盘定理</h2><p>精细圆盘定理：设 <span class="math display"><em>C</em></span>是盖尔区域的一个由<spanclass="math display"><em>k</em></span>个圆盘组成的连通分量，则<spanclass="math display"><em>C</em></span>恰好有<spanclass="math display"><em>k</em></span>个特征值。</p><p>证明思路: 设矩阵<spanclass="math display"><strong>A</strong> = <strong>D</strong> + <em>ϵ</em><strong>B</strong></span>,其中 <span class="math display"><strong>D</strong></span>是对角阵，<span class="math display"><strong>B</strong></span>是对角线为0的剩余部分。<spanclass="math display"><em>ϵ</em> ∈ [0, 1]</span> ,这样矩阵 <spanclass="math display"><strong>A</strong></span>从对角阵连续变化到原矩阵。对角阵的圆盘半径为0，圆心就是特征值，因此每个圆盘恰好有1个特征值，圆盘外面有<span class="math display"><em>n</em> − 1</span> 个特征值。在 <spanclass="math display"><em>ϵ</em></span>连续变化增长到1的过程中，圆盘半径逐渐变大，并且部分圆开始相交，<spanclass="math display"><em>k</em></span> 个圆盘相交，<spanclass="math display"><em>k</em></span> 个圆盘区域外有 <spanclass="math display"><em>n</em> − <em>k</em></span>个特征值。由于矩阵的特征多项式的根是其系数的连续函数，随意特征值也是从圆心出发连续游走的，不会出现跳跃的情况，且增长过程中一直被圆盘包着，所以保证了联通的k个盖尔圆一定恰好有<span class="math display"><em>k</em></span> 个特征值。</p><h2 id="调节圆盘大小的技巧">调节圆盘大小的技巧</h2><p>如何调整矩阵盖尔圆的半径，从而将特征值分离出来(比喻：就好像化学试剂提纯，分离似的)</p><p>原理如下： <span class="math display">$$\begin{align*}\mathbf{D}^{-1}\mathbf{A}\mathbf{D} &amp;= \text{Diag}(\frac{1}{d_1},\frac{1}{d_2}, \dots, \frac{1}{d_n}) \mathbf{A} \text{Diag}(d_1, d_2,\dots, d_n) \\&amp;= (\frac{d_j}{d_i}a_{ij})\end{align*}$$</span></p><p>首先看出对角线元素是不受影响的，非对角先元素会被放缩，放缩系数由该元素的行与列(对应的是其他行)的系数决定</p><p>某一行的放缩系数 <spanclass="math display"><em>d</em><sub><em>i</em></sub></span> 和其余行系数<span class="math display"><em>d</em><sub><em>k</em></sub></span>的关系如下：</p><p>若 <spanclass="math display"><em>d</em><sub><em>i</em></sub> &gt; <em>d</em><sub><em>k</em></sub></span>，为了简便起见，我们设除了第 <spanclass="math display"><em>i</em></span> 行的其他行均为1。第 <spanclass="math display"><em>i</em></span>行的非对角元素放缩系数分母比分子大，故圆盘被缩小；其他行的第 <spanclass="math display"><em>i</em></span>列元素分子比分母大，会被放大，除第 <spanclass="math display"><em>i</em></span>列外的其他非对角元素不变，总体圆盘会被放大。</p><p>若 <spanclass="math display"><em>d</em><sub><em>i</em></sub> &lt; <em>d</em><sub><em>k</em></sub></span>，类似的，设置除了第<span class="math display"><em>i</em></span> 行的其他行均为1。只有第<span class="math display"><em>i</em></span>行的圆盘会被放大，其余圆盘均缩小。</p><h2 id="利用圆盘定理估计谱半径">利用圆盘定理估计谱半径</h2><p>矩阵的谱半径 <span class="math display">$$\le \underset{i}{\max}\sum_{j} a_{ij}$$</span> , 并记录 <span class="math display">$$v=  \underset{i}{\max} \sum_{j} a_{ij}$$</span></p><p>证明思路:因为根据圆盘定理，矩阵的每个特征值都一定落在某个圆盘(比如<spanclass="math display"><em>a</em><sub><em>i</em><em>i</em></sub></span>)里。<spanclass="math display">|<em>λ</em>| = |<em>λ</em> − <em>a</em><sub><em>i</em><em>i</em></sub> + <em>a</em><sub><em>i</em><em>i</em></sub>| ≤ ∑<sub><em>j</em></sub><em>a</em><sub><em>i</em><em>j</em></sub></span>同理，矩阵的谱半径小于某个“最大”的列 <span class="math display">$$v’ =\underset{j}{\max} \sum_{i} a_{ij}$$</span></p><h2 id="其他估计">其他估计</h2><ol type="1"><li><p>Ostrwoski 圆盘定理</p></li><li><p>Brauer定理(Cassini卵形)</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>特征值</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：统计学习理论与方法(ELS_Chap7)</title>
    <link href="/posts/3c8c02ec/"/>
    <url>/posts/3c8c02ec/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记统计学习理论与方法els_chap7">课程笔记：统计学习理论与方法（ELS_Chap7）</h1><p>📚书籍：《Elements of Statistical Learning》Chap 7</p><h2 id="两个概念">两个概念</h2><ol type="1"><li>模型选择(Model Selection):模型由参数控制的，模型选择既包括模型类型选择，也包括参数的控制。后面讲贝叶斯信息准则时(BIC)会讲到。</li><li>模型评估(Model Assessment): 给定一个模型，评估其训练误差(In-sampleError)，测试误差(Extra-sample Error)，泛化误差(GeneralizationError)。评估训练误差与测试误差之间的差距Bound.</li></ol><p>两者都是侧重对模型的泛化误差进行评估，而非模型的训练误差。泛化误差是整个统计学习中最核心的关注问题。</p><h2 id="训练误差与泛化误差">训练误差与泛化误差</h2><p>训练误差： <span class="math display">$$\overline{\text{err}} = \frac{1}{N} \sum_{i=1}^N L(y_i, \hat{f}(x_i))$$</span> 泛化误差： <spanclass="math display">Err(<em>y</em><sub><em>i</em></sub>, <em>f̂</em>(<em>x</em><sub><em>i</em></sub>)) = <em>E</em><sub>(<em>X</em>, <em>Y</em>) ∼ <em>τ</em></sub>(<em>L</em>(<em>y</em><sub><em>i</em></sub>, <em>f̂</em>(<em>x</em><sub><em>i</em></sub>))|<em>τ</em>)</span>其中损失函数通常是:</p><ul><li>均方误差<spanclass="math display"><em>L</em> = (<em>y</em><sub><em>i</em></sub> − <em>f̂</em>(<em>x</em><sub><em>i</em></sub>))<sup>2</sup></span></li><li>对数极大似然函数【用于多分类】(严格来说这不算损失函数，但是极大似然最大等价于加符号最小，所以可以写成相同形式):<span class="math display">$$L = -2\cdot[\sum_{k=1}^KI(G=k)\log(\hat{\Pr}(\hat{G}=k)] $$</span>.其中<spanclass="math display"><em>I</em>(<em>G</em> = <em>k</em>)</span>相当于真实标签(GroundTruth).</li><li>对数极大似然的特殊情况【0-1分类】： <span class="math display">$$L =I(G=1)\log(\hat{\Pr}) + (1-I(G=1))\log(1-\hat{\Pr})$$</span></li></ul><p class="note note-primary">📢注意，我们算误差时的Ground Truth(GT) <spanclass="math display"><em>y</em></span>并不一定是真实的标签，因为获取数据的过程中无法避免有噪声存在(就比如用精密物理仪器测量的结果总会有无法避免的系统误差)。我们的损失函数是让模型的估计值<spanclass="math display"><em>f̂</em></span>和GT算误差，不是和<spanclass="math display"><em>f</em>(<em>x</em>)</span>算误差。</p><h2 id="bias-variance-分解">Bias-variance 分解</h2><p>假设：<spanclass="math display">{(<em>x</em><sub><em>i</em></sub>, <em>y</em><sub><em>i</em></sub>)}</span>是从某个数据分布中采样得到的某个数据集<spanclass="math display"><em>τ</em></span>。数据的真实分布是<spanclass="math display"><em>y</em> = <em>f</em>(<em>x</em>) + <em>ϵ</em></span>，数据的真实标签是<spanclass="math display"><em>f</em>(<em>x</em>)</span>，但是我们获取数据集时总会有噪声<spanclass="math display"><em>ϵ</em></span>干扰。通常假设噪声服从均值为0的某个高斯分布。<spanclass="math display"><em>ϵ</em> ∼ <em>N</em>(0, <em>σ</em><sub><em>ϵ</em></sub><sup>2</sup>)</span></p><p>考察在<spanclass="math display"><em>X</em> = <em>x</em><sub>0</sub></span>一点上的单点泛化误差，其中求期望操作<spanclass="math display"><em>E</em></span>是对所有的数据集<spanclass="math display"><em>τ</em></span>求的期望(Expectation over all thedataset <spanclass="math display"><em>τ</em></span>).<a name="t1">也就是说在这里，<spanclass="math display"><em>f̂</em>(<em>x</em><sub>0</sub>)</span>也是随机变量，它的随机性来源于可能选取不同的数据集<spanclass="math display"><em>τ</em></span> ；而<spanclass="math display"><em>f</em>(<em>x</em><sub>0</sub>)</span>的随机性只来源于<spanclass="math display"><em>ϵ</em></span>，二者独立无关联</a>。</p><p><span class="math display">$$\begin{align}E(L(Y, \hat{f}(X))|X=x_0, Y=y_0 )&amp;= (y_0 - f(x_0))\,\,\,\text{(这里采用均方误差分析，其他同理)}\\&amp;= E(f(x_0) - \hat{f}(x_0) + \epsilon)^2 \\&amp;= E(f(x_0) - \hat{f}(x_0))^2 + E(\epsilon)^2 + 2E(f(x_0) -\hat{f}(x_0))E(\epsilon)\,\,\,\text{(} \epsilon \text{与} f(x_0) -\hat{f(x_0)} \text{独立)} \\&amp;=  E(f(x_0) - \hat{f}(x_0))^2 + \sigma_{\epsilon}^2 + 0 \\&amp;= E(f(x_0) - E\hat{f}(x_0) + E\hat{f}(x_0) - \hat{f}(x_0))^2 +\sigma_{\epsilon}^2 \\&amp;= E(f(x_0) - E\hat{f}(x_0))^2 + E(\hat{f}(x_0) - \hat{f}(x_0))^2 +2E(f(x_0) - E\hat{f}(x_0))\cdot 0 + \sigma_{\epsilon}^2\\&amp;= E(f(x_0) - E\hat{f}(x_0))^2 + E(\hat{f}(x_0) - \hat{f}(x_0))^2  +\sigma_{\epsilon}^2 \\&amp;= \text{bias}^2 + \text{variance} + \sigma_{\epsilon}^2\end{align}$$</span></p><div class="note note-info">            <p>关于期望与方差的常用性质，请参见这篇👉<strong><ahref="https://www.oier99.cn/posts/f0d58712/">博客</a></strong>。</p>          </div><h3 id="knn的例子">KNN的例子</h3><p>KNN的模型是<span class="math display">$$\hat{f}(x_0) =\frac{1}{K}\sum_{i=1}^K y_i,\, x_i \in n_K(x_0)$$</span>. <spanclass="math display">$$\begin{align*}\hat{f}(x_0) &amp;= \frac{1}{K}\sum_{i=1}^K y_i \\&amp;= \frac{1}{K} \sum_{i=1}^K(f(x_i) + \epsilon_i ) \\&amp;\approx \frac{1}{K} \sum_{i=1}^K(f(x_0) + \epsilon_i ) \\&amp;= f(x_0) + \frac{1}{K} \sum_{i=1}^K \epsilon_i\end{align*}$$</span> 为了计算方差方便，在理论推导这里做了一个重要近似：因为KNN取的是K个近邻点来插值估计，所以假设认为他们的“本源”<spanclass="math display"><em>f</em>(<em>x</em><sub><em>i</em></sub>) ≈ <em>f</em>(<em>x</em><sub>0</sub>)</span>.这样，KNN的方差可以如下计算:<span class="math display">$$\begin{align*}\text{Var}(\hat{f}(x_0)) &amp;= \text{Var} \frac{1}{K} \sum_{i=1}^K\epsilon_i \\&amp;= \frac{1}{K^2} \text{Var}\sum_{i=1}^K \epsilon_i \\&amp;= \frac{1}{K^2} \sum_{i=1}^K \text{Var}(\epsilon_i) \\&amp;= \frac{1}{K^2} \sum_{i=1}^K \sigma_{\epsilon}^2 \\&amp;= \frac{1}{K} \sigma_{\epsilon}^2\end{align*}$$</span> 故泛化误差拆分为: <span class="math display">$$Err(y_0, \hat{f}(x_0)) \approx \sigma_{\epsilon}^2 + E(f(x_0) -\frac{1}{K}\sum_{i=1}^K y_i )^2 + \frac{1}{K} \sigma_{\epsilon}^2$$</span> 可以看出<spanclass="math display"><em>K</em></span>越小，模型复杂度越大，方差越大，bias应该会越小。</p><h3 id="线性回归的例子">线性回归的例子</h3><p>对于线性回归函数 <spanclass="math display"><em>f̂</em>(<em>x</em>) = <em>β̂</em><sup><em>T</em></sup><em>x</em></span>, 解最小二乘误差下的最佳近似参数是<spanclass="math display"><em>β̂</em> = (<em>X</em><sup><em>T</em></sup><em>X</em>)<sup>−1</sup><em>X</em><sup><em>T</em></sup><em>y</em></span></p><p>故<spanclass="math display"><em>f̂</em>(<em>x</em>) = <em>y</em><sup><em>T</em></sup><em>X</em>(<em>X</em><sup><em>T</em></sup><em>X</em>)<sup>−1</sup><em>x</em> = <em>x</em><sup><em>T</em></sup><em>X</em>(<em>X</em><sup><em>T</em></sup><em>X</em>)<sup>−</sup>1<em>X</em><sup><em>T</em></sup><em>y</em> = <em>h</em>(<em>x</em>)<em>y</em></span>.可以看出<spanclass="math display"><em>f̂</em>(<em>x</em>)</span>的随机性来源于y，即来源于<spanclass="math display"><em>ϵ</em></span> .（这与<ahref="#t1">刚才</a>说法不矛盾。因为之前对<spanclass="math display"><em>f̂</em></span>的分析是抽象的符号，而这里是对线性回归具体公式分析）<spanclass="math display">var(<em>f̂</em>(<em>x</em><sub>0</sub>)) = ||<em>h</em>(<em>x</em><sub>0</sub>)||<sup>2</sup><em>σ</em><sub><em>ϵ</em></sub><sup>2</sup></span>故 <spanclass="math display"><em>E</em><em>r</em><em>r</em>(<em>y</em><sub>0</sub>, <em>f̂</em>(<em>x</em><sub>0</sub>)) = <em>σ</em><sub><em>ϵ</em></sub><sup>2</sup> + <em>E</em>(<em>f</em>(<em>x</em><sub>0</sub>) − <em>β̂</em><sup><em>T</em></sup><em>x</em><sub>0</sub>)<sup>2</sup> + ||<em>h</em>(<em>x</em><sub>0</sub>)||<sup>2</sup><em>σ</em><sub><em>ϵ</em></sub><sup>2</sup></span></p><p>训练误差: <span class="math display">$$\frac{1}{N}\sum_{i=1}^N Err(y_0, \hat{f}(x_0)) = \sigma_{\epsilon}^2 +\sum_{i=1}^NE(f(x_0) - \hat{\beta}^Tx_0 )^2 +\frac{p}{N}\sigma_{\epsilon}^2$$</span> 可以看出模型复杂度由数据量<spanclass="math display"><em>N</em></span>和参数量<spanclass="math display"><em>p</em></span>共同控制。</p><p class="note note-primary">这部分的推导需要用到矩阵的迹的性质：</br> <spanclass="math display">tr(<em>A</em><em>B</em><em>C</em>) = tr(<em>B</em><em>C</em><em>A</em>) = tr(<em>C</em><em>A</em><em>B</em>)</span></br><spanclass="math display">∑<sub><em>i</em></sub>(<em>x</em><sub><em>i</em></sub><em>y</em><sub><em>i</em></sub>) = <em>x</em><sup><em>T</em></sup><em>y</em> = tr(<em>x</em><em>y</em><sup><em>T</em></sup>)</span></p><h2 id="乐观度">乐观度</h2><p>乐观度(Optimisim)的概念主要是为了比较模型训练误差与泛化误差之间的差距，或者如何用训练误差去估计泛化误差。</p><p>直接计算训练误差和泛化误差的差有困难，因为模型输入<spanclass="math display"><em>x</em></span>都不固定。所以可以采用重采样的技术，原先训练集<spanclass="math display">(<em>x</em>, <em>y</em>)</span>,是训练时采样得到的标签，记为<spanclass="math display"><em>y</em></span>；重采样是对相同的<spanclass="math display"><em>x</em></span>, 再次采样<spanclass="math display"><em>y</em><sup>New</sup></span>。 <spanclass="math display">$$\text{Err}_{in}(y_i, \hat{f}(x_i)) = \frac{1}{N}\sum_{i=1}^NE_{Y^{\text{new}}}[L(Y_i^{\text{New}}, \hat{f}(x_i))]$$</span></p><p><span class="math display">$$\overline{err} = \frac{1}{N} \sum_{i=1}^N L(y, \hat{f}(x_i))$$</span></p><p>结论 <span class="math display">$$\begin{align*}    w &amp;= E_y(op) \\    &amp;= \frac{2}{N}\text{Cov}(y, \hat{y})\end{align*}$$</span> 证明过程后续再补。</p><p>若<span class="math display"><em>ŷ</em></span>是由参数量为<spanclass="math display"><em>d</em></span>线性回归得到的估计，那么 <spanclass="math display">Cov(<em>y</em>, <em>ŷ</em>) = <em>d</em><em>σ</em><sub><em>ϵ</em></sub><sup>2</sup></span>可得到 <span class="math display">$$E_y(\text{Err}_{in}) = E_y(\overline{\text{err}}) + 2\cdot\frac{d}{N}\sigma_{\epsilon}^2$$</span></p><h2 id="有效参数量">有效参数量</h2><p><spanclass="math display"><em>ŷ</em> = <em>S</em><em>y</em></span></p><p>有效参数数量(Effective Number of Parameters): <spanclass="math display">df(<em>S</em>) = tr(<em>S</em>)</span>对于线性回归，有如下关系： <span class="math display">$$\sum_{i=1} ^N \text{Cov}(y_i, \hat{y}_i) = \text{df}(\hat{y})\cdot  \sigma_{\epsilon}^2$$</span></p><h2 id="贝叶斯信息量bic">贝叶斯信息量（BIC）</h2><p>题外话：前面的部分主要讲述的模型评价(ModelAssesment)部分，贝叶斯信息量主要讲述的是模型选择的部分</p><p>BIC for 极大似然回归 <spanclass="math display"><em>B</em><em>I</em><em>C</em> = −2 ⋅ <em>l</em><em>o</em><em>g</em><em>l</em><em>i</em><em>k</em> + (log <em>N</em>) ⋅ <em>d</em></span>BIC under Gaussian Model <span class="math display">$$BIC = \frac{N}{\sigma_{\epsilon}^2}[\overline{\text{err}}+ (\log N)\cdot\frac{d}{N} \sigma_{\epsilon}^2]$$</span>他的Motivation是源自贝叶斯派的思想(下面手抄草稿，由于过程有点难，整理后还有很多错误比如符号上下标对不上)</p><figure><img src="https://i.loli.net/2021/11/10/9qVUZlvD2MgstSo.jpg" alt="1" /><figcaption aria-hidden="true">1</figcaption></figure><figure><img src="https://i.loli.net/2021/11/10/yYAO547q1Nf39cM.jpg" alt="2" /><figcaption aria-hidden="true">2</figcaption></figure><figure><img src="https://i.loli.net/2021/11/10/uskaSvyOilJT8mU.jpg" alt="3" /><figcaption aria-hidden="true">3</figcaption></figure><h2 id="交叉验证以后有空再补">交叉验证(以后有空再补)</h2>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>统计学习理论与方法</category>
      
      <category>教材笔记</category>
      
      <category>Elements of Statistical Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>教材笔记</tag>
      
      <tag>统计学习理论与方法</tag>
      
      <tag>ELS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔：期望与方差的性质</title>
    <link href="/posts/f0d58712/"/>
    <url>/posts/f0d58712/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="随笔期望与方差的常用性质">随笔：期望与方差的常用性质</h1><h2 id="数学期望的性质">数学期望的性质</h2><h2 id="方差的性质">方差的性质</h2><ul><li><p>常数的方差为0：</p><p><span class="math display">Var(<em>C</em>) = 0</span></p><p>逆命题<spanclass="math display">Var(<em>X</em>) = 0</span>的充要条件是<spanclass="math display"><em>X</em></span>以概率1取常数<spanclass="math display">Pr <em>X</em> = <em>C</em> = 1</span></p></li><li><p><spanclass="math display">Var(<em>C</em><em>X</em>) = <em>C</em><sup>2</sup>Var(<em>X</em>)</span></p></li><li><p>若随机变量<spanclass="math display"><em>X</em>, <em>Y</em></span><strong>相互独立</strong>，而且<spanclass="math display">Var(<em>X</em>), Var(<em>Y</em>)</span>存在, <spanclass="math display"><em>C</em><sub>1</sub>，<em>C</em><sub>2</sub></span>是常数，，则有<spanclass="math display">Var(<em>C</em><sub>1</sub><em>X</em> + <em>C</em><sub>2</sub><em>Y</em>) = <em>C</em><sub>1</sub><sup>2</sup>Var(<em>X</em>) + <em>C</em><sub>2</sub><sup>2</sup>Var(<em>Y</em>)</span></p></li></ul><h2 id="期望的性质">期望的性质</h2><ul><li>常数的期望是本身： <spanclass="math display"><em>E</em>(<em>C</em>) = 0</span></li><li><strong>任意</strong>两个随机变量<spanclass="math display"><em>X</em>, <em>Y</em></span>和常数<spanclass="math display"><em>C</em><sub>1</sub>, <em>C</em><sub>2</sub></span>有<spanclass="math display"><em>E</em>(<em>C</em><sub>1</sub><em>X</em> + <em>C</em><sub>2</sub><em>Y</em>) = <em>C</em><sub>1</sub><em>E</em>(<em>X</em>) + <em>C</em><sub>2</sub><em>E</em>(<em>Y</em>)</span></li><li>若随机变量<spanclass="math display"><em>X</em>, <em>Y</em></span><strong>相互独立</strong>，则有<spanclass="math display"><em>E</em>(<em>X</em><em>Y</em>) = <em>E</em>(<em>X</em>)<em>E</em>(<em>Y</em>)</span></li><li>Note: <spanclass="math display"><em>E</em>(<em>X</em><sup>2</sup>)</span>和<spanclass="math display"><em>E</em><sup>2</sup>(<em>X</em>)</span>不是一回事</li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>概率统计</tag>
      
      <tag>期望方差</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔：每日bug系列（2021-11-05）</title>
    <link href="/posts/c0437afa/"/>
    <url>/posts/c0437afa/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="随笔-每日bug系列">随笔-每日bug系列</h1><h2 id="section">2021-11-05</h2><h3 id="bug-1">Bug 1</h3><p>计算矩阵特征值，求行列式<spanclass="math display">|<em>λ</em><em>I</em><sub><em>n</em></sub> − <em>A</em>|</span>时，一定要写成$$ $$的形式，否则展开求解时极容易出错(今天计算行列式看着原矩阵脑补写特征多项式一连写错五次的痛，对自己的算术能力表示深刻怀疑)</p><h3 id="bug-2">Bug 2</h3><p>记住 <spanclass="math display"><em>A</em> + 2 = <em>A</em> + 2<em>I</em><sub><em>n</em></sub></span>，是加在对角线元素上的，不是每个位置都加2。这个惯性错误犯了好多次了！！</p><h3 id="bug-3">Bug 3</h3><p>相似矩阵有相同特征值，但逆命题不成立(即有相同特征值，包括每个特征值的代数重数也相同的两个矩阵不一定相似)。因为可能有不同的Jordan标准型形式。比如一个矩阵可以对角化，相似于某个对角阵；而另一个矩阵无法对角化，只能相似于某个Jordan标准型，相同特征值个数的Jordan标准型又有很多种，所以导致这两个矩阵无法相似。</p><h2 id="section-1">2023-7-20</h2><h3 id="bug1">Bug1</h3><p>在安装一个Pyttorch项目的环境时，遇到了如下问题：</p><blockquote><p>TypeError: Descriptors cannot not be created directly. If this callcame from a _pb2.py file, your generated code is out of date and must beregenerated with protoc &gt;= 3.19.0. If you cannot immediatelyregenerate your protos, some other possible workarounds are:</p><ol type="1"><li>Downgrade the protobuf package to 3.20.x or lower.</li><li>Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will usepure-Python parsing and will be much slower).</li></ol></blockquote><p>参考如下(回答)[https://wenku.csdn.net/answer/dd136ae829e0919f8bdcdec4d407d728],初步判断是因为protoc版本无法兼容protobuf的版本，需要按照如上要求降低protobuf的版本到3.19.0较好。深层原因似乎是高版本的protobuf取消李直接构建描述子(descriptors)的代码规范。(具体也不太懂)</p><p>随后运行代码时指出来了段错误(SegmentationErrors)的报错，没有其他任何信息。这种错误一般都和c/c++编译相关，比如说CUDA编程时编译的Library库版本与新环境的GCC,CUDA-toolkit版本不一致的原因。我想到的比较好的调试思路就是逐行debug，看哪里出问题。后来发现在<code>import torch-scatter</code>,和<code>import torch-sparse</code>的地方出问题了。我就想到这两个包似乎都是需要有build的操作的。所以卸载了当下的这两个包，重新安装一遍就不会报这个错误了。</p><p>以后再出现类似错误的时候可以参考这个思路。</p><h3 id="bug2">Bug2</h3><p>解决<code>import error</code>的一些常见方法：</p><ul><li><p>添加到PYTHONPATH变量中，<code>export PYTHONPATH=XXXXX:$PYTHONPATH</code></p></li><li><p>在代码中添加路径，例如:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os.path <span class="hljs-keyword">as</span> osp<br><span class="hljs-keyword">import</span> sys<br>sys.path.append(osp.dirname(osp.dirname(osp.abspath(__file__))))<br></code></pre></td></tr></table></figure></li><li><p>如果使用VSCODE或PYCHARM这种IDE,可以选择右键单击文件夹，寻找设置为根目录(Set as SourceRoot)类似的命令，确定。应该这个配置会被写进VSCODE或PYCHARM的工程配置文件中(猜测)。</p></li></ul><h2 id="section-2">2023-09-08</h2><h3 id="oserror-errno-24-too-many-open-files">OSError: [Errno 24] Toomany open files</h3><p>同时还伴随着<code>RuntimeError: received 0 items of ancdata</code>是在dataloader加载数据时出现的错误，参考这篇博客：https://zhuanlan.zhihu.com/p/585186356</p><p>主要原因是文件打开太多或线程太多，超过系统最大数量限制(比如默认1024)。但是我遇到一个比较诡异的问题，同一个Pytorch程序在VSCode里能正常运行，但是在bashterminal里就报这个错误。所以我一开始觉得不太可能是文件打开太多(相同的worker_num情况下，难道VSCode打开的文件会变少？)但是后来，我采用他的第二条方法，<code>torch.multiprocessing.set_sharing_strategy('file_system')</code>，果然在bash里成功了。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
      <category>每日bug系列</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征值</tag>
      
      <tag>矩阵理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程专题：矩阵理论(a)——矩阵对角化的条件汇总</title>
    <link href="/posts/7e08a53a/"/>
    <url>/posts/7e08a53a/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程专题-矩阵理论之矩阵对角化的条件汇总">课程专题-矩阵理论之矩阵对角化的条件汇总</h1><p>题记：整个初等矩阵理论中要考虑最重要的事情，就是如何尽可能将矩阵化简、压缩。不论是化简为最普世的Jordan标准型，还是更为特殊的矩阵能化简为对角矩阵，以至到后续的各种矩阵分解，都是在研究这一问题。</p><h2 id="引入">引入</h2><ol type="1"><li>定义了特征值，特征向量，特征多项式，特征子空间，相似变换等概念(自我回顾一下)。一些比较重要的性质（常识）</li></ol><ul><li><spanclass="math display">tr(<em>A</em>) = ∑<em>λ</em><sub><em>i</em></sub></span>（包括多重次数的特征值），<spanclass="math display">|<em>A</em>| = <em>Π</em><em>λ</em><sub><em>i</em></sub></span></li><li>相似矩阵的特征值相同；但特征值相同不一定相似，因为两个矩阵特征值对应的特征子空间不一定一样。</li><li>属于不同特征值的特征向量必不可能相同;进一步，属于不同特征值的特征向量必线性无关。</li><li><span class="math display"><em>k</em><em>A</em></span> , <spanclass="math display"><em>A</em><sup><em>m</em></sup></span>, <spanclass="math display"><em>A</em><sup>−1</sup></span> (若<spanclass="math display"><em>A</em></span>可逆)特征值分别是 <spanclass="math display"><em>k</em><em>λ</em></span> , <spanclass="math display"><em>λ</em><sup><em>m</em></sup></span> , <spanclass="math display"><em>λ</em><sup>−1</sup></span></li><li>实矩阵的特征值/特征向量有可能是复数，因此特征子空间的数域一般定义在复数上</li><li>特征子空间：特征子空间本质上是 <spanclass="math display">(<em>λ</em><em>I</em> − <em>A</em>)<em>x</em> = 0</span>的解空间(除0以外)，若 <spanclass="math display"><em>x</em><sub>1</sub></span> 和 <spanclass="math display"><em>x</em><sub>2</sub></span> 都是 <spanclass="math display"><em>A</em></span> 的属于 <spanclass="math display"><em>λ</em></span> 的特征向量，则 <spanclass="math display"><em>k</em><sub>1</sub><em>x</em><sub>1</sub> + <em>k</em><sub>2</sub><em>x</em><sub>2</sub></span>(<span class="math display"> ≠ 0</span>)也是属于 <spanclass="math display"><em>λ</em></span> 的特征向量。</li></ul><ol start="2" type="1"><li>矩阵理论中为了更便捷地研究(玩耍)，引入了零化多项式和最小多项式的概念。</li></ol><h2 id="条件">条件</h2><ol type="1"><li><spanclass="math display"><em>n</em></span>阶矩阵可对角化的充分必要条件：<ul><li>矩阵有<spanclass="math display"><em>n</em></span>个线性无关的特征向量(原理：这<spanclass="math display"><em>n</em></span>个线性无关的特征向量正好构成了相似变换的可逆矩阵)</li><li>每个特征值的几何重数=代数重数(通常几何重数&lt;=代数重数)(原理：从Jordan标准型上直观的来解释一下，代数重数是Jordan标准型对角线上<spanclass="math display"><em>λ</em></span>总共出现的次数，几何重数(特征子空间的维数)是特征值为<span class="math display"><em>λ</em></span> 的Jordan块 <spanclass="math display"><em>J</em><sub><em>k</em></sub>(<em>λ</em>)</span>的块数，每一块对应一个特征子空间的基向量，块之间阶数不一样对应基向量之间是线性无关的。若代数重数=几何重数，则每个Jordan块大小为1，退化为对角矩阵)？</li></ul></li><li><spanclass="math display"><em>n</em></span>阶矩阵的最小多项式没有重根<ul><li>零化多项式：特征多项式f()肯定是满足 <spanclass="math display"><em>f</em>(<em>A</em>) = 0</span></li><li><span class="math display"><em>λ</em></span> 是<spanclass="math display"><em>A</em></span>的特征值充要条件：最小多项式 <spanclass="math display"><em>m</em>(<em>λ</em>) = 0</span></li><li>相似矩阵有相同的最小多项式</li><li>分块对角矩阵的最小多项式是各块最小多项式的最小公倍式(对照分块对角阵的特征多项式=各块最小多项式的乘积)</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
      <category>课程专题</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>矩阵对角化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：矩阵理论(1)——线性代数知识回顾(下)</title>
    <link href="/posts/7736990a/"/>
    <url>/posts/7736990a/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记矩阵理论1线性代数知识回顾下">课程笔记：矩阵理论(1)——线性代数知识回顾(下)</h1><p>📚书籍：《矩阵理论与应用》张跃辉 Chap 1</p><h2 id="相似矩阵">相似矩阵</h2><p><spanclass="math display"><strong>A</strong><em>x</em> = <em>λ</em><em>x</em></span></p><ol type="1"><li><span class="math display"><strong>A</strong></span>是方阵(所有有关特征值和特征响亮的讨论是对方阵而言的)</li><li><span class="math display"><em>x</em> ≠ 0</span></li></ol><p>则称<span class="math display"><em>x</em></span>是属于特征值 <spanclass="math display"><em>λ</em></span> 的 <spanclass="math display"><strong>A</strong></span> 上的特征向量</p><h3 id="特征多项式">特征多项式</h3><p><span class="math display">$$\begin{align}(\mathbf{A} - \lambda I)x &amp;= 0 \\\end{align}$$</span></p><p>由于<span class="math display"><em>x</em> ≠ 0</span>,若有满足该等式的<span class="math display"><em>λ</em></span>值，则必为降秩矩阵(奇异矩阵)，故记 <spanclass="math display"><em>f</em>(<em>λ</em>) = |<strong>A</strong> − <em>λ</em><strong>I</strong>| = 0</span>成为特征多项式。</p><p><strong>谱半径</strong>：数值(复数模长)最大的特征值 <spanclass="math display"><em>ρ</em>(<strong>A</strong>) = max {|<em>λ</em>| : <em>λ</em> ∈ <em>σ</em>(<strong>A</strong>)}</span></p><p>从几何上看，所有的特征值都落在以原点为圆心，谱半径 <spanclass="math display"><em>ρ</em>(<strong>A</strong>)</span>为半径的圆盘内</p><p><strong>矩阵的特征值 <spanclass="math display"><em>λ</em></span>的特征子空间</strong>：给定 <spanclass="math display"><em>λ</em></span>后 <spanclass="math display">(<strong>A</strong> − <em>λ</em><strong>I</strong>)<em>x</em> = 0</span>这个齐次方程的解空间，记为 <spanclass="math display"><em>V</em><sub><em>λ</em></sub></span>。该解空间的维度<spanclass="math display">dim (<em>V</em><sub><em>λ</em></sub>) = <em>n</em> − <em>r</em>(<strong>A</strong>)</span>称为特征值 <span class="math display"><em>λ</em></span>的<strong>几何重数</strong></p><p><strong>代数重数</strong>：将特征多项式因式分解后， <spanclass="math display"><em>f</em>(<em>λ</em>) = |<strong>A</strong> − <em>λ</em><strong>I</strong>| = ∏<sub><em>i</em></sub>(<em>λ</em> − <em>λ</em><sub><em>i</em></sub>)<sup><em>n</em><sub><em>i</em></sub></sup> = 0</span>，每个特征值的对应的<spanclass="math display"><em>n</em><sub><em>i</em></sub></span></p><p><strong><u><em>任何特征值的几何重数不会超过其代数重数</em></u></strong></p><h3 id="特征值的性质">特征值的性质</h3><ol type="1"><li><spanclass="math display">|<strong>A</strong>| = ∏<sub><em>i</em></sub>(<em>λ</em><sub><em>i</em></sub>)<sup><em>n</em><sub><em>i</em></sub></sup></span></li><li><spanclass="math display">tr(<strong>A</strong>) = ∑<sub><em>i</em></sub><em>n</em><sub><em>i</em></sub><em>λ</em><sub><em>i</em></sub></span></li><li><span class="math display"><strong>A</strong></span>可逆 <spanclass="math display">⇔</span> 0不是其特征值</li><li><span class="math display"><em>λ</em></span> 是 <spanclass="math display"><strong>A</strong></span> 的特征值，则 <spanclass="math display"><em>f</em>(<em>λ</em>)</span> 是 <spanclass="math display"><em>f</em>(<strong>A</strong>)</span>的特征值，特征向量不变</li><li>设 <span class="math display"><strong>A</strong></span>可逆，其特征多项式为 <spanclass="math display">|<strong>A</strong><sup>−1</sup> − <em>λ</em><strong>I</strong>| = ∏<sub><em>i</em></sub>(<em>λ</em> − <em>λ</em><sub><em>i</em></sub><sup>−1</sup>)<sup><em>n</em><sub><em>i</em></sub></sup></span>, <span class="math display"><em>λ</em><sup>−1</sup></span>也是特征值，而且对应特征向量不变</li><li><strong>相似矩阵具有相同的特征多项式，因此具有相同的特征值</strong></li></ol><h3 id="特征向量的性质">特征向量的性质</h3><ol type="1"><li>属于不同特征值的特征向量线性无关</li><li><span class="math display"><strong>A</strong></span> 可以对角化<span class="math display">⇔</span> <spanclass="math display"><strong>A</strong></span> 有 <spanclass="math display"><em>n</em></span>个线性无关的<strong>特征向量</strong> <spanclass="math display">⇔</span> <spanclass="math display">𝔽<sup><em>n</em></sup></span> 有一组由 <spanclass="math display"><strong>A</strong></span> 特征向量组成的基**<u><strong>(Que?)</strong></u></li></ol><h3 id="对角化主定理">对角化主定理</h3><p>一个 <span class="math display"><em>n</em></span> 阶矩阵可以对角化<span class="math display">⇔</span>矩阵的每个特征值代数重数与几何重数相等</p><p class="note note-info">1.特别地，若矩阵有 <span class="math display"><em>n</em></span>个不同的特征值，则可以对角化</br>2.实对称矩阵一定可以正交化。而且对于一个实对称矩阵 <spanclass="math display"><strong>A</strong></span> , 存在正交矩阵 <spanclass="math display"><strong>P</strong></span> ，使得 <spanclass="math display"><strong>A</strong> = <em>P</em><sup>−1</sup><em>D</em><em>P</em> = <em>P</em><sup><em>T</em></sup></span>,即通过正交变换变成对角阵， <spanclass="math display"><strong>A</strong></span> 正交相似于对角阵。</p><p>Note: 此处要填坑，系统复习一下相似矩阵那一章的证明</p><h2 id="矩阵分解">矩阵分解</h2><h3 id="满秩分解">满秩分解</h3><p>任何一个矩阵都可以被分解为列满秩矩阵 <spanclass="math display">×</span> 行满秩矩阵。</p><p>E.g. <span class="math display">$$\mathbf{A}_{n \times m} = \left[\begin{matrix} \mathbf{P}_{n \timesr(\mathbf{A})} &amp; 0 \end{matrix} \right] \left[\begin{matrix}\mathbf{Q}_{r(\mathbf{A}) \times m} \\ 0  \end{matrix}\right]$$</span> 一个比较简单的求解方法是，把 <spanclass="math display"><strong>Q</strong></span>搞成一个Hermitte标准形，这样 <spanclass="math display"><strong>P</strong></span> 直接可以得出结果：取矩阵<span class="math display"><strong>A</strong></span> 的前<spanclass="math display"><em>r</em>(<em>A</em>)</span> 列即可。</p><h3 id="lu分解">LU分解</h3><h3 id="奇异值svd分解">奇异值(SVD)分解</h3><h2 id="线性空间">线性空间</h2><ol type="1"><li>回顾加群(Abel群)</li></ol><ul><li>封闭性</li><li>结合律</li><li>交换律</li><li>有单位元(0元)</li><li>有逆元(-a)</li></ul><ol start="2" type="1"><li><p><strong>线性空间的直观理解</strong>：<spanclass="math display">(<em>V</em>, +)</span> 是一个加群，定义数域 <spanclass="math display">𝔽</span> 上的数乘运算，则 <spanclass="math display">(<em>V</em>, +, ⋅)</span> 是 <spanclass="math display">𝔽</span> 上的一个线性空间。 <spanclass="math display">𝔽</span> 叫做基域。<spanclass="math display">(ℝ<sup>3</sup>, +, ⋅)</span> 是 <spanclass="math display">ℝ</span> 的一个线性空间(有限维度的一个线性空间)</p><p>例如，<spanclass="math display"><em>f</em> ∈ <em>c</em>[<em>a</em>, <em>b</em>]</span>上的所有连续函数，则<spanclass="math display">(<em>c</em>[<em>a</em>, <em>b</em>], +, ⋅)</span>是一个线性空间(无穷维度)(有泛函的观点了，只要是线性无关的函数就能构成一组基底)。</p><p>例如，矩阵加法和数乘能构成一个线性空间，其一组基为全体基础矩阵<spanclass="math display">𝔼<sub><em>i</em><em>j</em></sub></span>。特别地，全体<spanclass="math display"><em>n</em></span> 阶方阵组成<spanclass="math display"><em>n</em><sup>2</sup></span>维线性空间；全体 <spanclass="math display"><em>n</em> × 1</span> 阶矩阵，即全体<spanclass="math display"><em>n</em></span>维向量，构成了<spanclass="math display">𝔽</span>上的一个<spanclass="math display"><em>n</em></span>维线性空间，其一组基由所有的标准向量组成即<spanclass="math display"><em>e</em><sub>1</sub>, <em>e</em><sub>2</sub>, …, <em>e</em><sub><em>n</em></sub></span></p></li><li><p><strong>线性空间的基向量定义：</strong>若 <spanclass="math display"><em>V</em></span> 中存在<spanclass="math display"><em>n</em></span>个线性无关的向量，使得 <spanclass="math display"><em>V</em></span>中任意向量都与他们线性相关，则称<spanclass="math display"><em>V</em></span>是<spanclass="math display"><em>n</em></span>线性空间。</p></li><li><p>V中任意向量均能唯一的表为<spanclass="math display"><em>α</em></span>的线性组合，<spanclass="math display"><em>α</em> = <em>k</em><sub>1</sub><em>α</em><sub>1</sub> + <em>k</em><sub>2</sub><em>α</em><sub>2</sub> + … + <em>k</em><sub><em>n</em></sub><em>α</em><sub><em>n</em></sub> = (<em>α</em><sub>1</sub>, <em>α</em><sub>2</sub>, …, <em>α</em><sub><em>n</em></sub>)<em>k</em></span>，则称为向量<spanclass="math display"><em>α</em></span>关于基的坐标<spanclass="math display">(<em>k</em><sub>1</sub>, <em>k</em><sub>2</sub>, …, <em>k</em><sub><em>n</em></sub>)<sup><em>T</em></sup></span>。</p><p>线性空间的基一般不唯一，但线性空间的维数是唯一确定的。所以不同基向量包含的向量个数相同。</p></li><li><p><strong>基扩充定理</strong>:</p></li><li><p>过渡矩阵：设<spanclass="math display"><em>n</em></span>维线性空间的一组基<spanclass="math display">(<em>α</em><sub>1</sub>, <em>α</em><sub>2</sub>, …, <em>α</em><sub><em>n</em></sub>)</span>与另一组基<spanclass="math display">(<em>β</em><sub>1</sub>, <em>β</em><sub>2</sub>, …, <em>β</em><sub><em>n</em></sub>)</span>如果存在如下关系:</p><p><span class="math display">$$\left[\begin{matrix} \beta_1&amp; \beta_2 &amp; \dots \beta_n\end{matrix} \right] = \left[\begin{matrix} \alpha_1&amp; \alpha_2 &amp;\dots \alpha_n \end{matrix} \right] \mathbf{P}$$</span></p><p>所以可见，每个列向量 <spanclass="math display"><em>β</em><sub><em>i</em></sub></span> 都是 <spanclass="math display"><em>α</em><sub><em>j</em></sub></span>的线性组合，组合系数由<spanclass="math display"><em>P</em><sub><em>i</em></sub></span>列向量控制。</p></li></ol><p><img src="https://i.loli.net/2021/10/11/UiXGb2vNnJfgFus.jpg" style="zoom: 33%;" /></p><h2 id="内积空间">内积空间</h2><ol type="1"><li><p>背景：内积的引入和“长度“有关。在线性空间中引入内积的概念，衡量两个向量的“远近”。如果内积归一化后，就是通过两个单位向量的夹角来衡量两个向量的距离。</p></li><li><p><strong>内积空间</strong>：设<spanclass="math display"><em>V</em><em>是</em></span><spanclass="math display">𝔽</span>上的线性空间，若对<spanclass="math display"><em>V</em></span>中的任意两个向量，都定义了<spanclass="math display">𝔽</span>中的一个数<spanclass="math display">(<em>α</em>, <em>β</em>)</span>,使得满足</p><ul><li><p>(共轭对称性) <span class="math display">$$(\alpha, \beta) =\overline{(\beta, \alpha)}$$</span></p></li><li><p>(正定性）<spanclass="math display">(<em>α</em>, <em>α</em>) ≥ 0</span></p></li><li><p>(双线性) <spanclass="math display">(<em>a</em><em>α</em> + <em>b</em><em>β</em>，<em>γ</em>) = <em>a</em>(<em>α</em>, <em>γ</em>) + <em>b</em>(<em>β</em>, <em>γ</em>)</span>(共轭双线性)<span class="math display">$$(\alpha, a \beta + b \gamma) =a(\alpha, \beta) + \overline{b}(\alpha, \beta)$$</span></p><p>则称其为内积空间。</p></li></ul></li><li><p>内积与范数性质</p><p>比较重要的且容易忘记的，Cauchy-Schwards不等式， 三角不等式</p></li><li><p>内积的作用：从代数上，两个向量的角度，两个向量在线性空间中的距离都可以由内积来定义。有了角度和长度，从几何上更好解释向量之间的位置关系。比如两个向量正交是垂直(角度90°)，实数域的线性空间上两个向量线性相关当且仅当夹角为0或<spanclass="math display"><em>π</em></span>。</p></li><li><p>Gram-Schmidt正交化方法: 已知线性无关组<spanclass="math display"><em>α</em><sub>1</sub>, …, <em>α</em><sub><em>n</em></sub></span>,求标准正交组</p><p><spanclass="math display"><em>β</em><sub><em>k</em></sub> = <em>α</em><sub><em>k</em></sub> − ∑<sub><em>i</em></sub>(<em>α</em><sub><em>i</em></sub>, <em>γ</em><sub><em>i</em></sub>)<em>γ</em><sub><em>i</em></sub></span></p><p><span class="math display">$$\gamma_k = \frac{\beta_k}{||\beta_k||}$$</span></p><p>几何意义：再求第<spanclass="math display"><em>k</em></span>个标准正交基时，投影到<spanclass="math display"><em>γ</em><sub>1</sub>…<em>γ</em><sub><em>k</em> − 1</sub></span>个标准正交基构成的”超平面“上的投影等于其在各个<spanclass="math display"><em>γ</em><sub><em>i</em></sub></span>分量上的投影向量之和。所以减去后自然就与<spanclass="math display"><em>γ</em><sub>1</sub>…<em>γ</em><sub><em>k</em> − 1</sub></span>正交，最后再单位化。</p></li><li><p>酉矩阵<spanclass="math display"><strong>A</strong><sup><em>H</em></sup><strong>A</strong> = <strong>I</strong></span>,在实数域上就是正交矩阵<spanclass="math display"><strong>A</strong><sup><em>T</em></sup><strong>A</strong> = <strong>I</strong></span>。该矩阵的每个行(列)向量两两正交，且为单位向量。正交矩阵的集合意义几乎就是旋转变换，利用正交矩阵做旋转变换可以去掉二次型中的交叉项，变成标准形式。</p><p><img src="https://i.loli.net/2021/10/11/BWxoCmdMzDG9c12.png" alt="image-20211011195300299" style="zoom: 67%;" /></p></li></ol><p><img src="https://i.loli.net/2021/10/11/agG4bAcnJCmlEeW.png" alt="image-20211011195358735" style="zoom: 60%;" /></p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>《线性代数》 第二版 居余马清华大学出版社<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>《矩阵理论与应用》 张跃辉上海交通大学出版社<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>矩阵，线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：矩阵理论(2)——矩阵与线性变换</title>
    <link href="/posts/84284e51/"/>
    <url>/posts/84284e51/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记矩阵理论2矩阵与线性变换">课程笔记：矩阵理论(2)——矩阵与线性变换</h1>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>矩阵，线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：矩阵理论(1)——线性代数知识回顾(上)</title>
    <link href="/posts/6e2da84b/"/>
    <url>/posts/6e2da84b/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记矩阵理论1线性代数知识回顾上">课程笔记：矩阵理论(1)——线性代数知识回顾(上)</h1><p>📚书籍：《矩阵理论与应用》张跃辉 Chap 1</p><h2 id="梦开始的地方">梦开始的地方：</h2><p>约定本课程讨论均在复数域<spanclass="math display"><em>C</em></span>的子域上进行。</p><h2 id="引言-线性代数是什么">引言： 线性代数是什么</h2><blockquote><p>本科阶段的线性代数课程讨论两个相关问题：一个是引入矩阵来解线性方程组，另一个是利用线性方程组来研究矩阵。</p></blockquote><h4 id="矩阵解线性方程组">矩阵解线性方程组</h4><p><span class="math display">$$\begin{align}\textbf{A}x &amp;= b \\x_1 \alpha_1 + x_2 \alpha_2 + \dots + x_n \alpha_n &amp;=  b\label{eq:eg1}\end{align}$$</span></p><p>可以通过研究齐次方程组 <spanclass="math display"><strong>A</strong><em>x</em> = 0</span></p><p>的解结构来来研究<spanclass="math display"><strong>A</strong><em>x</em> = <em>b</em></span>的解结构，任意解可以表达为基础解系的线性组合。</p><details><summary>展开可详细回顾线性方程组求解</summary><p><b>截图自《线性代数》第二版 居余马著 P47 - P48</b></p><img width="680" src="https://i.loli.net/2021/10/10/725C3QEdf1hiDVA.jpg" alt="图1"><img width="680" src="https://i.loli.net/2021/10/10/XlW96OenEbxMIZv.jpg" alt="图2"></details><h4 id="矩阵提供二次型的简洁表达">矩阵提供二次型的简洁表达</h4><p>$$ $$</p><p>请熟悉这个形式和上面的推导过程~系数矩阵 <spanclass="math display"><strong>A</strong></span> 中的每一项<spanclass="math display"><em>a</em><sub><em>i</em><em>j</em></sub></span>对应的 <spanclass="math display"><em>x</em><sub><em>i</em></sub></span>与<spanclass="math display"><em>x</em><sub><em>j</em></sub></span>的乘积的系数。假如<spanclass="math display"><strong>A</strong></span>是一个对角矩阵的话，那么就是标准型<spanclass="math display">∑<sub><em>i</em></sub><em>α</em><sub><em>i</em><em>i</em></sub><em>x</em><sub><em>i</em></sub><sup>2</sup></span>，（就好比二维平面上的圆），有交叉项的话好比二维平面的椭圆。<spanclass="math display"><strong>A</strong></span>一定是一个对称矩阵，因为<spanclass="math display"><em>x</em><sub><em>i</em></sub><em>x</em><sub><em>j</em></sub></span>的系数就是<spanclass="math display"><em>x</em><sub><em>j</em></sub><em>x</em><sub><em>i</em></sub></span>的系数。</p><h3 id="矩阵乘法与二次型的关系">矩阵乘法与二次型的关系</h3><p>如何理解<spanclass="math display"><em>n</em></span>阶方阵的高次幂<spanclass="math display"><strong>A</strong><sup><em>m</em></sup></span>?</p><p>相似矩阵<spanclass="math display"><strong>A</strong> ∼ <strong>B</strong></span>,<spanclass="math display"><strong>B</strong> = <strong>P</strong><sup>−1</sup><em>A</em><strong>P</strong></span>。如果利用特征值与特征向量可将<spanclass="math display"><strong>A</strong> = <strong>P</strong><sup>−1</sup><strong>D</strong><strong>P</strong></span>，那么<spanclass="math display"><strong>A</strong><sup><em>m</em></sup> = <strong>P</strong><sup>−1</sup><strong>D</strong><sup><em>m</em></sup><strong>P</strong></span>,<spanclass="math display"><strong>A</strong><sup>−1</sup> = <strong>P</strong><sup>−1</sup><strong>D</strong><sup>−1</sup><strong>P</strong></span>。</p><p>此外实对称矩阵可以正交对角化，即存在正交矩阵<spanclass="math display"><strong>Q</strong></span>，使得 <spanclass="math display"><strong>Q</strong><sup>−1</sup><strong>A</strong><strong>Q</strong> = <strong>Q</strong><sup><em>T</em></sup><strong>A</strong><strong>Q</strong> = <strong>D</strong></span></p><p>可以利用坐标变换 <spanclass="math display"><em>x</em> = <strong>Q</strong><em>y</em></span>将实二次型化为标准型 <spanclass="math display"><em>f</em> = <em>λ</em><sub>1</sub><em>y</em><sub>1</sub><sup>2</sup> + <em>λ</em><sub>2</sub><em>y</em><sub>2</sub><sup>2</sup> + <em>λ</em><sub>3</sub><em>y</em><sub>3</sub><sup>2</sup> + … + <em>λ</em><sub><em>n</em></sub><em>y</em><sub><em>n</em></sub><sup>2</sup></span></p><h2 id="矩阵的基础运算及其性质">矩阵的基础运算及其性质</h2><h3 id="共轭转置">共轭转置</h3><ol type="1"><li><spanclass="math display"><strong>A</strong></span>的共轭矩阵记作<spanclass="math display">$$\bar{\mathbf{A}}$$</span>，<spanclass="math display"><strong>A</strong></span>的共轭转置矩阵记作<spanclass="math display"><strong>A</strong><sup><em>H</em></sup></span>,<spanclass="math display">$$\mathbf{A}^H=\bar{\mathbf{A}}^T$$</span>当,<spanclass="math display"><strong>A</strong> ∈ ℝ<sup><em>m</em> × <em>n</em></sup></span>时，<spanclass="math display">$$\bar{\mathbf{A}}=\mathbf{A}^T$$</span></li><li><spanclass="math display"><strong>A</strong> = <strong>A</strong><sup><em>H</em></sup></span>的矩阵被称为Hermite矩阵，其中当<spanclass="math display"><strong>A</strong> ∈ ℝ<sup><em>n</em> × <em>n</em></sup></span>是实对称矩阵。</li></ol><h3 id="矩阵乘法">矩阵乘法</h3><p>矩阵乘法可以按照行向量进行, 比如对于<spanclass="math display"><em>n</em> × <em>m</em></span>与<spanclass="math display"><em>m</em> × <em>p</em></span>的两个矩阵相乘 $$ $$前一个矩阵的每一行乘以后一个矩阵的每一列，得到新矩阵的每一个元素。</p><p>其中，当<span class="math display"><strong>B</strong></span>是一个<span class="math display"><em>m</em> × 1</span>(列向量)时, <spanclass="math display">$$\begin{align}\mathbf{A}\mathbf{B}&amp;=\left[ \begin{matrix}\alpha_1^T \\ \alpha_2^T \\ \cdots \\\alpha_n^T  \end{matrix}\right]\left[x\right] \\&amp;=\left[ \begin{matrix} \alpha_1^T x\\ \alpha_2^Tx \\ \vdots \\\alpha_n^T x \end{matrix}\right] \\&amp;=\left[ \begin{matrix} a_{11}x_1 + a_{12}x_2 + a_{13}x_3 + \dots +a_{1m} x_m\\ a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + \dots + a_{2m} x_m \\\vdots \\ a_{n1}x_1 + a_{n2}x_2 + a_{n3}x_3 + \dots + a_{nm} x_m\end{matrix}\right] \\&amp;=x_1\left[ \begin{matrix} a_{11} \\ a_{21} \\ \vdots \\ a_{n1}\end{matrix}\right] + \dots + x_m\left[ \begin{matrix} a_{1m} \\ a_{2m}\\ \vdots \\ a_{nm} \end{matrix}\right] \\&amp;= x_1A_1 + x_2 A_2 + \dots + x_m A_m \\&amp;= \left[\begin{matrix}A_1 &amp; A_2 &amp; \cdots &amp; A_n\end{matrix} \right] \left[x \right]\end{align}$$</span></p><p>这里，可以把第一个矩阵摆成<spanclass="math display"><em>n</em></span>行组成的列向量形式，也可以摆成<spanclass="math display"><em>m</em></span>列的行向量的形式。<strong>其中第一种形式比较容易理解，因为这就是高斯消元法中<spanclass="math display"><em>n</em></span>个线性方程组联立在一起的形式</strong>。但是第二种形式特别容易出错，搞晕，因为第二种形式的线性组合是数乘向量求和的形式，多做几遍就熟悉了。</p><div class="note note-primary">            <p><strong>第二种形式的意义：矩阵乘一个列向量相当于矩阵所有列的线性组合。</strong></p>          </div><p>同样的，一个行向量乘矩阵相当于矩阵所有行的线性组合，即 <spanclass="math display">$$y^T \mathbf{A} = y^T \left[\begin{matrix}\alpha_1^T \\ \alpha_2^T \\\cdots \\ \alpha_n^T  \end{matrix} \right] = y_1\alpha_1^T +y_2\alpha_2^T + \dots y_n \alpha_n^T$$</span></p><p>因此矩阵乘法<spanclass="math display"><strong>A</strong><strong>B</strong> = <strong>C</strong></span>,可以看做<spanclass="math display"><strong>C</strong></span>的每一列，都是<spanclass="math display"><strong>A</strong></span>每一列的线性组合，组合系数是<spanclass="math display"><strong>B</strong></span>的对应列；<spanclass="math display"><strong>C</strong></span>的每一行可以看做是 <spanclass="math display"><strong>A</strong></span>的每一行的线性组合，组合系数是<spanclass="math display"><strong>A</strong></span>的对应行。若<spanclass="math display"><strong>A</strong><strong>B</strong> = 0</span>，那么<spanclass="math display"><strong>B</strong></span>的每一列向量都是<spanclass="math display"><strong>A</strong><em>x</em> = 0</span>的齐次方程组的解。同理，<spanclass="math display"><strong>A</strong></span>的每一行也是<spanclass="math display"><em>y</em><sup><em>T</em></sup><strong>B</strong> = 0</span>的齐次方程组的解。</p><h3 id="方阵的多项式">方阵的多项式</h3><p><spanclass="math display"><em>f</em>(<strong>A</strong>) = <em>a</em><sub>0</sub><strong>I</strong> + <em>a</em><sub>1</sub><strong>A</strong> + <em>a</em><sub>2</sub><strong>A</strong><sup>2</sup> + … + <em>a</em><sub><em>m</em></sub><strong>A</strong><sup><em>m</em></sup></span></p><p>称为<spanclass="math display"><strong>A</strong></span>的多项式。易知，<spanclass="math display"><em>f</em>(<strong>A</strong>)<em>g</em>(<strong>A</strong>) = <em>g</em>(<strong>A</strong>)<em>f</em>(<strong>A</strong>)</span>，同一方阵的多项式是可以交换的。</p><h2id="行列式迹伴随矩阵逆秩等性质">行列式，迹，伴随矩阵，逆，秩等性质</h2><h3 id="行列式">行列式</h3><ol type="1"><li><spanclass="math display">det <strong>A</strong> = |<strong>A</strong>|</span></li><li><spanclass="math display">|<strong>A</strong><strong>B</strong>| = |<strong>A</strong>||<strong>B</strong>|</span></li><li>当<span class="math display">|<strong>A</strong>| ≠ 0</span>时，<spanclass="math display"><strong>A</strong><strong>B</strong> = 0</span>必有<spanclass="math display"><strong>B</strong> = 0</span>,<spanclass="math display"><strong>A</strong><strong>B</strong> = <strong>A</strong><strong>C</strong></span>必有<spanclass="math display"><strong>B</strong> = <strong>C</strong></span>(满秩矩阵齐次方程有唯一解，就是0解)。</li><li>求解二阶行列式对角线法，求解三阶行列式用沙路法。求解<spanclass="math display"><em>n</em></span>阶行列式用递推法（如下按照第<spanclass="math display"><em>i</em></span>行展开）<spanclass="math display">$$\left|\begin{matrix} a_{11} &amp; a_{12} &amp;\dots &amp; a_{1n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\a_{n1} &amp; a_{n2}&amp; \dots &amp;a_{nn}  \end{matrix} \right| =\sum_j a_{ij}A_{ij}$$</span>(代数余子式忘了的自己wiki去)。最后完全展开的结果有<spanclass="math display"><em>n</em>!</span>项，每一项都是不同行不同列元素乘积，然后一半是正号，一半是负号。</li><li>n阶方程组有克莱默法则，理论意义上解释了线性方程组和其系数的关系。</li></ol><h3 id="矩阵的迹">矩阵的迹</h3><ol type="1"><li><p><spanclass="math display">tr(<em>α</em><strong>A</strong> + <em>β</em><strong>B</strong>) = <em>α</em>tr(<strong>A</strong>) + <em>β</em>tr(<strong>B</strong>)</span></p></li><li><p><spanclass="math display">tr(<strong>A</strong><strong>B</strong>) = tr(<strong>B</strong><strong>A</strong>)</span>（如果分别是<spanclass="math display"><em>n</em> × <em>m</em></span>和<spanclass="math display"><em>m</em> × <em>n</em></span>的矩阵即可）</p></li><li><p><spanclass="math display">tr(<strong>A</strong><strong>A</strong><sup><em>H</em></sup>) = ∑<sub><em>i</em>, <em>j</em></sub>|<em>a</em><sub><em>i</em><em>j</em></sub>|<sup>2</sup></span>（所有元素的平方和，故<spanclass="math display">tr(<strong>A</strong><strong>A</strong><sup><em>H</em></sup>) = 0 ⇒ <strong>A</strong> = 0</span>）</p></li></ol><h3 id="矩阵的秩">矩阵的秩</h3><ol type="1"><li>定义：矩阵所有不为0的子式中最高的阶数为矩阵的秩</li><li>如果一个矩阵的秩为1，那么一定存在列向量<spanclass="math display"><em>α</em>, <em>β</em></span>, 使得<spanclass="math display"><em>A</em> = <em>α</em><em>β</em><sup><em>T</em></sup></span>(列向量x行向量，得到矩阵的每一行都是<spanclass="math display"><em>α</em><sub><em>i</em></sub><em>β</em></span>，显然任意两行都线性相关)。那么计算矩阵高次幂时<spanclass="math display"><strong>A</strong><sup><em>m</em></sup> = <em>α</em>(<em>β</em><sup><em>T</em></sup><em>α</em>)<sup><em>m</em> − 1</sup><em>β</em><sup><em>T</em></sup></span></li><li>矩阵和和乘积的秩有如下不等式(<spanclass="math display"><em>n</em> × <em>p</em>, <em>p</em> × <em>m</em></span>)：</li></ol><p><span class="math display">$$\begin{align}r(\mathbf{A+B}) &amp;\le r(\mathbf{A}) + r(\mathbf{B}) \\r(\mathbf{A}) + r(\mathbf{B}) - p \le r(\mathbf{AB}) &amp; \le\min\{r(\mathbf{A}), r(\mathbf{B})\}\end{align}$$</span></p><details><summary>想看证明请展开</summary><p><b>截图自<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="《线性代数》 第二版 居余马 清华大学出版社">[1]</span></a></sup>P6</b></p><img width="680" src="https://i.loli.net/2021/10/11/NOefIEdJkPUxwVt.jpg" alt="图1"></details><h3 id="伴随矩阵逆矩阵">伴随矩阵，逆矩阵</h3><ol type="1"><li><p>伴随矩阵(如果是实矩阵通常用<spanclass="math display"><em>A</em><sup>*</sup></span>，否则常用<spanclass="math display"><em>a</em><em>d</em><em>j</em><strong>A</strong></span>表示，个人喜好，即便是在复数域上也喜欢用<spanclass="math display"><em>A</em><sup>*</sup></span>表示，敬请理解)。<spanclass="math display"><em>A</em><em>A</em><sup>*</sup> = <em>A</em><sup>*</sup><em>A</em> = |<em>A</em>|<em>I</em></span>(证明可参考<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="《线性代数》 第二版 居余马 清华大学出版社">[1]</span></a></sup>中2.2节例题6)</p></li><li><p><span class="math display">$$\mathbf{A}^{-1} =\frac{A^*}{|A|}$$</span></p></li><li><p><spanclass="math display">(<strong>A</strong><sup><em>T</em></sup>)<sup>−1</sup> = (<strong>A</strong><sup>−1</sup>)<sup><em>T</em></sup></span></p></li><li><p><spanclass="math display">|<strong>A</strong><sup>−1</sup>| = |<strong>A</strong>|<sup>−1</sup></span></p></li><li><p>可逆矩阵与任何矩阵乘积不会改变原矩阵的秩 <spanclass="math display"><em>r</em>(<strong>A</strong>) = <em>r</em>(<strong>P</strong><strong>A</strong>) = <em>r</em>(<strong>A</strong><strong>Q</strong>) = <em>r</em>(<strong>P</strong><strong>A</strong><strong>Q</strong>)</span></p></li><li><p>矩阵满秩，非奇异，可逆三者概念等价</p></li></ol><h3 id="分块矩阵">分块矩阵</h3><div class="note note-info">            <p>分块矩阵是一种非常好用的技巧。</p>          </div><ol type="1"><li><p>分块矩阵的加法和数乘就不说了🐒</p></li><li><p>分块矩阵乘法：对于<spanclass="math display"><em>n</em> × <em>p</em>, <em>p</em> × <em>m</em></span>的两个矩阵相乘，假设把矩阵<spanclass="math display"><strong>A</strong></span>横切<spanclass="math display"><em>r</em></span>刀，竖着切<spanclass="math display"><em>s</em></span>刀(你就想象一下家里切豆腐凉拌),矩阵<spanclass="math display"><strong>B</strong></span>横着切<spanclass="math display"><em>s</em></span>刀，竖着切<spanclass="math display"><em>t</em></span>刀，显然分块后也是对应每一行x每一列。最后乘起来得到的是<spanclass="math display"><em>r</em> × <em>t</em></span>块<del>油炸豆腐</del>矩阵。但由于分块乘法，乘法本身也是矩阵乘法，所以也要满足<spanclass="math display"><strong>A</strong></span>块的列数=<spanclass="math display"><strong>B</strong></span>块的行数。所以在<del>豆腐下锅</del>矩阵分块乘法前，第一个矩阵竖着切的每一刀的宽度要和第二块豆腐横着切的每一刀宽度相同</p><p>(脑子全是香喷喷的铁锅油炸豆腐，浇上味极鲜酱油撒上葱花后热气腾腾地出锅……)</p></li></ol><p>E.g.</p><details><summary>想看例子请戳我，不想看可忽略</summary><p><b>截图自<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="《线性代数》 第二版 居余马 清华大学出版社">[1]</span></a></sup>P82</b></p><img width="680" src="https://i.loli.net/2021/10/11/ktlFSeGERgNmb4B.png" alt="图1"><img width="680" src="https://i.loli.net/2021/10/11/OGQ1cNxFh4J2Uny.png" alt="图1"></details><ol start="3" type="1"><li>分块矩阵的转置(自己想，想不出来wiki去)</li><li>分块矩阵的逆矩阵：</li></ol><p>对角分块阵(假如<spanclass="math display"><strong>A</strong><sub><em>i</em></sub></span>都可逆)</p><p><span class="math display">$$\left[\begin{matrix}\mathbf{A}_1 &amp;  \\  &amp;\mathbf{A}_2&amp; \\&amp; &amp;\ddots&amp; \\ &amp;&amp;&amp; \mathbf{A}_k \end{matrix}\right]^{-1} = \left[\begin{matrix} \mathbf{A}^{-1}_1&amp;  \\  &amp;\mathbf{A}^{-1}_2&amp; \\ &amp; &amp;\ddots&amp; \\&amp;&amp;&amp; \mathbf{A}^{-1}_k  \end{matrix}  \right]$$</span></p><p>普通矩阵，分成几块后做乘法，比如设: <span class="math display">$$\left[\begin{matrix}\mathbf{A}_{11} &amp; \mathbf{A}_{12}   \\\mathbf{A}_{21} &amp; \mathbf{A}_{22} \\  \end{matrix} \right]^{-1} =\left[\begin{matrix}\mathbf{X} &amp; \mathbf{Y}  \\ \mathbf{U} &amp;\mathbf{V} \\  \end{matrix} \right]$$</span></p><p>可得</p><p><span class="math display">$$\begin{align}\mathbf{A}_{11}\mathbf{X} + \mathbf{A}_{12}\mathbf{U} &amp;= \mathbf{I}\\\mathbf{A}_{11}\mathbf{Y} + \mathbf{A}_{12}\mathbf{V} &amp;= 0 \\\mathbf{A}_{21}\mathbf{X} + \mathbf{A}_{22}\mathbf{U} &amp;= 0 \\\mathbf{A}_{21}\mathbf{Y} + \mathbf{A}_{22}\mathbf{V} &amp;= \mathbf{I}\end{align}$$</span>这样可以把一个高阶的矩阵求逆问题转化成低阶矩阵求逆问题。结合<strong>分治法</strong>和一些<strong>奇奇怪怪</strong>的trick，可以像矩阵乘法一样从<spanclass="math display"><em>O</em>(<em>n</em><sup>3</sup>)</span>进行时间复杂度优化。</p><ol start="5" type="1"><li>分块矩阵的初等变换(以<span class="math display">2 × 2</span>为例，以后填坑)</li></ol><p>分块对换阵 <span class="math display">$$\left[\begin{matrix}0 &amp; \mathbf{I} \\\mathbf{I} &amp; 0\end{matrix} \right]$$</span></p><p>分块倍乘阵 <span class="math display">$$\left[\begin{matrix}\mathbf{C}_1 &amp; 0 \\0 &amp; \mathbf{I}\end{matrix} \right]$$</span></p><p>分块倍加阵 <span class="math display">$$\left[\begin{matrix}\mathbf{I} &amp; 0 \\\mathbf{C}_1 &amp; \mathbf{I}\end{matrix} \right]$$</span></p><h2 id="线性方程组">线性方程组</h2><div class="note note-warning">            <p>这一块讲的非常简略，因为学过线代的人大多数这一章掌握的比较牢靠(往往相似矩阵，矩阵分解，二次型那一块快到期末，掌握得通常不太好)。所以省下笔墨只零零星星地提几点。</p>          </div><ol type="1"><li>（<strong>非常重要</strong>）<spanclass="math display">{<em>α</em><sub>1</sub>, <em>α</em><sub>2</sub>, …, <em>α</em><sub><em>m</em></sub>}</span>线性无关 <span class="math display">⇔</span> <spanclass="math display"><em>k</em><sub>1</sub><em>α</em><sub>1</sub> + <em>k</em><sub>2</sub><em>α</em><sub>2</sub> + … + <em>k</em><sub><em>m</em></sub><em>α</em><sub><em>m</em></sub> = 0</span>仅有0解</li><li>（<strong>非常重要</strong>）<strong>极大线性无关组</strong>的定义。(此外，极大线性无关组可以作为方程组的一个基础解系)</li><li>（<strong>非常重要</strong>）齐次线性方程组基本定理：基础解系的向量数量=<spanclass="math display"><em>n</em> − <em>r</em>(<strong>A</strong>)</span>(从这里可以看出齐次方程组的解空间是<spanclass="math display">𝔽<sup><em>n</em> × <em>m</em></sup></span>的子空间，正因为系数矩阵有了秩(序)，才导致解空间维度比原来的空间小了，变成了基础解系的向量数量)</li><li>线性方程组基本定理：是否有解(判别条件)；通解和特解。</li><li>具体计算：化简为Hermitte标准形(行阶梯矩阵标准型)。<spanclass="math display">$$\left[\begin{matrix} \mathbf{I} &amp; \mathbf{A}\\ 0 &amp; 0 \end{matrix} \right]$$</span></li></ol><div class="note note-success">            <blockquote><p>“有的人天生就是战士。他/她不会惧怕一切困难，也不会为一切失败与挫折低头，因为他/她的目标是星辰大海”⭐️</p></blockquote>          </div><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>《线性代数》 第二版 居余马清华大学出版社<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>《矩阵理论与应用》 张跃辉上海交通大学出版社<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>矩阵，线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔：关于高维空间样本分布稀疏性的联想</title>
    <link href="/posts/d05437cf/"/>
    <url>/posts/d05437cf/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="随笔关于高维空间样本分布稀疏性的联想">随笔：关于高维空间样本分布稀疏性的联想</h1><h2 id="背景提要">背景提要</h2><p>在高维的向量空间中，假设有限个数的样本从一个均匀分布随机采样得到的，那么可以证明这些样本大多会倾向于分布在高维球体或立方体的面都附近(Surface)，而且即便是最近的两个样本之间的距离也会非常远，因为高维空间实在太稀疏了。比如在卷积神经网络使用的向量维度通常是1024维或2048维。</p><p>我一直在思考有没有一个形象化的比喻来直观地解释，或者类比一下这个现象。先想到了两个例子，但可能原理不同，也不一定贴切。</p><h2 id="宇宙膨胀">宇宙膨胀</h2><p>现代的宇宙学最大的一个发现就是，我们所处的宇宙是在不断膨胀的，就像一个正在不断被吹胀的气球一样，我们周围的星系在不断离我们远去。但是据说我们的宇宙的维度可能有11维(包括宏观维度和微观物质的维度)，即便我们的宇宙是高维空间，也不像是1024或2048维这么高的维度；而且宇宙空间会纯粹的数学上的向量空间是两个完全不同的系统，即便在某些性质上可能会有相似的交集。</p><p>突然想起一句台词，”我们像星星一样近，我们像星星一样远。我们近得摩擦生电，我们远如天际云端。“</p><h2 id="社交距离人际关系的疏远">社交距离&amp;人际关系的疏远</h2><p>我想到的一个完全不相干，但是又很奇妙的比喻：人际关系的亲疏远近。有个比较普遍的现象：小时候很容易交到朋友和玩伴，建立一段Friendship的障碍没有那么大，小孩子之间的社交距离很近；但是随着我们逐渐长大，经历的事情越来越多，我们的心灵越来越社会化，会越发现建立一段Relationship会变得愈发困难。进入社会工作后，自己身边都是”同事“和陌生人，大家都很忙，彼此不想过度打扰，即便是你感觉再好的朋友距离也会比小时候变远，就好像在一段数轴上均匀分布的5个点可能离得很近，但是如果把这5个点随机撒入几十维的高维球中，那么即便是最近的两个点，其欧氏距离也比一维坐标轴上的距离要更遥远。</p><p>那么比喻的相似点在哪里呢？年龄小的时候，我们所生活在一个低维的世界中，在这个世界中只有”吃饭“，”睡觉“，”写作业“，”玩“等较少的维度中。维度越低的时候，也是我们的心智越稚嫩简单的时候，对陌生人和这个世界容易充满信任。当我们慢慢长大，上大学，读研，工作，建立家庭，我们的生活维度在不断扩张，直到变成一个高维空间。在这个高维空间里，生活和工作中的遇到的人际关系会更加复杂，再也不是在象牙塔里如此的单纯，仿佛整个世界只由几个简单的维度构成。生活在一个高维空间里是很容易疲惫，就好像训练一个高维的神经网络模型一样困难。自然而然，在这个高维空间里，如果把每个人作为这个社会随机采样下的样本，样本之间的距离也会随之自然变远。</p><p>PS:居然查到了有《社会统计学》这门课，不过真正的社会统计学肯定不是像我这般的胡思乱想。</p><p>“无尽的远方,无数的人们,都与我有关。” 希望这句话能带给我一些安慰。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>高维空间, 维度灾难</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔：那些在CV里用腻了的话术与说辞</title>
    <link href="/posts/29b99bf4/"/>
    <url>/posts/29b99bf4/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="随笔那些在cv里用腻了的话术与说辞">随笔：那些在CV里用腻了的话术与说辞</h1><p>究其根源，深度学习是一个黑盒子(BlackBox)，缺乏一套能够解释清楚深度神经网络的运作机理的数学理论，比如，能否解释深度网络究竟从海量数据中学到了什么，深度网络的反向传播过程的收敛条件，训练深度模型为什么花费那么长时间，究竟有多少时间是对学习真正有用的。由于缺乏理论支撑，很多搞深度学习应用的(如CV,NLP)大多follow这样的研究路线：通过紧跟研究”热点“，读paper参会看别人提出了哪些模型，然后想到一个”灵感“，提出一套自己魔改的结构，抱着试一试的心态做实验发现work了，再来解释一通，就去投稿发paper。所以很多paper(尤其搞CV/NLP+DL)都是从性能结果反向解释自己的神经网络结构，并且很多解释是基于经验和直觉的。</p><p>读多了后，发现有一些话术是比较Common的技巧，就是很多工作都愿意用上这些技巧来提点，写作时用上相似的话术去讲故事，总觉一下就觉得比较有意思。以后有空会继续补充。</p><ol type="1"><li>Multi-Resolution/Hierarchical/Pyramid Sturcture/Extract Feature atdiferent scales</li><li>不同层次/级别的特征如何融合：Feature Fusion</li><li>分类问题/多模态问题/迁移学习问题: Intra-class variance &amp;Inter-class Variance(最小化类内XXX，最大化类间XXX,以保留判别性信息)</li><li>Global Part + Local Part 的多分支设计</li><li>Local Discriminative Part</li><li>粗粒度+细粒度(Coarse + Fine的设计)</li><li>Grouping操作，把相似的归纳到一起(比如聚类，相似度打分)，分类做loss</li><li>Feature Alignment, Domain Alignment, PixelAlignment；各种Alignment</li><li>Meta Learning +各种领域；就近年会议review情况来看，元学习快被用烂了(烂大街的感觉)，基本是所有reviewer都觉得没啥可用的。把他作为主要创新点容易被喷novelty不够。</li><li>multi-task learning</li><li>point-voxel interaction/distillation</li><li>multi-scale的图像，点云，视频，文本等，总要提一嘴</li><li>Prompt+ everything</li><li>attention is all your need. 满大街的attention</li></ol><p>(未完待续…)</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CV</tag>
      
      <tag>话术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：统计学习理论与方法(ELS_Chap2)</title>
    <link href="/posts/3c10486b/"/>
    <url>/posts/3c10486b/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记统计学习理论与方法els_chap2">课程笔记：统计学习理论与方法(ELS_Chap2)</h1><p>📚书籍：《Elements of Statistical Learning》Chap 2</p><h2 id="符号约定">符号约定</h2><table><thead><tr><th style="text-align: center;">输入变量(Input Variable)</th><th style="text-align: center;"><spanclass="math display"><em>X</em></span>, 通常随机变量取值是<spanclass="math display"><em>p</em></span>维随机向量(<spanclass="math display"><em>X</em> ∈ ℝ<sup><em>p</em></sup></span>),其每个分量可用下角标<spanclass="math display"><em>X</em><sub><em>i</em></sub></span>表示是该维度特征的变量</th></tr></thead><tbody><tr><td style="text-align: center;">(随机)变量<spanclass="math display"><em>X</em></span>的第i个观测值</td><td style="text-align: center;"><spanclass="math display"><em>x</em><sub><em>i</em></sub></span>,可以是向量或标量</td></tr><tr><td style="text-align: center;">模型输出</td><td style="text-align: center;">Quantative Outputs:<spanclass="math display"><em>Y</em></span>, Category: <spanclass="math display"><em>G</em></span>； 模型预测值<spanclass="math display"><em>Ŷ</em></span></td></tr><tr><td style="text-align: center;">矩阵</td><td style="text-align: center;">大写加粗正体<spanclass="math display"><strong>X</strong></span>，如<spanclass="math display"><em>N</em></span>个维度为<spanclass="math display"><em>p</em></span>的向量<spanclass="math display"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>N</em></sub></span>组成<spanclass="math display"><em>N</em> × <em>p</em></span>的矩阵<spanclass="math display"><strong>X</strong></span></td></tr><tr><td style="text-align: center;">向量</td><td style="text-align: center;">约定:普通的<spanclass="math display"><em>p</em></span>维向量表示成<spanclass="math display"><em>x</em><sub><em>j</em></sub></span>，不用加粗;而维度为N时，若加粗为<spanclass="math display"><strong>x</strong><sub><em>i</em></sub></span>，代表的是随机变量第<spanclass="math display"><em>i</em></span>个特征分量的所有观测值(N个观测值)组成的向量。E.g.<spanclass="math display"><em>x</em><sub><em>j</em></sub><sup><em>T</em></sup></span>是矩阵<spanclass="math display"><strong>X</strong></span>的第<spanclass="math display"><em>j</em></span>行，<spanclass="math display"><strong>x</strong><sub><em>i</em></sub></span>是矩阵的第<spanclass="math display"><em>i</em></span>列。所有的向量规定为列向量的形式。</td></tr></tbody></table><h2 id="统计学习的重要原则">统计学习的重要原则</h2><p>Statistical Learning的目标是为了最好的泛化性能(Best GeneralizationProperty)而不是最小的训练误差(Minimum Training Error)</p><h2 id="线性回归">线性回归</h2><ol type="1"><li>设输入是一个<strong>列向量</strong><spanclass="math display"><em>X</em></span>, 其每个维度分量下标是<spanclass="math display"><em>X</em><sub><em>i</em></sub></span> <spanclass="math display">$$\begin{align}\hat{f}(x) &amp;= \sum_{i=1}^{p} \hat{\beta}_iX_i + \hat{\beta}_0 \\                      &amp;= \hat{\beta}^TX \\                      &amp;= X^T\hat{\beta}\end{align}$$</span> 写成矩阵形式，其中<spanclass="math display"><strong>X</strong> ∈ ℝ<sup><em>N</em> × <em>p</em></sup></span></li></ol><p><spanclass="math display"><em>Ŷ</em> = <strong>X</strong><em>β̂</em></span></p><ol start="2" type="1"><li><p>线性回归的误差函数衡量方式——最小二乘 <spanclass="math display">$$\begin{align}L(Y, \hat{f}(x)) &amp;= (Y-\hat{f}(x))^T(Y- \hat{f}(x)) \\&amp;= (Y- \textbf{X}\hat{\beta})(Y- \textbf{X}\hat{\beta}) \\&amp;= L(\hat{\beta})\end{align}$$</span></p></li><li><p>从最小化误差函数的方式，对<spanclass="math display"><em>β̂</em></span>求导，得： $$ $$</p></li><li><p>思考一下两种数据分布情况对线性回归模型影响：</p><ol type="1"><li>训练数据中的每一类都是服从高斯分布，而且每一类的方差一样但是均值不同。</li><li>某一类的训练数据是由多个高斯分布合成的，可能高斯分布的方差和均值都不太一样，</li></ol><p>对于第1种情况线性回归是比较optimal的，但是对于第二种效果比较差。</p><p>而K近邻算法对于第2中情况相对会更suitable.</p><p>(先占坑，后续补上解释)</p><p>如下图所示，先从<spanclass="math display"><em>N</em>((1, 0)<sup><em>T</em></sup>, <strong>I</strong>)</span>和<spanclass="math display"><em>N</em>((0, 1)<sup><em>T</em></sup>, <strong>I</strong>)</span>中分别随机采样10个sample(共计20个)，前十个作为类别1的均值，后十个作为类别2的均值。然后每个类中各生成100个sample，每个sample都是从<spanclass="math display">$$N(\mu,\frac{\textbf{I}}{5})$$</span>采样，而每次<spanclass="math display"><em>μ</em></span>是从该类中的10个中等概率(<spanclass="math display">$$\Pr = \frac{1}{10}$$</span>)抽样得到。</p><p><img src="https://i.loli.net/2021/09/27/4akOVlu9Q8ex6Wc.png" alt="image-20210927135914964" style="zoom:50%;" /></p><p>圆点连线代表的K近邻的Error曲线，两个方块代表三元线性回归的Error值。可以看到:</p><ul><li>随着<spanclass="math display"><em>k</em></span>值变小，K近邻模型自由度<spanclass="math display"><em>N</em>/<em>k</em></span>在变大，模型复杂度在上升。模型逐渐由欠拟合转变为过拟合。后面也会讲解</li><li>可以看到在这种多高斯分布混合采样得到的数据集中，K近邻最好的Error要比线性回归好。</li></ul></li></ol><h2 id="k近邻">K近邻</h2><ol type="1"><li><p>几何意义：</p><blockquote><p>在特征空间中，对每个训练实例点<spanclass="math display"><em>x</em><sub><em>i</em></sub></span>,距离该点比其他点更近的所有点组成一个区域，叫做单元(Cell).每个训练实例点都拥有一个单元，所有训练实例点的单元对特征空间构成一个划分。<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="《统计学习方法》第二版 李航">[1]</span></a></sup><img src="https://i.loli.net/2021/09/27/bQgTdCXlYJNnRzW.png" alt="image-20210927144131480" style="zoom:50%;" /></p></blockquote></li><li><p>具体内容参见李航《统计学习方法》Chap3课程笔记(先占坑，后面补上笔记后建立链接🔗)</p></li></ol><h2id="统计决策理论statistical-decision-theory">统计决策理论(StatisticalDecision Theory)</h2><p>根据模型选择(ModelSelection)的理论，通过最小化<strong>期望预测误差</strong>(ExpectedPrediction Error, EPE)来确定模型中的参数选择，从而确定模型。 <spanclass="math display"><em>E</em><em>P</em><em>E</em>(<em>f</em>) = <em>E</em>(<em>L</em>(<em>Y</em>, <em>f̂</em>(<em>X</em>)))</span></p><h3 id="对于回归问题的epe">对于回归问题的EPE</h3><p>对于回归问题(不仅仅是线性回归)，<ahref="">大多采用最小均方误差的方法计算误差</a>: <spanclass="math display">$$\begin{align}EPE(f) &amp;= E(L(Y, \hat{f}(X)) \\             &amp;= E_{X,Y}((Y - \hat{f}(X))^2) \\             &amp;= \int\int (Y-f(X)^2)P(x, y) dxdy \\             &amp;= \int_X \{\int_Y(Y- f(X))^2P(y|x)dy\}P(x) dx \\             &amp;= E_X E_{Y|X}(Y - f(X))^2 \\\end{align}$$</span> 故，若想使得EPE取最小值，只需让对于每个<spanclass="math display"><em>X</em> = <em>x</em></span>, 使得<spanclass="math display"><em>E</em><sub><em>Y</em>|<em>X</em></sub></span>最小即可(逐点最小化)，可得<span class="math display">$$\begin{align*}E_{Y|X}(Y- \hat{f}^2(X)) &amp;= E_{Y|X}(Y^2 - 2\hat{f}(X)Y +\hat{f}^2(X)) \\&amp;=E(Y^2)- 2\hat{f}(X)E(Y) + \hat{f}^2(X) \\\end{align*}$$</span> 故 <spanclass="math display"><em>f̂</em>(<em>X</em>) = <em>E</em><sub><em>Y</em>|<em>X</em></sub>(<em>Y</em>) = <em>E</em>(<em>Y</em>|<em>X</em> = <em>x</em>)</span></p><p><strong>含义解释</strong>：理论上期望预测误差最小的回归函数模型，<u>在最小二乘误差意义下</u>，每个训练数据当<spanclass="math display"><em>X</em> = <em>x</em></span>时，取<spanclass="math display"><em>Y</em></span>的平均值(<spanclass="math display"><em>E</em>(<em>Y</em>)</span>)是能够达到的。</p><p>注意，在这里我们认为所有训练数据的<spanclass="math display"><em>X</em></span>与<spanclass="math display"><em>Y</em></span>是两个独立的随机变量，分别取样自不同的分布中组成了联合概率分布。这是一个重要的assumption，也是统计学习里的基本思路。</p><p>另外<spanclass="math display"><em>X</em> = <em>x</em></span>可以拓展为<spanclass="math display"><em>X</em> ∈ <em>ϵ</em>(<em>x</em>)</span>,在某个邻域附近的所有样本的<spanclass="math display"><em>Y</em></span>值取平均。</p><p class="note note-info">关于期望 <spanclass="math display"><em>E</em>(<em>Y</em>|<em>X</em>)</span> 和 <spanclass="math display"><em>E</em><sub><em>X</em></sub>(<em>Y</em>|<em>X</em> = <em>x</em>)</span>，条件期望中的角标，随机变量和事件区别，符号大小写的不同意义等细微区别，参见随笔XXX（先占坑），以帮助理解。—2021.10.08 简要补充 首先要区分随机变量和事件，比如对于随机变量<spanclass="math display"><em>X</em></span>, <spanclass="math display"><em>X</em> = <em>x</em></span>是一个事件，即<spanclass="math display"><em>X</em></span>取一个固定的值<strong>这件事情</strong>发生了，所以是一个事件。而随机变量<spanclass="math display"><em>X</em></span>自身是有可能取任何一个值的，只不过有不同的概率(所以为什么随机变量会有概率分布这个概念)。所以我们说条件期望时，就是指<spanclass="math display"><em>E</em>(<em>Y</em>|<em>X</em> = <em>x</em>)</span>，即在<spanclass="math display"><em>X</em> = <em>x</em></span>事件发生的条件下，<spanclass="math display"><em>Y</em></span>取各种值的可能性的期望。 <spanclass="math display"><em>E</em>(<em>Y</em>|<em>X</em> = <em>x</em>) = ∑<sub><em>y</em> ∈ <em>Y</em></sub><em>p</em>(<em>Y</em> = <em>y</em>|<em>X</em> = <em>x</em>)<em>y</em></span>。此外事件的独立和变量的独立不是一回事。n个事件的独立要求n个事件两两独立，任意三个独立，任意四个独立……任意n个独立。但是随机变量的独立只需要<spanclass="math display"><em>P</em>(<em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, …, <em>X</em><sub><em>n</em></sub>) = ∏<sub><em>i</em></sub><em>X</em><sub><em>i</em></sub></span>一个条件即可。因为每个随机变量可以取很多值，包含了很多<spanclass="math display"><em>X</em><sub><em>i</em></sub> = <em>x</em></span>的事件，所以显然约束要比事件独立要强得多。在信息论中，信息熵和条件熵，互信息与条件互信息中，条件部分是随机变量<spanclass="math display"><em>X</em></span>还是事件<spanclass="math display"><em>X</em> = <em>x</em></span>是大有讲究的。比如见此图：<img src="https://i.loli.net/2021/10/08/2Hr4F1ec7hJEKoI.png" alt="image-20211008200426280" style="zoom: 33%;" />以互信息<spanclass="math display"><em>I</em>(<em>X</em>; <em>Y</em>|<em>Z</em>, <em>V</em>)</span>和<spanclass="math display"><em>I</em>(<em>X</em>; <em>Y</em>|<em>Z</em> = <em>z</em>, <em>V</em>)</span>的关系为例，如下图公式所示<spanclass="math display"><em>I</em>(<em>X</em>; <em>Y</em>|<em>Z</em>, <em>V</em>) = ∑<sub><em>Z</em></sub><em>p</em>(<em>Z</em> = <em>z</em>)<em>I</em>(<em>X</em>; <em>y</em>|<em>Z</em> = <em>z</em>, <em>V</em>)</span>对于条件信息熵: <span class="math display">$$\begin{align}H(X|Y) &amp;= \sum_{y \in Y} p(Y=y) H(X|Y=y) \\&amp;= \sum_{y \in Y} p(Y=y) \sum_{x \in X} p(X=x|Y=y)\log\frac{1}     {p(X=x|Y=y)} \\&amp;= \sum_{X,Y} p(x,y)\log\frac{1}{p(X=x|Y=y)}\end{align}$$</span></p><h3 id="对于分类问题的epe">对于分类问题的EPE</h3><p>先做一些符号约定：</p><p>分类问题的输出是离散型(随机)变量，定义其符号为<spanclass="math display"><em>G</em></span>(Group),属于同一类的值在同一个Group里，以此来表示类别信息。 <spanclass="math display">$$\begin{align}EPE &amp;= E(L(G, \hat{G}(X))) \\&amp;= E_X E_{Y|X}(L(G, \hat{G}(X))) \\&amp;= E_X \sum_k  P(Y=G_k| X=x)L(G_k, \hat{G}(X)) \\\end{align}$$</span> 同理，通过逐点最小化，只需要 <span class="math display">$$\hat{G}(X) = \underset{g}{\operatorname{arg\min}}\, {L(G_k,g)P(Y=G_k|X=x)}$$</span>原因比较显然，哪个样本属于类的概率大，并且损失函数对于分类错误的惩罚大小共同决定了EPE。对于二分类+0-1 Loss来说(分类正确L为0，否则L为1)，那么可得 <spanclass="math display">$$\hat{G}(x) = \underset{g \in {0,1}}{\operatorname{arg\max}}\,P(Y=G_g|X=x)$$</span> 这个分类方式被称为<code>贝叶斯分类器</code>（Bayesclassification）: 我们通过<spanclass="math display"><em>P</em>(<em>G</em><sub><em>k</em></sub>|<em>X</em>)</span>把样本分类到最可能的类别中(classifyto the most probable class)。贝叶斯分配器得到的Error Rate(BayesRate)是所有分类模型中统计学理论上误差最小的。</p><p class="note note-warning">经过刚才的讲解，读者应该能较为清楚的感受到，统计学习中把<spanclass="math display"><em>X</em></span>,<spanclass="math display"><em>Y</em></span>作为两个独立的随机变量研究他们的联合分布，本质上研究的是数据与Label的<strong><u>相关性</u></strong>信息。即几乎目前绝大多数有监督学习的方法(机器学习,深度神经网络等)本质上都是学习数据与Label的分布和相关性信息。所以说这类模型是数据驱动(data-driven)的，机器并没有理解数据，只是学习到数据的分布而已。</p><p class="note note-primary">k近邻及其Majority Vote选出类别的机制，本质上也是通过训练数据去近似<spanclass="math display"><em>P</em>(<em>Y</em> = <em>G</em><sub><em>g</em></sub>|<em>X</em> = <em>x</em>)</span>。而当<spanclass="math display">$$k ,N \rightarrow \infty，\frac{k}{N} \rightarrow0 $$</span>时，K近邻近似得到<spanclass="math display"><em>E</em>(<em>Y</em>|<em>X</em> = <em>x</em>)</span>。通过这个例子，我们也发现EPE的结果在回归和分类问题上本质上是一致的。</p><h2 id="bias-variance-decomposition">Bias-Variance Decomposition</h2><p>先考虑在某个点<spanclass="math display"><em>X</em> = <em>x</em><sub>0</sub></span>上的MSE拆分：<span class="math display">$$\begin{align}MSE(x_0) &amp;= E_\tau[f(x_0) - \hat{y_0}]^2 \\&amp;= E_\tau [f^2(x_0) - 2f(x_0)\hat{y}_0 + \hat{y_0}^2] \\&amp;= E_\tau [\hat{y_0}^2 - 2\hat{y_0}E_\tau(\hat{y_0}) +E_\tau^2(\hat{y_0}) + f^2(x_0) - 2f(x_0)\hat{y_0} +2\hat{y_0}E_\tau(\hat{y_0}) - E_\tau^2(\hat{y_0})] \\&amp;= E_\tau[(\hat{y_0} - E_\tau(\hat{y_0}))^2] + f^2(x_0) -2f(x_0)E_\tau(\hat{y_0}) + E_\tau^2(\hat{y_0}) \\&amp;= E_\tau[(\hat{y_0} - E_\tau(\hat{y_0}))^2] + [f(x_0) -E_\tau(\hat{y_0})]^2\end{align}$$</span></p><p>设训练集为<span class="math display"><em>τ</em></span>, 函数<spanclass="math display"><em>f</em>(<em>x</em>)</span>为准确的预测模型(hasno bias), 计算<strong><spanclass="math display"><em>X</em> = <em>x</em><sub>0</sub></span>这一测试点(testpoint)</strong>上的最小均方误差。由于<spanclass="math display"><em>f</em>(<em>x</em>)</span>为精准模型，因此其变化只与输入有关，因此在<spanclass="math display"><em>τ</em></span>的期望上可以看做是常数，就有如上变换。</p><p>其中第一项是预测模型<spanclass="math display">$$\hat{y_0}$$</span>自身的方差，第二项是预测模型与真实模型之间的Bias。对于方差来说，是由于对计算<spanclass="math display"><em>y</em><sub>0</sub></span>时带来的数据方差(通常假设是<spanclass="math display"><em>N</em>(0, 1)</span>的高斯噪声，方差由此带来)。对于Bias而言，一方面由于在<spanclass="math display"><em>x</em><sub>0</sub></span>的邻域里采样，另一方面由于预测模型总会有误差(<spanclass="math display"><em>N</em>(0, 1)</span>高斯噪声的均值)。</p><p class="note note-warning">注意这里算MSE拆分时的<spanclass="math display"><em>f</em>(<em>x</em><sub>0</sub>)</span>与数据集<spanclass="math display"><em>τ</em></span>中的随机变量<spanclass="math display"><em>Y</em></span>的取值不等价的。因为数据集中的标签<spanclass="math display"><em>Y</em></span>往往充满噪声的，而我们假设理想数据集是满足某种分布<spanclass="math display"><em>f</em>(<em>x</em>)</span>，然后再该分布上加上噪声<spanclass="math display"><em>ϵ</em></span>才得到了数据集中的真实的<spanclass="math display"><em>Y</em></span>。所以算bias时 <spanclass="math display">$$ f(x_0) - E_\tau(\hat{y_0})$$</span>是不能写成<span class="math display">$$ Y - E_\tau(\hat{y_0})$$</span>的。 后面我们算EPE的拆分时，把数据集中的真实<spanclass="math display"><em>Y</em></span>放了进去，因此拆分中多了关于噪声的方差项。详细见下面介绍。</p><h3 id="对于线性回归模型-y-xhatbeta-xbeta-epsilon">对于线性回归模型<spanclass="math display"><em>Y</em> = <em>X</em><em>β̂</em> = <em>X</em><em>β</em> + <em>ϵ</em></span></h3><p>假设数据集 <span class="math display"><em>τ</em></span>中<spanclass="math display"><em>Y</em></span>与<spanclass="math display"><em>X</em></span>成近似线性分布，假设中间差个高斯噪声<spanclass="math display"><em>N</em>(0, <em>I</em><sub><em>p</em></sub>)</span></p><p>由于 <span class="math display">$$\begin{align}\hat{\beta} &amp;= (X^TX)^{-1}X^Ty \\&amp;= (X^TX)^{-1}X^T(X \beta + \epsilon) \\&amp;= \beta + (X^TX)^{-1}X^T \epsilon\end{align}$$</span></p><p>可得到 <span class="math display">$$E(\hat{\beta}) = \beta \\Var(\hat{\beta}) = Var((X^TX)^{-1}X^T \epsilon) = A^T Var(\epsilon) A =A^TA = Var(XX^T)^{-1} \, ???对吗$$</span> 故知道对于<spanclass="math display"><em>β</em></span>的估计<spanclass="math display"><em>β̂</em></span>是无偏估计，因而得到在均方误差中线性回归模型满足<spanclass="math display"><em>E</em>(<em>X</em><em>β</em>) − <em>E</em>(<em>X</em><em>β̂</em>) = 0</span>即<spanclass="math display"><em>f</em>(<em>x</em><sub>0</sub>) − <em>E</em><sub><em>τ</em></sub>(<em>f̂</em>(<em>x</em><sub>0</sub>)) = 0</span>,bias 为0。</p><p>并且在<spanclass="math display"><em>X</em> = <em>x</em><sub>0</sub></span>这一测试点(testpoint) <span class="math display">$$\hat{y_0} = \hat{x_0}^T\beta + x_0^T\sum_{i=1}^N l_i(x_0)\epsilon_i \\where\,\, l_i(x_0) = (x_0^T(X^TX)^{-1}X^T)_i$$</span> 下面，我们对整个数据集(over training set <spanclass="math display"><em>τ</em></span>)的<strong>EPE</strong>进行bias-variance拆分。先考虑固定<spanclass="math display"><em>x</em><sub>0</sub></span>, 让<spanclass="math display"><em>y</em><sub>0</sub></span>变 <spanclass="math display">$$\begin{align}EPE(x_0) &amp;= E_{y_0 | x_0} E_\tau(y_0 - \hat{y_0})^2 \\&amp;= Var(y_0|x_0) + E_\tau ((\hat{y_0} - E_\tau(\hat{y_0})^2)) +(E_\tau \hat{y_0} - x_0^T \beta)^2 \\&amp;= \sigma^2(\epsilon) + E_\tau x_0^T(X^TX)^{-1}x_0\sigma^2(\epsilon) + 0^2\end{align}$$</span></p><h2 id="维度灾难curse-of-dimensionality">维度灾难(Curse ofDimensionality)</h2><p>待填坑</p><h2 id="模型选择与误差model-selection-and-bias">模型选择与误差(ModelSelection and Bias)</h2><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>《统计学习方法》第二版 李航<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>统计学习理论与方法</category>
      
      <category>教材笔记</category>
      
      <category>Elements of Statistical Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>教材笔记</tag>
      
      <tag>统计学习理论与方法</tag>
      
      <tag>ELS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：强化学习(2)——Bellman方程的动态规划求解</title>
    <link href="/posts/5ee11453/"/>
    <url>/posts/5ee11453/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记强化学习2bellman方程的动态规划求解">课程笔记：强化学习(2)——Bellman方程的动态规划求解</h1><h2 id="动态规划">动态规划</h2><div class="note note-info">            <p>本节只是非常粗略的把动态规划的要点总结一下，是课堂笔记纲要，并非详细讲解。先占个坑，以后有空再补</p>          </div><ol type="1"><li><p>动态规划需要满足的条件：</p><ol type="i"><li>最优子结构(Optimal substructure):</li></ol><ul><li><p>原问题的最优解一定也是子问题的最优解。例如，一个输入长度为N的序列的某个Scheduling问题（记为T(1，N)) 的最优解，拆分后一定也是其子问题T(1,N/2-1), T(N/2,N)的最优解。</p></li><li><p>问题的最优解可以通过某种形式组合和计算，得到原问题的最优解</p></li></ul><ol start="2" type="i"><li>重叠子问题</li></ol><ul><li>子问题与原问题有同样的format和structure</li></ul><ol start="3" type="i"><li>无后效性</li></ol><ul><li>计算原问题最优解时，不会对子问题最优解产生影响。</li></ul></li></ol><p><div class="note note-success">            <p>马尔科夫链和马尔科夫决策过程满足上述三个条件的，因为:</p><ul><li>Bellman方程将原问题递归的分解为规模更小的子问题</li><li>子问题拥有同样的format和structure</li><li>Que？有疑问，虽然马尔科夫过程强调每个state只与上一个state有关，与其他state无关(<spanclass="math display"><em>P</em>(<em>x</em><sub><em>n</em></sub>|<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>n</em> − 1</sub>) = <em>P</em>(<em>x</em><sub><em>n</em></sub>|<em>x</em><sub><em>n</em> − 1</sub>)</span>)，但这是保证MDP无后效性的原因嘛？因为通过迭代的方式求解贝尔曼方程，value早晚会传播到很多时间段以后的。</li></ul>          </div></p><ol start="2" type="1"><li><p>Dynamic Planning in MDP:</p><ol type="i"><li>For prediction(Policy Evaluation):</li></ol><p>Input: MDP <spanclass="math display"> &lt; <em>S</em>, <em>A</em>, <em>P</em>, <em>R</em>, <em>γ</em>&gt;</span>和策略 <span class="math display"><em>π</em></span> or MRP <spanclass="math display"> &lt; <em>S</em>, <em>P</em><sup><em>π</em></sup>, <em>R</em><sup><em>π</em></sup>, <em>γ</em>&gt;</span>(没有显示的策略表示，是Implicit Policy) Output: value function <spanclass="math display"><em>v</em><sub><em>π</em></sub></span> 对PolicyEvaluation的理解：给定一个策略，通过贝尔曼方程迭代求出所有状态的价值<spanclass="math display"><em>v</em><sub><em>π</em></sub>(<em>s</em>)</span></p><p>ii.For control:</p><p>Input: MDP <spanclass="math display"> &lt; <em>S</em>, <em>A</em>, <em>P</em>, <em>R</em>, <em>γ</em>&gt;</span></p><p>Output: 最优价值函数 <spanclass="math display"><em>v</em><sub>*</sub></span> 和最优策略 <spanclass="math display"><em>π</em><sub>*</sub></span></p></li><li><p>Bellman方程:</p></li></ol><p><strong>(State-) Value Function:</strong></p><p><img src="https://i.loli.net/2021/09/29/7yIKAVjkGHTbXeU.png" alt="image-20210929174143894" style="zoom:80%;" /><span class="math display">$$\begin{align}  v_\pi(s) &amp;= \sum_{a}\pi(a|s)q_\pi(s,a)\\  &amp;= \sum_a\,\pi(a|s)(R_s^a+\gamma\sum_{s'}(P_{ss'}^a\,v_\pi(s')))(代入q_\pi(s',a)公式)  \end{align}$$</span></p><p>求Optimal <spanclass="math display"><em>v</em><sub>*</sub>(<em>s</em>)</span>(把求期望操作<spanclass="math display">→</span>找<spanclass="math display">arg max </span>操作)：</p><p><img src="https://i.loli.net/2021/09/29/x9D7TQga6GShrsi.png" alt="image-20210929203919230" style="zoom:80%;" /></p><p>$$ $$</p><p><strong>Action Function:</strong></p><p><img src="https://i.loli.net/2021/09/29/VMlc1N2g4DPJo5I.png" alt="image-20210929174301352" style="zoom: 80%;" /><span class="math display">$$\begin{align}  q_*(s, a) &amp;= R_{s}^a + \gamma\sum_{s'}\, P_{ss'}^a v_\pi(s') \\  (or&amp;= \sum_{s'}\, P_{ss'}^a (R_{ss'}^a + v_\pi(s')) )(考虑奖励R_s^a是否与s'有关)\\  &amp;=R_s^a + \gamma\sum_{s'}\,P_{ss'}^a\sum_{a'}\pi(a'|s')q_\pi(s',a') (代入v_\pi(s')公式)\\  \end{align}$$</span> 求Optimal <spanclass="math display"><em>q</em><sub>*</sub>(<em>s</em>, <em>a</em>)</span>时(把求期望操作<span class="math display">→</span>找<spanclass="math display">arg max </span>操作)：</p><p><img src="https://i.loli.net/2021/09/29/xIVUrmEWBt4Nlzf.png" alt="image-20210929204248905" style="zoom:80%;" /><span class="math display">$$q_*(s, a) = R_s^a + \gamma\sum_{s'}\,P_{ss'}^a{\underset{a'}\max\,}q_\pi(s', a'))$$</span></p><h2 id="policy-evaluation">Policy Evaluation</h2><figure><img src="https://i.loli.net/2021/09/29/ghzERuXiC1MtsQp.png"alt="image-20210929211939079" /><figcaption aria-hidden="true">image-20210929211939079</figcaption></figure><p>给定一个policy <spanclass="math display"><em>π</em></span>，评估/衡量在 <spanclass="math display"><em>π</em></span> 下每个状态的value function.</p><h2 id="policy-iteration">Policy Iteration</h2><figure><img src="https://i.loli.net/2021/09/29/2THt83175YAOZRJ.png"alt="image-20210929212427612" /><figcaption aria-hidden="true">image-20210929212427612</figcaption></figure><p>这个是通过贪心的策略来迭代得到最好的policy。基本思想是<spanclass="math display">$$\pi(s) = \underset{a}{\operatorname{arg\,max}}\,q(s, a)$$</span>，即在state function固定情况下，通过贪心的选择给自己带来最大受益的action a来作为自己的策略。可以证明经过多次迭代后，策略能收敛到贪心策略下的最优解上。</p><h2 id="value-iteration">Value Iteration</h2><figure><img src="https://i.loli.net/2021/09/29/NpwbPFi3k52uedH.png"alt="image-20210929213203328" /><figcaption aria-hidden="true">image-20210929213203328</figcaption></figure><p>这个的特点是没有给定任何显示的(explicit)策略<spanclass="math display"><em>π</em></span>, 而是只求解state function。</p><p>与Policy Evaluation 和Policy Iteration 不同， 前两者都出现了<spanclass="math display"><em>π</em>(<em>a</em>|<em>s</em>)</span>这个东西，也就是说每个状态下选择某个动作的概率是被记录的，并且可能随着算法也会迭代更新；而ValueIteration 每次选择受益最大的动作来更新value。</p><p>等迭代结束，value全部求出来后，最后一遍求解<spanclass="math display"><em>π</em></span>。</p><h2 id="summary">Summary</h2><figure><img src="https://i.loli.net/2021/09/29/pNR9Q8biXWH2GOq.png"alt="image-20210929213757669" /><figcaption aria-hidden="true">image-20210929213757669</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：算法分析与设计(0)——基础知识回顾</title>
    <link href="/posts/ffef3022/"/>
    <url>/posts/ffef3022/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="课程笔记算法设计与分析0">课程笔记：算法设计与分析(0)</h1><h2 id="notation">Notation</h2><h3 id="o-notation">O-notation</h3><ol type="1"><li><spanclass="math display"><em>f</em>(<em>n</em>) = <em>Ω</em>(<em>g</em>(<em>n</em>))</span>:<spanclass="math display">∃<em>c</em>, <em>n</em><sub>0</sub>, <em>w</em><em>h</em><em>e</em><em>n</em>  <em>n</em> ≥ <em>n</em><sub>0</sub>, <em>f</em>(<em>n</em>) ≥ <em>c</em><em>g</em>(<em>n</em>)</span>(<span class="math display"><em>ω</em>(·)</span>严格<spanclass="math display">&gt;</span>)</li><li><spanclass="math display"><em>f</em>(<em>n</em>) = <em>O</em>(<em>g</em>(<em>n</em>))</span>:<spanclass="math display">∃<em>c</em>, <em>n</em><sub>0</sub>, <em>w</em><em>h</em><em>e</em><em>n</em>  <em>n</em> ≥ <em>n</em><sub>0</sub>, <em>f</em>(<em>n</em>) ≤ <em>c</em><em>g</em>(<em>n</em>)</span>(<span class="math display"><em>o</em>(·)</span>严格<spanclass="math display">&lt;</span>)</li><li><spanclass="math display"><em>f</em>(<em>n</em>) = <em>Θ</em>(<em>g</em>(<em>n</em>))</span>:<spanclass="math display">∃<em>c</em>1, <em>c</em>2, <em>n</em><sub>0</sub>, <em>w</em><em>h</em><em>e</em><em>n</em>  <em>n</em> ≥ <em>n</em><sub>0</sub>, <em>c</em><sub>1</sub><em>g</em>(<em>n</em>) ≤ <em>f</em>(<em>n</em>) ≤ <em>c</em><sub>2</sub><em>g</em>(<em>n</em>)</span></li></ol><h2 id="高中数学复习回顾">高中数学复习回顾</h2><h3 id="指数函数exponentials">指数函数(Exponentials)</h3><ol type="1"><li><p>泰勒展开(忘了的话自己再推一遍) <span class="math display">$$f(x) = f(x_0) + f'(x_0)(x-x_0) + \frac{f''(x_0)}{2}(x-x_0)^2 +\frac{f^{(3)}(x_0)}{3!}(x-x_0)^3 + \dots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^{(n)}$$</span> 常用形式: <span class="math display">$$e^x = 1 + x +\frac{x^2}{2} + \frac{x^3}{3!} + \dots +\frac{x^{(n)}}{n!}+ \dots$$</span> <span class="math display">ln (1 + <em>x</em>)</span>在0点展开， <span class="math display"><em>x</em> ∈ (−1, 1)</span> <spanclass="math display">$$\ln(1+x) = x-\frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} +\frac{x^5}{5} - \dots + (-1)^{n-1}\frac{x^n}{n}$$</span></p><p><strong>记忆：</strong><spanclass="math display">ln (1 + <em>x</em>)</span>和<spanclass="math display"><em>e</em><sup><em>x</em></sup></span>对比记忆，都是标准形式，不过对数是±交替并且少了1的常数项</p><p><span class="math display">sin (<em>x</em>)</span>在0点展开， <spanclass="math display"><em>x</em> ∈ <em>R</em></span>: <spanclass="math display">$$\sin(x) = x- \frac{x^3}{3!} + \frac{x^5}{5!} - \dots +(-1)^{n-1}\frac{x^{(2n-1)!}}{(2n-1)!}$$</span> <strong>记忆</strong>： <spanclass="math display">sin </span>是“奇”函数, 所以是第“1,3,5…”项。 <spanclass="math display">$$cos(x)=1 - \frac{x^2}{2} + \frac{x^4}{4!} - \dots +(-1)^{(n-1)}\frac{x^{2(n-1)}}{(2n-2)!}$$</span></p></li><li><p>关于指数函数的重要不等式(从Taylor直接得到) <spanclass="math display">1 + <em>x</em> ≤ <em>e</em><sup><em>x</em></sup> ≤ 1 + <em>x</em> + <em>x</em><sup>2</sup>(<em>x</em> → 0)</span></p><p><spanclass="math display"><em>e</em><sup><em>x</em></sup> = 1 + <em>x</em> + <em>Θ</em>(<em>x</em><sup>2</sup>)(<em>x</em> → 0)</span></p></li><li><p>关于对数函数的重要不等式</p></li></ol><p><span class="math display">$$\frac{x}{1+x} \le \ln(x+1) \le x, x &gt; -1$$</span></p><p>当且仅当<span class="math display"><em>x</em> = 0</span>时成立</p><h3 id="对数函数logarithm">对数函数(Logarithm)</h3><ol type="1"><li><p>换底公式 <span class="math display">$$\log_ba =\frac{\log_ca}{\log_cb}$$</span></p></li><li><p><spanclass="math display"><em>a</em><sup>log<sub><em>b</em></sub><em>c</em></sup> = <em>a</em><sup>log<sub><em>b</em></sub><em>a</em>log<sub><em>a</em></sub><em>c</em></sup> = <em>c</em><sup><em>l</em><em>o</em><em>g</em><sub><em>b</em></sub><em>a</em></sup></span></p></li></ol><h2 id="阶乘factorials">阶乘(Factorials)</h2><ol type="1"><li>Stirling’s Approximation</li></ol><p><span class="math display">$$n! = \sqrt{2\pi n}\left(\frac{n}{e} \right)^n\left(1 +\Theta(\frac{1}{n}) \right)$$</span></p><p>不等式(tight upper bound): <spanclass="math display">2<sup><em>n</em></sup> &lt; <em>n</em>! ≤ <em>n</em><sup><em>n</em></sup></span></p><ol start="2" type="1"><li><p><span class="math display">$$n! = \sqrt{2 \pi n}\left(\frac{n}{e} \right)^n e^{\alpha_n},\frac{1}{12n +1} &lt; \alpha_n &lt; \frac{1}{12n}$$</span></p></li><li><p><spanclass="math display">lg (<em>n</em>!) = <em>Θ</em>(<em>n</em>lg <em>n</em>)</span></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>算法分析与设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文精读笔记：Luna--Linear Unified Nested Attention</title>
    <link href="/posts/45554f7f/"/>
    <url>/posts/45554f7f/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="论文精读笔记neural-ips在投2021-lunalinear-unified-nested-attention">论文精读笔记：(NeuralIPS在投,2021) Luna–Linear Unified Nested Attention</h1><h2 id="abstract">Abstract</h2><p>传统Transformer中的注意力机制计算量是平方级别的。本文提出了Luna，LinearUnified Nested Attention的方法，通过增加一个额外的固定长度的序列作为输入和输出，把平方级别的注意力计算拆分成两个线性时间的计算步骤来做近似，并且该固定长度的序列可以存储足够的上下文相关信息(ContexualInfomation)。</p><h2 id="motivation">Motivation</h2><ol type="1"><li>想提出一个简单有效减低计算复杂度的方法<ul><li>传统的注意力机制的计算和存储都是<spanclass="math inline"><em>O</em>(<em>n</em><sup>2</sup>)</span>的(<spanclass="math inline"><em>n</em></span>表示序列的长度)，但是对于长序列输入，序列间相关性往往是稀疏的(不是完全图)</li><li>已有的改进方法： 1)利用稀疏性约束(Sparse Attention)，如localattention<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, andDustin Tran. Image transformer. In *International Conference on Machine Learning*, pages 4055–4064. PMLR, 2018.">[1]</span></a></sup>,strided<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*, 2019.">[2]</span></a></sup><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.">[3]</span></a></sup>,Hierarchical GlobalAttention<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., &amp; Zhang, Z. (2019). Star-transformer. *arXiv preprint arXiv:1902.09113*.">[4]</span></a></sup>,attentionwith learnablepatterns<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Effificient content-based sparse attention with routing transformers. *Transactions of the Association for Computational Linguistics*,9:53–68, 2021.">[5]</span></a></sup>2)利用注意力矩阵的低秩性，如Linformer<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Wang, S., Li, B., Khabsa, M., Fang, H., &amp; Ma, H. L. (2020). Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*.">[6]</span></a></sup>3)kernel 方法，如LinearTransformer<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In *International Conference on Machine Learning*, pages 5156–5165. PMLR, 2020.">[7]</span></a></sup>,Performer<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. *arXiv preprint arXiv:2009.14794*, 2020.">[8]</span></a></sup>, Random FeatureAttention<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. In *International Conference on Learning Representations*, 2021. URL https://openreview.net/forum?id=QtTKTdVrFBB.">[9]</span></a></sup></li><li>本文所提出的引入额外固定长度序列的方法简洁优美，且能有效将计算复杂度降低到线性级别。</li></ul></li></ol><h2 id="theoremmodel">Theorem&amp;Model</h2><p><img src="https://i.loli.net/2021/08/20/LA4bgRN5s2cFiqG.png" /> <imgsrc="https://i.loli.net/2021/08/20/FfWyYXRlCiumvqZ.png" /> <imgsrc="https://i.loli.net/2021/08/20/jJdZAOwCgLXbWH8.png" /> <imgsrc="https://i.loli.net/2021/08/20/h8w5AaLgQjYMNDo.png" /> <imgsrc="https://i.loli.net/2021/08/20/pgj62btJTOqGYlZ.png" /></p><h2 id="contributioninnovation">Contribution(Innovation)</h2><p>Under review，暂不分析innovation如何。</p><h2 id="evaluation">Evaluation</h2><p>暂略，待后续自己实验后分析。</p><h2 id="其他">其他</h2><p>不过，本篇的idea和几乎同一时间(2021.06)放到Archiv上的ExternalAttention using Two Linear Layers for VisualTasks<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="M. H., Liu, Z. N., Mu, T. J., &amp; Hu, S. M. (2021). Beyond self-attention: External attention using two linear layers for visual tasks. *arXiv preprint arXiv:2105.02358*.URL https://arxiv.org/abs/2105.02358">[10]</span></a></sup>有同工异曲之处，详细可参考知乎<ahref="https://zhuanlan.zhihu.com/p/382961255">介绍</a>。</p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Niki Parmar, Ashish Vaswani,Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, andDustinTran. Image transformer. In <em>International Conference on MachineLearning</em>, pages 4055–4064. PMLR, 2018.<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Rewon Child, Scott Gray,Alec Radford, and Ilya Sutskever. Generating long sequences with sparsetransformers. <em>arXiv preprint arXiv:1904.10509</em>, 2019.<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Iz Beltagy, Matthew EPeters, and Arman Cohan. Longformer: The long-document transformer.<em>arXiv preprint arXiv:2004.05150</em>, 2020.<a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Guo, Q., Qiu, X., Liu, P.,Shao, Y., Xue, X., &amp; Zhang, Z. (2019). Star-transformer. <em>arXivpreprint arXiv:1902.09113</em>.<a href="#fnref:4" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Aurko Roy, Mohammad Saffar,Ashish Vaswani, and David Grangier. Effificient content-based sparseattention with routing transformers. <em>Transactions of the Associationfor Computational Linguistics</em>,9:53–68, 2021.<a href="#fnref:5" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Wang, S., Li, B., Khabsa,M., Fang, H., &amp; Ma, H. L. (2020). Self-attention with linearcomplexity. <em>arXiv preprint arXiv:2006.04768</em>.<a href="#fnref:6" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Angelos Katharopoulos,Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers arernns: Fast autoregressive transformers with linear attention. In<em>International Conference on Machine Learning</em>, pages 5156–5165.PMLR, 2020. <a href="#fnref:7" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Krzysztof Choromanski,Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, TamasSarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, etal. Rethinking attention with performers. <em>arXiv preprintarXiv:2009.14794</em>, 2020.<a href="#fnref:8" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Hao Peng, Nikolaos Pappas,Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Randomfeature attention. In <em>International Conference on LearningRepresentations</em>, 2021. URLhttps://openreview.net/forum?id=QtTKTdVrFBB.<a href="#fnref:9" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:10" class="footnote-text"><span>M. H., Liu, Z. N., Mu, T.J., &amp; Hu, S. M. (2021). Beyond self-attention: External attentionusing two linear layers for visual tasks. <em>arXiv preprintarXiv:2105.02358</em>.URL https://arxiv.org/abs/2105.02358<a href="#fnref:10" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>Transformer</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文精读笔记：Partial FC--Training 10 Million Identities on a Single Machine</title>
    <link href="/posts/a35429c/"/>
    <url>/posts/a35429c/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="论文精读笔记partial-fctraining-10-million-identities-on-a-single-machine">论文精读笔记：PartialFC–Training 10 Million Identities on a Single Machine</h1><p>[TOC]</p><p>To Xicc,</p><p>​ 封面图感谢Xicc的分享！！此生无悔入四月。</p><h2 id="motivation">Motivation</h2><ol type="1"><li><p>传统的DataParallel模式，无法解决大数据集(包含几十万至千万ID,千万至亿个训练样本)训练时，分类器的矩阵参数<spanclass="math inline"><em>W</em><sub><em>N</em><em>K</em> × <em>d</em></sub></span>爆显存的问题(<spanclass="math inline"><em>N</em></span>是Batch数量，<spanclass="math inline"><em>K</em></span>是GPU个数，d是FeatureMap的维度)</p></li><li><p>ModelParallel模式能够有效解决1中问题：通过将矩阵<spanclass="math inline"><em>W</em></span>按<spanclass="math inline"><em>N</em></span>的维度拆分成多个子矩阵放在不同GPU上即可。但是该模式无法解决<spanclass="math inline"><em>l</em><em>o</em><em>g</em><em>i</em><em>t</em><em>s</em><sub><em>N</em> × <em>C</em></sub></span>当<spanclass="math inline"><em>C</em></span>很大时爆显存的问题</p></li><li><p>本文提出的解决方式：保留一个Batch中所有的Positive Class，随机采样NegativeClass，并证明采样率较低情况下，仍能保持performance几乎无损。</p></li><li><p>Related Work(只是听作者介绍，没看):</p></li></ol><ul><li><p>HF-softmax(Goodman,2001)<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Goodman, J. 2001. Classes for fast maximum entropy training. In *IEEE International Conference on Acoustics, Speech,* *and Signal Processing. Proceedings (Cat. No.01CH37221)*, volume 1, 561–564.">[1]</span></a></sup>通过对FeatureMap构建随机hash森林，每次检索最近的class center来获得activeclass subset 缺点： ①class center 存储在RAM中 ②计算featureretrieval也要耗时</p></li><li><p>Softmax Dissection(He etal.2020)<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="He, L.; Wang, Z.; Li, Y.; and Wang, S. 2020. Softmax Dissection: Towards Understanding Intra- and Inter-class Objective for Embedding Learning. In *AAAI Conference on* *Artifificial Intelligence*, 10957–10964.">[2]</span></a></sup>将softmax分成了intra-class objective和inter-classobjective两部分，并且通过减低intra-class objective的计算冗余</p><p>缺点：①难以拓展到其他softmax based方法</p></li></ul><h2 id="theoremmodel">Theorem&amp;Model</h2><ol type="1"><li><p>softmax公式 <span class="math display">$$\sigma (X, i) = \frac{e^{w_i^T X}}{\sum_{j=1}^C e^{w_j^T X}}$$</span>分子可以在GPU-i上算，只要batch大小不会导致爆显存。分母的话每个GPU只需要提供一个scalar，代表一个求和。</p></li><li><p>Model Parallel 在第i块GPU上的算法：</p></li></ol><p><img src="https://i.loli.net/2021/08/15/dgfnUjzxpqQGVke.png" alt="image-20210717092711902" style="zoom:50%;" /></p><p>注解:</p><ul><li>Line 2,<code>allgather</code>是因为同时要使用DataParallel来训练模型</li><li>Line7, <code>allreduce</code>是因为对参数<spanclass="math inline"><em>W</em></span>实行ModelParallel,要从不同GPU上reduce<spanclass="math inline">∑<em>e</em><sup><em>l</em><em>o</em><em>g</em><em>t</em><em>i</em><em>t</em><em>s</em><sub><em>i</em></sub></sup></span></li></ul><ol start="3" type="1"><li><p>随机选取Negative Class的方法：</p><p><img src="https://i.loli.net/2021/08/15/MYaywVFGzf9IeEW.png" alt="image-20210717093753863" style="zoom:50%;" /><img src="https://i.loli.net/2021/08/15/1tEZPeDjrLW8Gwi.png" alt="image-20210717093818660" style="zoom:50%;" /></p></li><li></li></ol><h2 id="contribution">Contribution</h2><p>暂略(以后想到了再写)</p><h2 id="evaluation">Evaluation</h2><ol type="1"><li><p>Training Dataset, Validation Dataset, TestingDataset分别是啥？Backbone? 损失函数？优化器？参数设置？ Training DatasetCASIA, MS1MV2, Celab-500k Testing Dataset LFW(CPLFW, CFLFW), CFP-FP,AgeDB30, Backbone: Resnet-50, 100 Mini-batch size = 512, 8 x 2080Ti LR:0.1起步，后面有衰减(不同数据集不一样)</p><p>训练次数：CASIA： 32K ； MS！MV2 180K； Glint360K 600K.</p></li><li><p>自身Ablation</p><p>比较不同的sample rate, 以及all-sample 和只对negative class sample评价指标：Average Cos Diatnce，因为使用了CosFace和ArcFace的损失函数，度量<code>$x_i$</code>,<code>$W_{y_i}$</code>间的余弦距离<img src="https://i.loli.net/2021/08/15/43dKpjCLtfSJ16v.png" alt="image-20210717094807189" style="zoom:50%;" /></p><p><img src="https://i.loli.net/2021/08/15/r8xdvhPjUKLzR7O.png" alt="image-20210717094939172" style="zoom:50%;" />评估在不同的分类数量情况下的内存使用情况 <imgsrc="https://i.loli.net/2021/08/15/ywXRMfK24xJtrHc.png"alt="image-20210717095831255" /></p></li><li><p>和其他同类方法比较</p></li></ol><p><img src="https://i.loli.net/2021/08/15/1GxVTFIYbv9m5s3.png" alt="image-20210717095513744" style="zoom:50%;" /></p><p><img src="https://i.loli.net/2021/08/15/nfsvX9QTZkBRDqa.png" alt="image-20210717095601380" style="zoom:50%;" /></p><ol start="4" type="1"><li><p>和其他不同类方法比较(看总榜)<img src="https://i.loli.net/2021/08/15/vXkL96DZQta4wbH.png" alt="image-20210717095254291" style="zoom:50%;" /></p></li><li><p>该文章的长处和不足 长处： ①在<strong>ID量</strong>特别大的时候，到达十几万甚至百万时，开一个较大的batch-size也不用担心爆GPU显存② 扔掉大部分negative class确实很节约训练时间，而且loss损失不大</p><p>不足: ① 在ID较少的数据集上用处不大(比赛9万ID用不到，sample rate=1)②只是随机的扔掉负样本比较粗糙，能不能有一些Mining HardNegative？(但是要保证时间成本，保证存储Hard Negative Center不需要耗费太多内存等)</p></li><li><p>自己感觉可以改进的地方①其实结合分布式原语操作(primitive)中的ReduceAll，想要得到<spanclass="math inline"><em>l</em><em>o</em><em>g</em><em>i</em><em>t</em><em>s</em></span>只需要统计部分信息，比如Max,Sum等，所以不扔掉负样本也能做。但是可能有些特殊情况还是要用到完整的<spanclass="math inline"><em>l</em><em>o</em><em>g</em><em>i</em><em>t</em><em>s</em></span></p></li></ol><h2 id="其他">其他</h2><p>搜一搜知乎评论，CSDN博客，查一查OpenReview；这些讨论，质疑对我的思维启发非常大的</p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Goodman, J. 2001. Classesfor fast maximum entropy training. In <em>IEEE International Conferenceon Acoustics, Speech,</em> <em>and Signal Processing. Proceedings (Cat.No.01CH37221)</em>, volume 1, 561–564.<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>He, L.; Wang, Z.; Li, Y.;and Wang, S. 2020. Softmax Dissection: Towards Understanding Intra- andInter-class Objective for Embedding Learning. In <em>AAAI Conferenceon</em> <em>Artifificial Intelligence</em>, 10957–10964.<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>分类器</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Model Parallel</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/posts/4a17b156/"/>
    <url>/posts/4a17b156/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="//music.163.com/outchain/player?type=2&amp;id=26202039&amp;auto=0&amp;height=66"></iframe><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><p>推荐资料：</p><ol type="1"><li>如何搭配Fluid优雅的写一篇文档：https://hexo.fluid-dev.com/posts/fluid-write/</li><li>使用ECharts插件绘制炫酷图表：https://hexo.fluid-dev.com/posts/hexo-echarts/</li><li>给博客文章迁入PPT演示：https://hexo.fluid-dev.com/posts/hexo-nodeppt/</li></ol><p>优雅，实在是太优雅了！</p><hr /><p>下面我们开始不正经：</p><p>再次感谢Fluid官方和作者大大，为表感谢直接把你们的网站强行搬来套娃了😁</p><p>官方示例:</p><iframe src="https://hexo.fluid-dev.com/" width="100%" height="500" name="topFrame" scrolling="yes" noresize="noresize" frameborder="0" id="topFrame"></iframe><hr /><p>作者：</p><iframe src="https://zkqiang.cn/" width="100%" height="500" name="topFrame" scrolling="yes" noresize="noresize" frameborder="0" id="topFrame"></iframe>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
